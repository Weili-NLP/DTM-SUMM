Using Supervised Bigram - based ILP for Extractive Summarization In this paper , we propose a bigram based supervised method for extractive document summarization in the integer linear programming ( ILP ) framework .
For each bigram , a regression model is used to estimate its frequency in the reference summary .
The regression model uses a variety ofindicative features and is trained dis - criminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary .
During testing , the sentence selection problem is formulated as an ILP problem to maximize the bigram gains .
We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets , and performs competitively compared to the best results in the TAC evaluations .
We also conducted various analysis to show the impact of bigram selection , weight estimation , and ILP setup .
1 Introduction Extractive summarization is a sentence selection problem : identifying important summary sentences from one or multiple documents .
Many methods have been developed for this problem , including supervised approaches that use classifiers to predict summary sentences , graph based approaches to rank the sentences , and recent global optimization methods such as integer linear programming ( ILP ) and submodular methods .
These global optimization methods have been shown to be quite powerful for extractive summarization , because they try to select important sentences and remove redundancy at the same time under the length constraint .
Gillick and Favre ( Gillick and Favre , 2009 ) introduced the concept - based ILP for summarization .
Their system achieved the best result in the TAC 09 summarization task based on the ROUGE evaluation metric .
In this approach the goal is to maximize the sum of the weights of the language concepts that appear in the summary .
They used bigrams as such language concepts .
The association between the language concepts and sentences serves as the constraints .
This ILP method is formally represented as below ( see ( Gillick and Favre , 2009 ) for more details ): max Y , i WiCi (!)
YJj Sj Occij > Ci ( 3 ) ci and Sj are binary variables ( shown in ( 5 ) and ( 6 )) that indicate the presence of a concept and a sentence respectively .
wi is a concept ' s weight and Occij means the occurrence of concept i in sentence j . Inequalities ( 2 )( 3 ) associate the sentences and concepts .
They ensure that selecting a sentence leads to the selection of all the concepts it contains , and selecting a concept only happens when it is present in at least one of the selected sentences .
There are two important components in this concept - based ILP : one is how to select the concepts ( ci ); the second is how to set up their weights Gillick and Favre ( Gillick and Favre , 2009 ) used bigrams as concepts , which are selected from a subset of the sentences , and their document frequency as the weight in the objective function .
In this paper , we propose to find a candidate summary such that the language concepts ( e . g ., bi - grams ) in this candidate summary and the reference summary can have the same frequency .
We expect this restriction is more consistent with the ROUGE evaluation metric used for summarization ( Lin , 2004 ).
In addition , in the previous concept - based ILP method , the constraints are with respect to the appearance of language concepts , hence it cannot distinguish the importance of different language concepts in the reference summary .
Our method can decide not only which language concepts to use in ILP , but also the frequency of these language concepts in the candidate summary .
To estimate the bigram frequency in the summary , we propose to use a supervised regression model that is discriminatively trained using a variety of features .
Our experiments on several TAC summarization data sets demonstrate this proposed method outperforms the previous ILP system and often the best performing TAC system .
2 Proposed Method 2 . 1 Bigram Gain Maximization by ILP We choose bigrams as the language concepts in our proposed method since they have been successfully used in previous work .
In addition , we expect that the bigram oriented ILP is consistent with the ROUGE - 2 measure widely used for summarization evaluation .
We start the description of our approach for the scenario where a human abstractive summary is provided , and the task is to select sentences to form an extractive summary .
Then Our goal is to make the bigram frequency in this system summary as close as possible to that in the reference .
For each bigram b , we define its gain : where nb > re / is the frequency of b in the reference summary , and nbySum is the frequency of b in the automatic summary .
The gain of a bigram is no more than its frequency in the reference summary , hence adding redundant bigrams will not increase the gain .
The total gain of an extractive summary is defined as the sum of every bigram gain in the summary : where s is a sentence in the document , nb ; S is the frequency of b in sentence s , z ( s ) is a binary variable , indicating whether s is selected in the summary .
The goal is to find z that maximizes Gain ( sum ) ( formula ( 8 )) under the length constraint L . This problem can be casted as an ILP problem .
First , using the fact that Now the problem is equivalent to : This is equivalent to the ILP : where Cb is an auxiliary variable we introduce that is equal to \! nb > re / — s z ( s ) * nb ; S \!, and nb > re / is a constant that can be dropped from the objective function .
2 . 2 Regression Model for Bigram Frequency Estimation In the previous section , we assume that nb > re / is at hand ( reference abstractive summary is given ) and propose a bigram - based optimization framework for extractive summarization .
However , for the summarization task , the bigram frequency is unknown , and thus our first goal is to estimate such frequency .
We propose to use a regression model for this .
Since a bigram ' s frequency depends on the summary length ( L ), we use a normalized frequency in the summary .
Now the problem is to automatically estimate Nb , ref - Since the normalized frequency Nb > ref is a real number , we choose to use a logistic regression model to predict it : where f ( bj ) is the feature vector of bigram bj and w ' is the corresponding feature weight .
Since even for identical bigrams bi = bj , their feature vectors may be different (/( ftj ) / f { bj )) due to their different contexts , we sum up frequencies for identical bigrams { bi \ bi = b }\ Eiexp { w //( 6i )} To train this regression model using the given reference abstractive summaries , rather than trying to minimize the squared error as typically done , we propose a new objective function .
Since the normalized frequency satisfies the probability constraint J2b Nb , ref = 1 , we propose to use KL divergence to measure the distance between the estimated frequencies and the ground truth values .
The objective function for training is thus to minimize the KL distance : min Y Nb , ref log where Nb , ref is the true normalized frequency of bigram b in reference summaries .
Finally , we replace N ^ ref m Formula ( 15 ) with Eq ( 14 ) and get the objective function below : This shares the same form as the contrastive estimation proposed by ( Smith and Eisner , 2005 ).
We use gradient decent method for parameter estimation , initial w is set with zero .
2 . 3 Features Each bigram is represented using a set of features in the above regression model .
We use two types of features : word level and sentence level features .
Some of these features have been used in previous work ( Aker and Gaizauskas , 2009 ; Brandow et al ., 1995 ; Edmundson , 1969 ; Radev , 2001 ): • Word Level : - 1 .
Term frequencyl : The frequency of this bigram in the given topic .
Term frequency2 : The frequency of this bigram in the selected sentences .
Similarity with topic title : The number of common tokens in these two strings , divided by the length of the longer string .
Similarity with description of the topic : Similarity of the bigram with topic description ( see next data section about the given topics in the summarization task ).
• Sentence Level : ( information of sentence containing the bigram ) - 6 .
Sentence ratio : Number of sentences that include this bigram , divided by the total number of the selected sentences .
Sentence similarity : Sentence similarity with topic ' s query , which is the concatenation of topic title and description .
Sentence position : Sentence position in the document .
Sentence length : The number of words in the sentence .
Paragraph starter : Binary feature indicating whether this sentence is the beginning of a paragraph .
3 Experiments 3 . 1 Data We evaluate our method using several recent TAC data sets , from 2008 to 2011 .
The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query ( with a title and more detailed description ).
For model training , we also included two years ' DUC data ( 2006 and 2007 ).
When evaluating on one TAC data set , we use the other years of the TAC data plus the two DUC data sets as the training data .
See next section about the sentence selection step in our method .
Let nb > ref = Nb > ref * L , where is the normalized frequency 3 . 2 Summarization System We use the same system pipeline described in ( Gillick et al ., 2008 ; McDonald , 2007 ).
The key modules in the ICSI ILP system ( Gillick et al ., 2008 ) are briefly described below .
• Step 1 : Clean documents , split text into sentences .
• Step 2 : Extract bigrams from all the sentences , then select those bigrams with document frequency equal to more than 3 .
We call this subset as initial bigram set in the following .
• Step 3 : Select relevant sentences that contain at least one bigram from the initial bigram set .
• Step 4 : Feed the ILP with sentences and the bigram set to get the result .
• Step 5 : Order sentences identified by ILP as the final result of summary .
The difference between the ICSI and our system is in the 4th step .
In our method , we first extract all the bigrams from the selected sentences and then estimate each bigram ' s Nb re / using the regression model .
Then we use the top - n bigrams with their Nb re / and all the selected sentences in our proposed ILP module for summary sentence selection .
When training our bigram regression model , we use each of the 4 reference summaries separately , i . e ., the bigram frequency is obtained from one reference summary .
The same pre - selection of sentences described above is also applied in training , that is , the bigram instances used in training are from these selected sentences and the reference summary .
4 Experiment and Analysis 4 . 1 Experimental Results Table 1 shows the ROUGE - 2 results of our proposed system , the ICSI system , and also the best performing system in the NIST TAC evaluation .
We can see that our proposed system consistently outperforms ICSI ILP system ( the gain is statistically significant based on ROUGE ' s 95 % confidence internal results ).
Compared to the best reported TAC result , our method has better performance on three data sets , except 2011 data .
Note that the best performing system for the 2009 data is the ICSI ILP system , with an additional compression step .
Our ILP method is purely extractive .
Even without using compression , our approach performs better than the full ICSI system .
The best performing system for the 2011 data also has some compression module .
We expect that after applying sentence compression and merging , we will have even better performance , however , our focus in this paper is on the bigram - based extractive summarization .
There are several differences between the ICSI system and our proposed method .
First is the bigrams ( concepts ) used .
We use the top 100 bigrams from our bigram estimation module ; whereas the ICSI system just used the initial bigram set described in Section 3 . 2 .
Second , the weights for those bigrams differ .
We used the estimated value from the regression model ; the ICSI system just uses the bigram ' s document frequency in the original text as weight .
Finally , two systems use different ILP setups .
To analyze which factors ( or all of them ) explain the performance difference , we conducted various controlled experiments for these three factors ( bigrams , weights , ILP ).
All of the following experiments use the TAC 2009 data as the test set .
4 . 2 Effect of Bigram Weights In this experiment , we vary the weighting methods for the two systems : our proposed method and the ICSI system .
We use three weighting setups : the estimated bigram frequency value in our method , document frequency , or term frequency from the original text .
Table 2 and 3 show the results using the top 100 bigrams from our system and the initial bigram set from the ICSI system respectively .
We also evaluate using the two different ILP configurations in these experiments .
First of all , we can see that for both ILP systems , our estimated bigram weights outperform the other frequency - based weights .
For the ICSI ILP system , using bigram document frequency achieves better performance than term frequency ( which verified why document frequency is used in their system ).
In contrast , for our ILP method , Table 1 : ROUGE - 2 summarization results .
Table 2 : Results using different weighting methods on the top 100 bigrams generated from our proposed system .
Table 3 : Results using different weighting methods based on the initial bigram sets .
The average number of bigrams is around 80 for each topic .
the bigram ' s term frequency is slightly more useful than its document frequency .
This indicates that our estimated value is more related to bi - gram ' s term frequency in the original text .
When the weight is document frequency , the ICSI ' s result is better than our proposed ILP ; whereas when using term frequency as the weights , our ILP has better results , again suggesting term frequency fits our ILP system better .
When the weight is estimated value , the results depend on the bigram set used .
The ICSI ' s ILP performs slightly better than ours when it is equipped with the initial bigram , but our proposed ILP has much better results using our selected top100 bigrams .
This shows that the size and quality of the bigrams has an impact on the ILP modules .
4 . 3 The Effect of Bigram Set ' s size In our proposed system , we use 100 top bigrams .
There are about 80 bigrams used in the ICSI ILP system .
A natural question to ask is the impact of the number of bigrams and their quality on the summarization system .
Table 4 shows some statistics of the bigrams .
We can see that about one third of bigrams in the reference summary are in the original text ( 127 . 3 out of 321 . 93 ), verifying that people do use different words / bigram when writing abstractive summaries .
We mentioned that we only use the top - N ( n is 100 in previous experiments ) bigrams in our summarization system .
On one hand , this is to save computational cost for the ILP module .
On the other hand , we see from the table that only 127 of these more than 2K bi - grams are in the reference summary and are thus expected to help the summary responsiveness .
Including all the bigrams would lead to huge noise .
# bigrams used in our regression model 2140 . 7 ( i . e ., in selected sentences ) Table 4 : Bigram statistics .
The numbers are the average ones for each topic .
Fig 1 shows the bigram coverage ( number ofbi - grams used in the system that are also in reference summaries ) when we vary N selected bigrams .
As expected , we can see that as n increases , there are more reference summary bigrams included in the system .
There are 25 summary bigrams in the top - 50 bigrams and about 38 in top - 100 bigrams .
Compared with the ICSI system that has around 80 bigrams in the initial bigram set and 29 in the reference summary , our estimation module has better coverage .
Figure 1 : Coverage of bigrams ( number of bi - grams in reference summary ) when varying the number of bigrams used in the ILP systems .
Increasing the number of bigrams used in the system will lead to better coverage , however , the incorrect bigrams also increase and have a negative impact on the system performance .
To examine the best tradeoff , we conduct the experiments by choosing the different top - N bigram set for the two ILP systems , as shown in Fig 2 .
For both the ILP systems , we used the estimated weight value for the bigrams .
We can see that the ICSI ILP system performs better when the input bigrams have less noise ( those bigrams that are not in summary ).
However , our proposed method is slightly more robust to this kind of noise , possibly because of the weights we use in our system - the noisy bigrams have lower weights and thus less impact on the final system performance .
Overall the two systems have similar trends : performance increases at the beginning when using more bigrams , and after certain points starts degrading with too many bigrams .
The optimal number of bigrams differs for the two systems , with a larger number of bigrams in our method .
We also notice that the ICSI ILP system achieved a ROUGE - 2 of 0 . 1218 when using top 60 bigrams , which is better than using the initial bigram set in their method ( 0 . 1160 ).
Figure 2 : Summarization performance when varying the number of bigrams for two systems .
4 . 4 Oracle Experiments Based on the above analysis , we can see the impact of the bigram set and their weights .
The following experiments are designed to demonstrate the best system performance we can achieve ifwe have access to good quality bigrams and weights .
Here we use the information from the reference summary .
The first is an oracle experiment , where we use all the bigrams from the reference summaries that are also in the original text .
In the ICSI ILP system , the weights are the document frequency from the multiple reference summaries .
In our ILP module , we use the term frequency of the bigram .
The oracle results are shown in Table 5 .
We can see these are significantly better than the automatic systems .
From Table 5 , we notice that ICSI ' s ILP performs marginally better than our proposed ILP .
We hypothesize that one reason may be that many bi - grams in the summary reference only appear once .
Table 6 shows the frequency of the bigrams in the summary .
Indeed 85 % of bigram only appear once Table 5 : Oracle experiment : using bigrams and their frequencies in the reference summary as weights .
and no bigrams appear more than 9 times .
For the majority of the bigrams , our method and the ICSI ILP are the same .
For the others , our system has slight disadvantage when using the reference term frequency .
We expect the high term frequency may need to be properly smoothed / normalized .
We also treat the oracle results as the gold standard for extractive summarization and compared how the two automatic summarization systems differ at the sentence level .
This is different from the results in Table 1 , which are the ROUGE results comparing to human written abstractive summaries at the n - gram level .
We found that among the 188 sentences in this gold standard , our system hits 31 and ICSI only has 23 .
This again shows that our system has better performance , not just at the word level based on ROUGE measures , but also at the sentence level .
There are on average 3 different sentences per topic between these two results .
In the second experiment , after we obtain the estimated Nb re / for every bigram in the selected sentences from our regression model , we only keep those bigrams that are in the reference summary , and use the estimated weights for both ILP modules .
Table 7 shows the results .
We can consider these as the upper bound the system can achieve if we use the automatically estimated weights for the correct bigrams .
In this experiment ICSI ILP ' s performance still performs better than ours .
This might be attributed to the fact there is less noise ( all the bigrams are the correct ones ) and thus the ICSI ILP system performs well .
We can see that these results are worse than the previous oracle experiments , but are better than using the automatically generated bigrams , again showing the bigram and weight estimation is critical for summarization .
Figure 2 : Summarization performance when varying the number of bigrams for two systems .
Table 6 : Average number of bigrams for each term frequency in one topic ' s reference summary .
Table 7 : Summarization results when using the estimated weights and only keeping the bigrams that are in the reference summary .
4 . 5 Effect of Training Set Since our method uses supervised learning , we conduct the experiment to show the impact of training size .
In TAC ' s data , each topic has two sets of documents .
For set A , the task is a standard summarization , and there are 4 reference summaries , each 100 words long ; for set B , it is an update summarization task - the summary includes information not mentioned in the summary from set A .
There are also 4 reference summaries , with 400 words in total .
Table 8 shows the results on 2009 data when using the data from different years and different sets for training .
We notice that when the training data only contains set A , the performance is always better than using set B or the combined set A and B .
This is not surprising because of the different task definition .
Therefore , for the rest of the study on data size impact , we only use data set A from the TAC data and the DUC data as the training set .
In total there are about 233 topics from the two years ' DUC data ( 06 , 07 ) and three years ' TAC data ( 08 , 10 , 11 ).
We incrementally add 20 topics every time ( from DUC06 to TAC11 ) and plot the learning curve , as shown in Fig 3 .
As expected , more training data results in better performance .
Figure 3 : Learning curve 4 . 6 Summary of Analysis The previous experiments have shown the impact of the three factors : the quality of the bigrams themselves , the weights used for these bigrams , and the ILP module .
We found that the bigrams and their weights are critical for both the ILP setups .
However , there is negligible difference between the two ILP methods .
An important part of our system is the supervised method for bigram and weight estimation .
We have already seen for the previous ILP method , when using our bigrams together with the weights , better performance can be achieved .
Therefore we ask the question whether this is simply because we use supervised learning , or whether our proposed regression model is the key .
To answer this , we trained a simple supervised binary classifier for bigram prediction ( positive means that a bigram appears in the summary ) using the same set of features as used in our bigram weight estimation module , and then used their document frequency in the ICSI ILP system .
The result for this method is 0 . 1128 on the TAC 2009 data .
This is much lower than our result .
We originally expected that using the supervised method may outperform the unsupervised bigram selection which only uses term frequency information .
Further experiments are needed to investigate this .
From this we can see that it is not just the supervised methods or using annotated data that yields the overall improved system performance , but rather our proposed regression setup for bigrams is the main reason .
5 Related Work We briefly describe some prior work on summarization in this section .
Unsupervised methods have been widely used .
In particular , recently several optimization approaches have demonstrated competitive performance for extractive summarization task .
Maximum marginal relevance ( MMR ) ( Carbonell and Goldstein , 1998 ) uses a greedy algorithm to find summary sentences .
( McDonald , 2007 ) improved the MMR algorithm to dynamic programming .
They used a modified objective function in order to consider whether the selected sentence is globally optimal .
Sentence - level ILP was also first introduced in ( McDonald , 2007 ), but ( Gillick and Favre , 2009 ) revised it to concept - based ILP .
( Woodsend and Lapata , 2012 ) utilized ILP to jointly optimize different aspects including content selection , surface realization , and rewrite rules in summarization .
( Galanis et al ., 2012 ) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary .
( Berg - Kirkpatrick et al ., 2011 ) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization .
( Jin et al ., 2010 ) made a comparative study on sentence / concept selection and pairwise and list ranking algorithms , and concluded ILP performed better than MMR and the diversity penalty strategy in sentence / concept selection .
Other global optimization methods include submodularity ( Lin and Bilmes , 2010 ) and graph - based approaches ( Erkan and Radev , 2004 ; Leskovec et al ., 2005 ; Mihalcea and Tarau , 2004 ).
Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising .
For example , ( Celikyilmaz and Hakkani - Tur , 2011 ) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non - redundant summaries .
( Darling and Song , 2011 ) applied it to separate the semantically important words from the low - content function words .
Table 8 : Summarization performance when using different training corpora .
In contrast to these unsupervised approaches , there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not .
Different features and classifiers have been explored for this task , such as Bayesian method ( Kupiec et al ., 1995 ), maximum entropy ( Osborne , 2002 ), CRF ( Galley , 2006 ), and recently reinforcement learning ( Ryang and Abekawa , 2012 ).
( Aker et al ., 2010 ) used discriminative reranking on multiple candidates generated by A * search .
Recently , research has also been performed to address some issues in the supervised setup , such as the class data imbalance problem ( Xie and Liu , 2010 ).
In this paper , we propose to incorporate the supervised method into the concept - based ILP framework .
Unlike previous work using sentence - based supervised learning , we use a regression model to estimate the bigrams and their weights , and use these to guide sentence selection .
Compared to the direct sentence - based classification or regression methods mentioned above , our method has an advantage .
When abstractive summaries are given , one needs to use that information to automatically generate reference labels ( a sentence is in the summary or not ) for extractive summarization .
Most researchers have used the similarity between a sentence in the document and the abstractive summary for labeling .
This is not a perfect process .
In our method , we do not need to generate this extra label for model training since ours is based on bigrams - it is straightforward to obtain the reference frequency for bigrams by simply looking at the reference summary .
We expect our approach also paves an easy way for future automatic abstractive summarization .
One previous study that is most related to ours is ( Conroy et al ., 2011 ), which utilized a Naive Bayes classifier to predict the probability of a bigram , and applied ILP for the final sentence selection .
They used more features than ours , whereas we use a discrim - inatively trained regression model and a modified ILP framework .
Our proposed method performs better than their reported results in TAC 2011 data .
Another study closely related to ours is ( Davis et al ., 2012 ), which leveraged Latent Semantic Analysis ( LSA ) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem .
6 Conclusion and Future Work In this paper , we leverage the ILP method as a core component in our summarization system .
Different from the previous ILP summarization approach , we propose a supervised learning method ( a discriminatively trained regression model ) to determine the importance of the bigrams fed to the ILP module .
In addition , we revise the ILP to maximize the bigram gain ( which is expected to be highly correlated with ROUGE - 2 scores ) rather than the concept / bigram coverage .
Our proposed method yielded better results than the previous state - of - the - art ILP system on different TAC data sets .
From a series of experiments , we found that there is little difference between the two ILP modules , and that the improved system performance is attributed to the fact that our proposed supervised bigram estimation module can successfully gather the important bigram and assign them appropriate weights .
There are several directions that warrant further research .
We plan to consider the context of bigrams to better predict whether a bigram is in the reference summary .
We will also investigate the relationship between concepts and sentences , which may help move towards abstractive summarization .
Acknowledgments This work is partly supported by DARPA under Contract No .
HR0011 - 12 - C - 0016 and FA8750 - 13 - 2 - 0041 , and NSF IIS - 0845484 .
Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or NSF .
