A Hybrid Hierarchical Model for Multi - Document Summarization Scoring sentences in documents given abstract summaries created by humans is important in extractive multi - document summarization .
In this paper , we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference .
We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model .
Then , using these scores , we train a regression model based on the lexical and structural characteristics of the sentences , and use the model to score sentences of new documents to form a summary .
Our system advances current state - of - the - art improving ROUGE scores by ~ 7 %.
Generated summaries are less redundant and more coherent based upon manual quality evaluations .
1 Introduction Extractive approach to multi - document summarization ( MDS ) produces a summary by selecting sentences from original documents .
Document Understanding Conferences ( DUC ), now TAC , fosters the effort on building MDS systems , which take document clusters ( documents on a same topic ) and description of the desired summary focus as input and output a word length limited summary .
Human summaries are provided for training summarization models and measuring the performance of machine generated summaries .
Extractive summarization methods can be classified into two groups : supervised methods that rely on provided document - summary pairs , and unsupervised methods based upon properties derived from document clusters .
Supervised methods treat the summarization task as a classification / regression problem , e . g ., ( Shen et al ., 2007 ; Yeh et al ., 2005 ).
Each candidate sentence is classified as summary or non - summary based on the features that they pose and those with highest scores are selected .
Unsupervised methods aim to score sentences based on semantic groupings extracted from documents , e . g ., ( DauméIII and Marcu , 2006 ; Titov and McDonald , 2008 ; Tang et al ., 2009 ; Haghighi and Vanderwende , 2009 ; Radev et al ., 2004 ; Branavan et al ., 2009 ), etc .
Such models can yield comparable or better performance on DUC and other evaluations , since representing documents as topic distributions rather than bags of words diminishes the effect oflexical variability .
To the best ofourknowl - edge , there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper .
In this paper , we present a novel approach that formulates MDS as a prediction problem based on a two - step hybrid model : a generative model for hierarchical topic discovery and a regression model for inference .
We investigate if a hierarchical model can be adopted to discover salient characteristics of sentences organized into hierarchies utilizing human generated summary text .
We present a probabilistic topic model on sentence level building on hierarchical Latent Dirich - let Allocation ( hLDA ) ( Blei et al ., 2003a ), which is a generalization of LDA ( Blei et al ., 2003b ).
We construct a hybrid learning algorithm by extracting salient features to characterize summary sentences , and implement a regression model for inference ( Fig . 3 ).
Contributions of this work are : — construction of hierarchical probabilistic model designed to discover the topic structures of all sentences .
Our focus is on identifying similarities of candidate sentences to summary sentences using a novel tree based sentence scoring algorithm , concerning topic distributions at different levels of the discovered hierarchy as described in § 3 and § 4 , — representation of sentences by meta - features to characterize their candidacy for inclusion in summary text .
Our aim is to find features that can best represent summary sentences as described in § 5 , — implementation of a feasible inference method based on a regression model to enable scoring of sentences in test document clusters without retraining , ( which has not been investigated in generative summarization models ) described in § 5 . 2 .
We show in § 6 that our hybrid summarizer achieves comparable ( if not better ) ROUGE score on the challenging task of extracting the summaries of multiple newswire documents .
The human evaluations confirm that our hybrid model can produce coherent and non - redundant summaries .
2 Background and Motivation There are many studies on the principles governing multi - document summarization to produce coherent and semantically relevant summaries .
Previous work ( Nenkova and Vanderwende , 2005 ; Conroy et al ., 2006 ), focused on the fact that frequency of words plays an important factor .
While , earlier work on summarization depend on a word score function , which is used to measure sentence rank scores based on ( semi -) supervised learning methods , recent trend of purely data - driven methods , ( Barzilay and Lee , 2004 ; DauméIII and Vanderwende , 2009 ), have shown remarkable improvements .
Our work builds on both methods by constructing a hybrid approach to summarization .
Our objective is to discover from document clusters , the latent topics that are organized into hierarchies following ( Haghighi and Vanderwende , 2009 ).
A hierarchical model is particularly appealing to summarization than a " flat " model , e . g .
LDA ( Blei et al ., 2003b ), in that one can discover " abstract " and " specific " topics .
For instance , discovering that " baseball " and " football " are both contained in an abstract class " sports " can help to identify summary sentences .
It follows that summary topics are commonly shared by many documents , while specific topics are more likely to be mentioned in rather a small subset of documents .
Feature based learning approaches to summarization methods discover salient features by measuring similarity between candidate sentences and summary sentences ( Nenkova and Vanderwende , 2005 ; Conroy et al ., 2006 ).
While such methods are effective in extractive summarization , the fact that some of these methods are based on greedy algorithms can limit the application areas .
Moreover , using information on the hidden semantic structure of document clusters would improve the performance of these methods .
Recent studies focused on the discovery of latent topics of document sets in extracting summaries .
In these models , the challenges of inferring topics of test documents are not addressed in detail .
One of the challenges of using a previously trained topic model is that the new document might have a totally new vocabulary or may include many other specific topics , which may or may not exist in the trained model .
A common method is to re - build a topic model for new sets of documents ( Haghighi and Vanderwende , 2009 ), which has proven to produce coherent summaries .
An alternative yet feasible solution , presented in this work , is building a model that can summarize new document clusters using characteristics of topic distributions of training documents .
Our approach differs from the early work , in that , we combine a generative hierarchical model and regression model to score sentences in new documents , eliminating the need for building a generative model for new document clusters .
3 Summary - Focused Hierarchical Model Our MDS system , hybrid hierarchical summa - rizer , HybHSum , is based on an hybrid learning approach to extract sentences for generating summary .
We discover hidden topic distributions of sentences in a given document cluster along with provided summary sentences based on hLDA described in ( Blei et al ., 2003a ).
We build a summary - focused hierarchical probabilistic topic model , sumHLDA , for each document cluster at sentence level , because it enables capturing expected topic distributions in given sentences directly from the model .
Besides , document clusters contain a relatively small number of documents , which may limit the variability of topics if they are evaluated on the document level .
As described in § 4 , we present a new method for scoring candidate sentences from this hierarchical structure .
Let a given document cluster D be represented with sentences O ={ om } l ° L 1 and its corresponding human summary be represented with sentences S ={ sn } lfi 1 .
All sentences are comprised of words V = { w \, w2 , ..
Please refer to ( Blei et al ., 2003b ) and ( Blei et al ., 2003a ) for details and demonstrations of topic models .
Summary hLDA ( sumHLDA ): The hLDA represents distribution of topics in sentences by organizing topics into a tree of a fixed depth L ( Fig . l . a ).
Each candidate sentence om is assigned to a path c0m in the tree and each word Wj in a given sentence is assigned to a hidden topic z0mat a level l of c0m .
Each node is associated with a topic distribution over words .
The sampler method alternates between choosing a new path for each sentence through the tree and assigning each word in each sentence to a topic along that path .
The structure of tree is learnt along with the topics using a nested Chinese restaurant process ( nCRP ) ( Blei et al .
, 2003a ), which is used as a prior .
The nCRP is a stochastic process , which assigns probability distributions to infinitely branching and infinitely deep trees .
In our model , nCRP specifies a distribution of words into paths in an L - level tree .
The assignments of sentences to paths are sampled sequentially : The first sentence takes the initial L - level path , starting with a single branch tree .
Later , mth subsequent sentence is assigned to a path drawn from the distribution : pipathotd , c \! m , mc ) pipathnew , c \ m , m , c ) path0id and pathnew represent an existing and novel ( branch ) path consecutively , mc is the number of previous sentences assigned to path c , m is the total number of sentences seen so far , and 7 is a hyper - parameter which controls the probability of creating new paths .
Based on this probability each node can branch out a different number of child nodes proportional to 7 .
Small values of 7 suppress the number of branches .
Summary sentences generally comprise abstract concepts of the content .
With sumHLDA we want to capture these abstract concepts in candidate sentences .
The idea is to represent each path shared by similar candidate sentences with representative summary sentence ( s ).
We let summary sentences share existing paths generated by similar candidate sentences instead of sampling new paths and influence the tree structure by introducing two separate hyper - parameters for nCRP prior : • if a summary sentence is sampled , use 7 = 7s , • if a candidate sentence is sampled , use 7 = 70 .
At each node , we let summary sentences sample a path by choosing only from the existing children of that node with a probability proportional to the number of other sentences assigned to that child .
This can be achieved by using a small value for 7s ( 0 < 7s 1 ).
We only let candidate sentences to have an option of creating a new child node with a probability proportional to y0 .
By choosing 7s << c 70 we suppress the generation of new branches for summary sentences and modify the 7 of nCRP prior in Eq .
( l ) using 7s and 70 hyperparameters for different sentence types .
In the experiments , we discuss the effects of this modification on the hierarchical topic tree .
The following is the generative process for sumHLDA used in our HybHSum : ( 1 ) For each topic k G T , sample a distribution ßk ^ Dirichlet ( n ).
( 2 ) For each sentence d G { O U S }, ( a ) if d G O , draw a path cd ^ nCRP ( 70 ), else if d G S , draw a path cd ^ nCRP ( 7s ).
( b ) Sample L - vector 6d mixing weights from Dirichlet distribution 6d ~ Dir ( a ).
Given sentence d , 6d is a vector of topic proportions from L dimensional Dirichlet parameterized by a ( distribution over levels in the tree .)
The nth word of d is sampled by first choosing a level zd , n = I from the discrete distribution 6d with probability 6d ) l . Dirichlet parameter rj and 70 control the size of tree effecting the number of topics .
( Small values of 7s do not effect the tree .)
Large values of r favor more topics ( Blei et al ., 2003a ).
Model Learning : Gibbs sampling is a common method to fit the hLDA models .
The aim is to obtain the following samples from the posterior of : ( i ) the latent tree T , ( ii ) the level assignment z for all words , ( iii ) the path assignments c for all sentences conditioned on the observed words w . Given the assignment of words w to levels z and assignments of sentences to paths c , the expected posterior probability of a particular word w at a given topic z = 1 of a path c = c is proportional to the number of times w was generated by that topic : Similarly , posterior probability of a particular topic z in a given sentence d is proportional to number of times z was generated by that sentence : n ( ) is the count of elements of an array satisfying the condition .
Note from Eq .
( 3 ) that two sentences d1 and d2 on the same path c would have different words , and hence different posterior topic probabilities .
Posterior probabilities are normalized with total counts and their hyperparameters .
4 Tree - Based Sentence Scoring The sumHLDA constructs a hierarchical tree structure of candidate sentences ( per document cluster ) by positioning summary sentences on the tree .
Each sentence is represented by a path in the tree , and each path can be shared by many sentences .
The assumption is that sentences sharing the same path should be more similar to each other because they share the same topics .
Moreover , if a path includes a summary sentence , then candidate sentences on that path are more likely to be selected for summary text .
In particular , the similarity of a candidate sentence om to a summary sentence sn sharing the same path is a measure of strength , indicating how likely om is to be included in the generated summary ( Algorithm 1 ): Let c0m be the path for a given om .
We find summary sentences that share the same path with Om via : M = { sn G S \ csn = C0m }.
The score of each sentence is calculated by similarity to the best matching summary sentence in M : If M = 0 , then score ( om )= 0 .
The efficiency of our similarity measure in identifying the best matching summary sentence , is tied to how expressive the extracted topics of our sumHLDA models are .
Given path c0m , we calculate the similarity of omto each sn , n = 1 .. \! M \!
by measuring similarities on : ★ sparse unigram distributions ( sim1 ) at each topic l on : similarity between p ( w0m ) i \! z0m = distributions of topic proportions ( sim2 ); similarity between p ( z0m \! c0m ) and p ( zsn \! c0m ).
— sim1 : We define two sparse ( discrete ) unigram distributions for candidate om and summary sn at each node l on a vocabulary identified with words generated by the topic at that node , vi C V . Given = { w1 , ..., wi [ 0m \), let w0m ) l C w0m be the set of words in om that are generated from topic z0m at level l on path c0m .
The discrete unigram distribution p0ml = P ( w0m , l \! z0m = l , , vi ) represents the probability over all words vl assigned to topic z0m at level l , by sampling only for words in w0m ) l . Similarly , Psn , l = p ( wsn , l \! zsn , C0m , Vl ) is the probability of words wsn in sn of the same topic .
The probability of each word in p0m ) l and psn ) l are obtained using Eq .
( 2 ) and then normalized ( see Fig . 1 . b ).
Algorithm 1 Tree - Based Sentence Scoring 1 : Given tree T from sumHLDA , candidate and summary The similarity between p0m ) l and psn ) l is obtained by first calculating the divergence with information radius - IR based on Kullback - Liebler ( KL ) divergence , p = P0m , l , Q = Psn , l : where , KL ( pjjq )= Y ,.
vi log ^.
Then the divergence is transformed into a similarity measure ( Manning IR is a measure of total divergence from the average , representing how much information is lost when two distributions p and q are described in terms of average distributions .
We opted for IR instead of the commonly used KL because with IR there is no problem with infinite values since Vi ++ qi = 0 if either pi = 0 or qj = 0 .
Moreover , unlike KL , IR is symmetric , i . e ., KL ( p , q )= KL ( q , p ).
Finally sim1 is obtained by average similarity of sentences using Eq .
( 6 ) at each level of c0m by : The similarity between p0m ) l and psn ) l at each level is weighted proportional to the level l because the similarity between sentences should be rewarded if there is a specific word overlap at child nodes .
— sim2 : We introduce another measure based on sentence - topic mixing proportions to calculate the concept - based similarities between om and sn .
We calculate the topic proportions of om and sn , represented by J ) Zom = p ( z0m \ c0m ) and pzSn = p ( zsn \! c0m ) via Eq .
The similarity between the distributions is then measured with transformed IR level - human research S -^\ incidence change ( Zl ) global Posterior Topic Distributions asjJ ^ eal , g (\ Apredict ( 7k \ temperatureyltforecast ( a ) Snapshot of Hierarchical Topic Structure of a document cluster on " global warming ".
( Duc06 ) om : " Globali warming - i may rise * incidencei of malarias ."
sn : " Globali warming - effects ^ humam healths ."
candidate om w1w5w6 w7 .... w5 .... _J_I I Vzl P ( Wsn i Izi , ) ( b ) Magnified view of sample path c [ zi , zi , Z3 ] showing Figure 1 : ( a ) A sample 3 - level tree using sumHLDA .
Each sentence is associated with a path c through the hierarchy , where each node z ;, c is associated with a distribution over terms ( Most probable terms are illustrated ).
( b ) magnified view of a path ( darker nodes ) in ( a ).
Distribution of words in given two sentences , a candidate ( om ) and a summary ( sn ) using sub - vocabulary of words at each topic vzi .
Discrete distributions on the left are topic mixtures for each sentence , pZom and pZsn .
sim \ provides information about the similarity between two sentences , om and sn based on topic - word distributions .
Similarly , sim2 provides information on the similarity between the weights of the topics in each sentence .
They jointly effect the sentence score and are combined in one measure : The final score for a given om is calculated from Eq .
Fig . 1 . b depicts a sample path illustrating sparse unigram distributions of om and sm at each level as well as their topic proportions , pZom , and pZsn .
In experiment 3 , we discuss the effect of our tree - based scoring on summarization performance in comparison to a classical scoring method presented as our baseline model .
5 Regression Model Each candidate sentence om , m = 1 .. \! O \!
is represented with a multi - dimensional vector of q features fm = {/ mi ,..., fmq }.
We build a regression model using sentence scores as output and selected salient features as input variables described below : 5 . 1 Feature Extraction We compile our training dataset using sentences from different document clusters , which do not necessarily share vocabularies .
Thus , we create n - gram mefa - features to represent sentences instead of word n - gram frequencies : ( I ) nGram Meta - Features ( NMF ): For each document cluster D , we identify most frequent ( non - stop word ) unigrams , i . e ., v / req = { wi } ri = 1 C V , where r is a model parameter of number of most frequent unigram features .
We measure observed unigram probabilities for each wi G v / req with pD = nD ( wi )/ Xj = i nD ( wj ), where no ( wi ) is the number of times wi appears in D and \! V \!
is the total number of unigrams .
For any ith feature , the value is / mi = 0 , if given sentence does not contain wi , otherwise / mi = pD ( wi ).
These features can be extended for any n - grams .
We similarly include bigram features in the experiments .
( II ) Document Word Frequency MetaFeatures ( DMF ): The characteristics of sentences at the document level can be important in summary generation .
DMF identify whether a word in a given sentence is specific to the document in consideration or it is commonly used in the document cluster .
This is important because summary sentences usually contain abstract terms rather than specific terms .
To characterize this feature , we re - use the r most frequent unigrams , i . e ., wi G v / req .
Given sentence om , let d be the document that om belongs to , i . e ., om G d . We measure unigram probabilities for each wi by p ( wi G om ) = nd ( wi G om )/ nD ( wi ), where nd ( wi G om ) is the number of times wi appears in d and nD ( wi ) is the number of times wi appears in D . For any ith feature , the value is / mi = 0 , if given sentence does not contain wi , otherwise / mi = p ( wi G om ).
We also include bigram extensions of DMF features .
Posterior Topic - Word Distributions summary sn middle - east ( III ) Other Features ( OF ): Term frequency of sentences such as SUMBASIC are proven to be good predictors in sentence scoring ( Nenkova and Vanderwende , 2005 ).
We measure the average unigram probability of a sentence by : p ( om ) = E „£ om jomrpD ( w ), where PD ( w ) is the observed unigram probability in the document collection D and \!
is the total number of words in om .
We use sentence bigram frequency , sentence rank in a document , and sentence size as additional features .
5 . 2 Predicting Scores for New Sentences Due to the large feature space to explore , we chose to work with support vector regression ( SVR ) ( Drucker et al ., 1997 ) as the learning algorithm to predict sentence scores .
Given training sentences { fm , ym }\!°= 1 , where / m = {/ mi , / mq } is a multi - dimensional vector of features and ym = score ( om ) G R are their scores obtained via Eq .
( 4 ), we train a regression model .
In experiments we use non - linear Gaussian kernel for SVR .
Once the SVR model is trained , we use it to predict the scores of ntest number of sentences in test ( unseen ) document clusters , CW = { oi , ... o \! otest \!}.
Our HybHSum captures the sentence characteristics with a regression model using sentences in different document clusters .
At test time , this valuable information is used to score testing sentences .
Redundancy Elimination : To eliminate redundant sentences in the generated summary , we incrementally add onto the summary the highest ranked sentence om and check if om significantly repeats the information already included in the summary until the algorithm reaches word count limit .
We use a word overlap measure between sentences normalized to sentence length .
A om is discarded if its similarity to any of the previously selected sentences is greater than a threshold identified by a greedy search on the training dataset .
6 Experiments and Discussions In this section we describe a number of experiments using our hybrid model on 100 document clusters each containing 25 news articles from DUC2005 - 2006 tasks .
We evaluate the performance of HybHSum using 45 document clusters each containing 25 news articles from DUC2007 task .
From these sets , we collected ^ 80K and ^ 25K sentences to compile training and testing data respectively .
The task is to create max .
250 word long summary for each document cluster .
We use Gibbs sampling for inference in hLDA and sumHLDA .
The hLDA is used to capture abstraction and specificity of words in documents ( Blei et al ., 2009 ).
Contrary to typical hLDA models , to efficiently represent sentences in summarization task , we set ascending values for Dirichlet hyper - parameter n as the level increases , encouraging mid to low level distributions to generate as many words as in higher levels , e . g ., for a tree of depth = 3 , n = { 0 . 125 , 0 . 5 , 1 }.
This causes sentences share paths only when they include similar concepts , starting higher level topics of the tree .
For SVR , we set e = 0 . 1 using the default choice , which is the inverse of the average of (/>( f ) T0 ( f ) ( Joachims , 1999 ), dot product of kernelized input vectors .
We use greedy optimization during training based on ROUGE scores to find best regular - izer C = { 10 - 1 .. 10 } using the Gaussian kernel .
We applied feature extraction of § 5 . 1 to compile the training and testing datasets .
ROUGE is used for performance measure ( Lin and Hovy , 2003 ; Lin , 2004 ), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries .
We use R - 1 ( recall against uni - grams ), R - 2 ( recall against bigrams ), and R - SU4 ( recall against skip - 4 bigrams ).
Experiment 1 : sumHLDA Parameter Analysis : In sumHLDA we introduce a prior different than the standard nested CRP ( nCRP ).
Here , we illustrate that this prior is practical in learning hierarchical topics for summarization task .
We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in document clusters .
Since summary sentences generally contain abstract words , they are indicative of sentences in documents and should produce minimal amount of new topics ( if not none ).
To implement this , in nCRP prior of sumHLDA , we use dual hyper - parameters and choose a very small value for summary sentences , 7s = 10e - 4 < C 7o .
We compare the results to hLDA ( Blei et al ., 2003a ) with nCRP prior which uses only one free parameter , 7 .
To analyze this prior , we generate a corpus of ^ 1300 sentences of a document cluster in DUC2005 .
We repeated the experiment for 9 other clusters of similar size and averaged the total number of generated topics .
We show results for different values of 7 and 7o hyper - parameters and tree depths .
Table 1 : Average # of topics per document cluster from sumHLDA and hLDA for different Y and Yo and tree depths .
Ys = 10e - 4 is used for sumHLDA for each depth .
Table 2 : ROUGE results ( with stop - words ) on DUC2006 for different features and methods .
Results in bold show statistical significance over baseline in corresponding metric .
As shown in Table 1 , the nCRP prior for sumHLDA is more effective than hLDA prior in the summarization task .
Less number of top - ics ( nodes ) in sumHLDA suggests that summary sentences share pre - existing paths and no new paths or nodes are sampled for them .
We also observe that using 7o = 0 . 1 causes the model to generate minimum number of topics (# of top - ics = depth ), while setting 7o = 10 creates excessive amount of topics .
70 = 1 gives reasonable number of topics , thus we use this value for the rest of the experiments .
In experiment 3 , we use both nCRP priors in HybHSum to analyze whether there is any performance gain with the new prior .
Experiment 2 : Feature Selection Analysis Here we test individual contribution of each set of features on our HybHSum ( using sumHLDA ).
We use a Baseline by replacing the scoring algorithm of HybHSum with a simple cosine distance measure .
The score of a candidate sentence is the cosine similarity to the maximum matching summary sentence .
Later , we build a regression model with the same features as our HybHSum to create a summary .
We train models with DUC2005 and evaluate performance on DUC2006 documents for different parameter values as shown in Table 2 .
As presented in § 5 , NMF is the bundle of frequency based meta - features on document cluster level , DMF is a bundle of frequency based metafeatures on individual document level and OF represents sentence term frequency , location , and size features .
In comparison to the baseline , OF has a significant effect on the ROUGE scores .
In addition , DMF together with OF has shown to improve all scores , in comparison to baseline , on average by 10 %.
Although the NMF have minimal individual improvement , all these features can statistically improve R - 2 without stop words by 12 % ( significance is measured by t - test statistics ).
Experiment 3 : ROUGE Evaluations We use the following multi - document summarization models along with the Baseline presented in Experiment 2 to evaluate HybSumm .
★ PYTHY : ( Toutanova et al ., 2007 ) A state - of - the - art supervised summarization system that ranked first in overall ROUGE evaluations in DUC2007 .
Similar to HybHSum , human generated summaries are used to train a sentence ranking system using a classifier model .
★ HIERSUM : ( Haghighi and Vanderwende , 2009 ) A generative summarization method based on topic models , which uses sentences as an additional level .
Using an approximation for inference , sentences are greedily added to a summary so long as they decrease KL - divergence .
★ HybFSum ( Hybrid Flat Summarizer ): To investigate the performance of hierarchical topic model , we build another hybrid model using flat is a superposition of all K topics with sentence specific weights , there is no hierarchical relation between topics .
We keep the parameters and the features of the regression model of hierarchical HybHSum intact for consistency .
We only change the sentence scoring method .
Instead of the new tree - based sentence scoring (§ 4 ), we present a similar method using topics from LDA on sentence level .
Note that in LDA the topic - word distributions 0 are over entire vocabulary , and topic mixing proportions for sentences 6 are over all the topics discovered from sentences in a document cluster .
Hence , we define sim1 and sim2measures for LDA using topic - word proportions 0 ( in place of discrete topic - word distributions from each level in Eq . 2 ) and topic mixing weights 6 in sentences ( in place of topic proportions in Eq . 3 ) respectively .
Maximum matching score is calculated as same as in HybHSum .
★ HybHSum1 and HybHSum2 : To analyze the effect of the new nCRP prior of sumHLDA on sum - ( a ) Ref .
Output ( b ) HybHSum2 Output Table 3 : ROUGE results of the best systems on DUC2007 dataset ( best results arebolded .)
marization model performance , we build two different versions of our hybrid model : HybHSum1using standard hLDA ( Blei et al ., 2003a ) and HybHSum2 using our sumHLDA .
The ROUGE results are shown in Table 3 .
The HybHSum2 achieves the best performance on R - 1 and R - 4 and comparable on R - 2 .
When stop words are used the HybHSum2 outperforms state - of - the - art by 2 . 5 - 7 % except R - 2 ( with statistical significance ).
Note that R - 2 is a measure of bigram recall and sumHLDA of HybHSum2 is built on unigrams rather than bigrams .
Compared to the HybFSum built on LDA , both HybHSum1 & 2yield better performance indicating the effectiveness of using hierarchical topic model in summarization task .
HybHSum2 appear to be less redundant than HybFSum capturing not only common terms but also specific words in Fig .
2 , due to the new hierarchical tree - based sentence scoring which characterizes sentences on deeper level .
Similarly , HybHSum1 & 2 far exceeds baseline built on simple classifier .
The results justify the performance gain by using our novel tree - based scoring method .
Although the ROUGE scores for HybHSum1 and HybHSum2 are not significantly different , the sumHLDA is more suitable for summarization tasks than hLDA .
HybHSum2 is comparable to ( if not better than ) fully generative HIERSUM .
This indicates that with our regression model built on training data , summaries can be efficiently generated for test documents ( suitable for online systems ).
Experiment 4 : Manual Evaluations Here , we manually evaluate quality of summaries , a common DUC task .
Human annotators are given two sets of summary text for each document set , generated from two approaches : best hierarchical hybrid HybHSum2 and flat hybrid HybFSum models , and are asked to mark the better summary The Agriculture Department began to propose standards for all organic foods in the late 1990 ' s because their sale had grown more than 20 per cent a year in that decade .
In January 1999 the USDA approved a " certified organic " label for meats and poultry that were raised without growth hormones , pesticide - treated feed , and antibiotics .
New federal rules for organic food will assure consumers that the products are grown and processed to the same standards nationwide .
But as sales grew more than 20 percent a year through the 1990s , organic food came to account for $ 1 of every the agency took notice , proposing national organic standards for all food .
( c ) HybFSum Output By the year 2001 , organic products are projected to command 5 percent of total food sales in the United States .
The sale of organics rose by about 30 percent last year , driven by concerns over food safety , the environment and a fear of genetically engineered food .
U . S . sales of organic foods have grown by 20 percent annually for the last seven years .
organic genetic allow agriculture standard sludge federal bar certified Figure 2 : Example summary text generated by systems compared in Experiment 3 .
Ref .
is the human generated summary .
Table 4 : Frequency results of manual quality evaluations .
Results are statistically significant based on t - test .
Tie indicates evaluations where two summaries are rated equal .
according to five criteria : non - redundancy ( which summary is less redundant ), coherence ( which summary is more coherent ), focus and readability ( content and not include unnecessary details ), responsiveness and overall performance .
We asked 4 annotators to rate DUC2007 predicted summaries ( 45 summary pairs per anno - tator ).
A total of 92 pairs are judged and evaluation results in frequencies are shown in Table 4 .
The participants rated HybHSum2 generated summaries more coherent and focused compared to HybFSum .
All results in Table 4 are statistically significant ( based on t - test on 95 % confidence level .)
indicating that HybHSum2 summaries are rated significantly better .
Document Cluster 1 Document Cluster Document Clustern candidate sentence scores candidate sentence scores •• • candidate sentence scores ~ g = fH \!
y - output f - input features M f - input features k f - input features h ( f , y ) : regression model for sentence ranking Figure 3 : Flow diagram for Hybrid Learning Algorithm for Multi - Document Summarization .
7 Conclusion In this paper , we presented a hybrid model for multi - document summarization .
We demonstrated that implementation of a summary focused hierarchical topic model to discover sentence structures as well as construction of a discriminative method for inference can benefit summarization quality on manual and automatic evaluation metrics .
Acknowledgement Research supported in part by ONR N00014 - 02 - 1 - 0294 , BT Grant CT1080028046 , Azerbaijan Ministry of Communications and Information Technology Grant , Azerbaijan University of Azerbaijan Republic and the BISC Program of UC Berkeley .
