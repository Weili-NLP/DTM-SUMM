Query - focused Multi - Document Summarization : Combining a Topic Model with Graph - based Semi - supervised Learning Graph - based learning algorithms have been shown to be an effective approach for query - focused multi - document summarization ( MDS ).
In this paper , we extend the standard graph ranking algorithm by proposing a two - layer ( i . e .
sentence layer and topic layer ) graph - based semi - supervised learning approach based on topic modeling techniques .
Experimental results on TAC datasets show that by considering topic information , we can effectively improve the summary performance .
1 Introduction Query - focused multi - document summarization ( MDS ) can facilitate users to grasp the main idea of the documents according to the users ' concern .
In query - focused summarization , one query is firstly proposed at the beginning of the documents .
Then according to the given query and its influence on sentences , a ranking score is assigned to each of the sentences and higher ranked sentences are picked into a summary .
Among existing approaches , graph - based semi - supervised learning algorithms have been shown to be an effective way to impose a query ' s influence on sentences ( Zhou et al , 2003 ; Zhou et al , 2004 ; Wan et al , 2007 ).
Specifically , a weighted network is constructed where each sentence is modeled as a node and relationships between sentences are modeled as directed or undirected edges .
With the assumption that a query is the most important node , initially , a positive score is assigned to the query and zero to the remaining nodes .
All nodes then spread their ranking scores to their nearby neighbors via the weighted network .
This spreading process is repeated until a global stable state is achieved , and all nodes obtain their final ranking scores .
The primary disadvantage of existing learning method is that sentences are ranked without considering topic level information .
As we know , a collection of related documents usually covers a few different topics .
For example , the specific event " Quebec independence " may involve the topics such as " leader in independence movement ", " referendum ", " related efforts in independence movement " and so on .
It is important to discover the latent topics when summarizing a document collection , because sentences in an important topic would be more important than those talking about trivial topics ( Hardy et al , 2002 ; Harabagiu and Lacatusu , 2005 ; Otterbacher et al , 2005 ; Wan and Yang , 2008 ).
The topic models ( Blei et al , 2003 ) offer a good opportunity for the topic - level information modeling by offering clear and rigorous probabilistic interpretations over other existing clustering techniques .
So far , LDA has been widely used in summarization task by discovering topics latent in the document collections ( Daume and Marcu , 2006 ; Haghighi and Vanderwende , 2009 ; Jin et al , 2010 ; Mason and Charniak , 2011 ; Delort and Alfonseca , 2012 ).
However , as far as we know , how to combine topic information and semi - supervised learning into a unified framework has seldom been exploited .
In this paper , inspired by the graph - based semi - supervised strategy and topic models , we propose a two - layer ( i . e .
sentence layer and topic layer ) graph - based semi - supervised learning approach for correspondence author This work is licensed under a Creative Commons Attribution 4 . 0 International Licence .
Page numbers and proceedings footer are added by the organisers .
Licence details : http :// creativecommons . org / licenses / by / 4 . 0 / query - focused MDS .
By using two revised versions of LDA topic model ( See Section 2 ), our approach naturally models the relations between topics and sentences , and further use these relations to construct the two - layer graph .
Experiments on the TAC datasets demonstrate that we can improve summarization performance under the framework of two - layer graph - based semi - supervised learning .
The rest of this paper is organized as follows : Section 2 describes our LDA based topic models , W - LDA and S - LDA .
Section 3 presents the construction of the two - layer graph and the semi - supervised learning and the experimental results are provided in Section 4 .
Then , Section 5 describes related work on query - focused multi - document summarization and topic modeling techniques and we conclude this paper in Section 6 .
2 Topic Modeling 2 . 1 Model Description As discussed in Section 1 , a collection of documents often involves different topics related to a specific event .
The basic idea of our summarization approach is to discover the latent topics and cluster sentences according to the topics .
Inspired by ( Chemudugunta et al , 2006 ) and ( Li et al , 2011 ), we find 4 types of words in the text : ( 1 ) Stop words that occur frequently in the text .
( 2 ) Background words that describe the general information about an event , such as " Quebec " and " independence ".
( 3 ) Aspect words talking about topics across the corpus .
( 4 ) Document - specific words that are local to a single document and do not appear across different corpus .
Similar ideas can also be found in many LDA based summarization techniques ( Haghighi and Vanderwende , 2009 ; Li et al , 2011 ; Delort and Alfonseca , 2012 ).
Stop words can easily be filtered out by a standard list of stopwords .
We use a background word distribution <\> b to model vocabularies commonly used in the document collection .
We assume that there are K aspect topics shared across corpus and each topic is associated with a topic - word distribution 4 > k , k G [ 1 , K \.
For each document m , there is a document - specific word distribution < fim , m e [ K + 1 , K + M ].
Each word w is modeled as a mixture of background topics , document - specific topics or aspect topics .
We use a latent parameter yw to denote whether it is a background word , a document - specific word or an aspect word .
yw is sampled from a multinomial distribution with parameter tt .
2 . 2 W - LDA and S - LDA We describe two models : a word level model W - LDA and a sentence level S - LDA .
Their difference only lies in whether the words within a sentence are generated from the same topic .
W - LDA : Figure 1 and Figure 3 show the graphical model and generation process of W - LDA , which is based on Chemudugunta et al ' s work ( 2007 ).
Using the Gibbs sampling technique , in each iteration two latent parameters yw and zw are sampled simultaneously as follows : where Nmo - w , Nm \- w and Nm2 - w denote the number of words assigned to background , document - specific and aspect topic in current document .
Nm >- W denotes the total number of words in current document .
Eß , E ^ and E ™ are the number of times that word w appears in background topic , document - specific topic and aspect topic k . denotes the number of words assigned to topic k in current document .
With one Gibbs sampling , we can make the following estimation : Then , the probability that a sentence s is generated from topic k is computed based on the probability that each of its aspect words is generated from topic k : 1 .
Draw background distribution < f > B ~ Dir ( X ) 2 .
For each document m : draw doc proportion vector 6m ~ Dir { a ) draw doc proportion vector nm ~ Dir {^) draw doc specific distribution 0m ~ Dir ( X ) 3 .
For each topic fc : draw topic distribution 0 *.
~ Dir ( X ) 4 .
For each word «; in document m : ( a ) draw yw ~ Multi { iim ) draw doc proportion vector 6m ~ Dir { ot ) draw doc proportion vector 7rTO ~ Dir {^) draw doc specific distribution 0m ~ Dir ( X ) Figure 3 : Generation process for W - LDA 4 .
For each sentence s in document m : 4 . 2 for each word in sentence s : ( a ) draw yw ~ Multi { nm ) Figure 4 : Generation process of S - LDA S - LDA : In S - LDA , each sentence is treated as a whole and words within a sentence are generated from the same topic ( Gruber et al ., 2007 ).
Its graphical model and generated process are shown in Figure 2 and Figure 4 .
In S - LDA , we firstly sample the topic zs for each sentence as follows : C ^ j , denotes the number of sentences in document m assigned to topic k . denotes the number of aspect words in current sentence .
Then yw is sampled .
In our experiments , we set hyperparameters a = 1 , /?
We run 500 burn - in iterations through all documents in the collection to stabilize the distribution of z and y before collecting samples .
3 Graph - based Semi - supervised Learning As stated before , the consideration of higher level information ( i . e .
topics ) would be helpful for sentence ranking in summarization .
In our two - layer graph , the upper layer is composed of topic nodes and the lower layer is composed of sentences nodes , among which there is one node representing the query .
Formally , given a document set D , let G =< Vs , Vt , E > be the two - layer graph , where Vs = { s \, s2 , ..., sn } denotes the set of all the sentence nodes and s \ is the query .
Vt = { z \, z2 , zk } corresponds to all the topic nodes .
The collection of edges E in the graph consists of the relations within layers and between layers .
And the edge weights are measured according to the similarities between nodes , which are computed based on the topic distribution from our two topic model extensions .
Specifically , we introduce four edge weight matrices Wn * k > Wk * n > U and P to describe the sentence - to - topic relations , the topic - to - sentence relations , the sentence - to - sentence relations and the topic - to - topic relations respectively .
Firstly , the row - normalized edge weight matrices Wn * k and Wk * n denotes the similarity matrix between sentences and topics , where sim ( si , Zk ) = p { si \ zSi = zk ) is the probability that the sentence is generated from that topic calculated in Equation ( 5 ).
The edge weight matrix U describe the sentence - to - sentence relations .
In the same way , the similarity between two sentences is the cosine similarity between their topic distributions , sim ( si , Sj ) = Th T , k ' P ( si \ zsi = k ) ■ p ( sj \ zSj = zk ), where Cx = ^/ YJkP ( si \ zsi = k )^ T , kP ( sj \ zsj = k ) is the normalized factor .
Since the row - normalization process will make the sentence - to - sentence relation matrix asymmetric , we adopt the following strategy : let Sim ( s ) denote the similarity matrix between sentences , where Sim ( s )( i , j ) = sim ( si , Sj ) and D denotes the diagonal matrix with ( i , i )- element equal to the sum of the ith row of Sim ( s ).
Edge weight matrix between sentences U is calculated as follows : Then , the edge weight matrix between topics P is the normalized symmetric matrix of the similairty matrix between two topics .
The cosine similarity between two topics is calculated according to word - topic distribution .
where C2 = \/ J2wP ( w \ zi ) ' V ^ wP ( w \ zj ) ^ s the normalized factor .
We further transform the task to an optimizing problem based on the assumption that closely related nodes ( sentences and topics ) tend to have similar scores .
So we would give more penalty for the difference between closely related nodes with regard to edge weight matrices Wn * k > Wk * n , U and P . This motivates the following optimization function Q ( f , g ) in Equation ( 10 ) similar to the graph harmonic function ( Zhu et al , 2003 ).
/ denotes the sentence score vector and g denotes the topic score vector .
Intuitively , Q ( f , g ) measures the sum of difference between graph nodes ; the more they differ , the larger Q ( f , g ) would be .
The score vectors can be achieved by minimizing the function in Equation ( 10 ).
That is , (/, g )= argminj flQ (/, g ).
We can get the following equations ( details are shown in Appendix ).
Equation ( 11 ) conforms to our intuition : ( 1 ) A sentence would be important if it is heavily connected with many important sentences and a topic would be important if it is closely related to other important topics .
( 2 ) A sentence would be important if it is expressing an important topic , and in turn a topic would be important if it is referred by an important sentence .
Based on Equation ( 11 ), the ranking algorithm is designed in a semi - supervised way , where the score of the labeled query is fixed to the largest score of 1 during each iteration , as shown in Figure 5 .
Then , our algorithm iteratively calculates the score of topics and sentences until convergence .
Input : The sentence set { s \, s2 , %}, topic set { z \, z2 , zk }, edge weight matrix W , W , U and P . s i is the query .
Output : Sentence score vector / and topic score vector g . 1 .
Initialization , k = 0 : 2 .
Update sentence score vector 3 .
Update topic score vector 4 . nx tne score oi query in / " * ro i .
5 . k = k + l Go to Step 2 until convergence .
END _ Figure 5 : Sentence Ranking Algorithm Input : The sentence set S = { s \, s2 , sn }, sentence score vector / Output : Summary Y .
BEGIN : 1 .
Initialization : Y = $, X = { S - Si }.
2 . while word num is less than 100 : ( a ) sm = argmaxs . ex f ( si ) Figure 6 : Sentence Selection Algorithm 3 . 1 Summary Generation Sentence compression can largely improve summarization quality ( Zajic et al , 2007 ; Peng et al , 2011 ).
Since sentence compression is not the main task in this paper , we just use the revised sentence compression techniques in ( Li et al , 2011 ).
Here , we remove the redundant modifiers such as adverbials , relative clause modifiers , abbreviations , participials and infinitive modifiers for each sentence .
As for the sentence selection process , sentences with higher ranking score are selected into the summary .
Then Maximum Marginal Relevance ( MMR )( Goldstein et al , 1999 ) is further used for redundancy removal .
We just apply a simple greedy algorithm for sentence selection as shown in Figure 6 .
We use Y to denote the summary set which contains the selected summary sentences .
The algorithm first initializes Y to $ and X as the set { S — s \}.
During each iteration , we select the highest ranked sentence Sj from the sentence set X .
We need to assure that the value of semantic similarity between two sentences is less than Thsem - Thsem denotes the threshold for the cosine similarity between two sentences and is set to 0 . 5 in our model .
4 Experiments The query - focused MDS task defined in TAC ( Text Analysis Conference ) evaluations requires generating a concise and well organized summary for a collection of related documents according to a given query .
The query usually consists of a narrative / question sentence .
Our experiment data is composed of TAC ( 2008 - 2009 ) data , which contain 48 and 44 document collections respectively .
We use docset - A data sets in TAC which has 10 documents per collection .
The average numbers of sentences per document in TAC2008 and TAC2009 are 252 and 243 respectively , and the system - generated summary is limited to 100 words .
It is noted that the corpus of TAC2008 and TAC2009 are similar .
In our experiment , we apply the optimal topic number trained on TAC2008 dataset to TAC2009 dataset .
our experiments , if \ ff - f £+ 1 \ < 0 . 0001 ( 1 < i < N ) and \ gï - g ^+ 1 \ < 0 . 0001 ( 1 < i < T ), iteration stops .
TAC data sets are for the update summarization tasks , where the summarization for docset - A can be seen the query - focused summarization task referred in this paper .
As for evaluation metrics , we use ROUGE ( Recall - Oriented Understudy for Gisting Evaluation ) ( Lin , 2004 ) measures .
ROUGE measures summary quality by counting overlapping units such as the n - gram , word sequences and word pairs between the candidate summary and the reference summary .
We report ROUGE - 1 , ROUGE - 2 , and ROUGE - SU4 scores and their corresponding 95 % confidential intervals , to evaluate the performance of the system - generated summaries .
As a preprocessing step , stopwords are firstly removed with a list of 598 stop words and the remaining words are then stemmed using PorterStemmer .. 4 . 1 Parameter Tuning There are two parameters to tune in our model .
The first parameter is a in Equation ( 11 ) that controls the tradeoff between influence from topics and from sentences .
The second one is the topic number K in LDA topic model .
The combination of the two factors makes it hard to find a global optimized solution .
So we apply a gradient search strategy .
At first , parameter a is fixed to a given value .
Then the performance of using different topic numbers is evaluated .
After that , we fix the topic number to the value which has achieved the best performance , and conduct experiments to find an appropriate value for a .
Here , we use TAC2008 as training data and test our model on TAC2009 .
First , a is set to 0 . 5 , then we change topic number K from 2 to 20 at the interval of 2 .
The ROUGE score reaches their peaks when the topic number is around 12 , as shown in Figure 7 ( 1 ) and Figure 7 ( 2 ).
Then we fix the number of K to 12 and change the value of parameter a from 0 to 1 with the interval of 0 . 1 .
When the value of a is set to 0 , the model degenerates into a one - layer graph ranking algorithm where topic clustering information is neglected .
As we can see from Figure 7 ( 3 ) and Figure 7 ( 4 ) , the ROUGE scores reach their peaks around 0 . 6 and then drop afterwards .
Thus , the topic number is set to 12 and a is set to 0 . 6 in the test dataset .
Jackknife scoring for ROUGE is used in order to compare with the human summaries .
4 http :// tartarus . org / martin / PorterStemmer / 4 . 2 Baseline Comparison We firstly compare W - LDA and S - LDA with other clustering approaches .
To be fair , we use the identical sentence compression techniques and preprocessing methods for all baselines .
Summaries are truncated to the same length of 100 words .
Standard - LDA : A simplified version of W - LDA without considering the background or document - specific information .
K - means : Using the K - means clustering algorithm for graph construction .
We firstly randomly select K sentences as initial centroid for clusters and then iteratively assign a sentence to each cluster .
The centroid is recomputed until convergence .
The similarity between nodes in the graph ( sentence or cluster ) is computed using the standard cosine measure based on the tf - idf information .
K is set to 12 , the same as topic number in LDA .
Agglomerative : a bottom - up hierarchical clustering algorithm and starts with the sentences as individual clusters and , at each step , merges the most similar or closest pair of clusters , until the number of the clusters reduces to the desired number K = 12 .
Divisive : a top - down hierarchical clustering algorithm and starts with one , all - inclusive cluster and , at each step , splits the largest cluster until the number of clusters increases to the desired number K , K = 12 .
Table 1 : Comparison with other clustering baselines .
Table 1 presents the performance of different clustering algorithms for summarization .
Traditional clustering algorithms such as K - means , Agglomerative and Divisive clustering achieve comparative results .
Compared with traditional clustering algorithms , LDA based models ( W - LDA , S - LDA , Standard - LDA ) achieve better results .
This can be explained by the clear and rigorous probabilistic interpretation of topic models .
Background information and document - specific information would influence the performance of topic modeling ( Chemudugunta et al , 2006 ), that is why S - LDA and W - LDA achieve better ROUGE performance than the standard LDA .
We can also see that S - LDA is slightly better than W - LDA in regard with ROUGE performance .
The reason can be explained as follows : The aim of topic modeling in this task is to cluster sentences according to their topics .
So treating sentence as a unit in topic modeling would be better than treating it as a set of independent words .
In addition , forcing the words in one sentence to share the same aspect topic can ensure semantic cohesion of the mined topics .
Next , we compare our model with the following widely used summarization approaches .
Manifold : One - layer graph - based semi - supervised approach developed by Wan et al .( 2008 ).
Sentence relations are calculated according to tf — idf and topic information is neglected .
LexRank : An unsupervised graph - based summarization approach ( Erkan and Radev , 2004 ), which is a revised version of the famous web ranking algorithm PageRank .
KL - Divergence : The approach developed by ( Lin et al , 2006 ) by using a KL - divergence based sentence selection strategy .
where Ps is the unigram distribution of candidate summary and Qd denotes the unigram distribution of document collection .
Since this approach is designed for general summarization , query influence is not considered .
Hiersum : A LDA based approach proposed by ( Haghighi and Vanderwende , 2009 ), where unigram distribution is calculated from LDA topic model in Equation ( 12 ).
MEAD : A centroid based summary algorithm by ( Radev et al , 2004 ).
Cluster centroids in MEAD consists of words which are central not only to one article in a cluster , but to all the articles .
Similarity is measured by using tf — idf .
Performance is presented at Table 2 .
We can find that ROUGE performance of one - layer graph ranking algorithms such as Manifold and LexRank , where topic information is neglected , achieve worse results than all two - layer models where topic information is considered ( See Table 1 ).
This verifies our previous claim ( Hardy et at ., 2002 ; Harabagiu and Lacatusu , 2005 ; Wan and Yang , 2008 ) that the consideration of topic information will improve summarization performance .
S - LDA and W - LDA achieve better performance than KL - divergence and Hiersum .
This is because the sentence selection strategy for KL - divergence and Hiersum tries to select sentence best representing the document as shown in Equation ( 12 ), but do not consider the influence of query .
4 . 3 Manual Evaluation W - LDA and S - LDA get comparative ROUGE scores .
To obtain a more accurate measure to decide which approach is better , we perform a simple user study concerning the following aspects on 40 randomly selected topics in TAC2009 : ( 1 ) Overall quality .
( 2 ) Focus : Whether the summary contains less irrelevant content ?
( 3 ) Responsiveness : Whether the summary is responsive to the query .
( 4 ) Non - Redundancy : Whether the summary is non - redundant .
Each respect is rated from 1 ( very poor ) to 5 ( very good ).
Four native speakers who are Ph . D . students in computer science ( none are authors ) performed the task .
The average score and standard deviation for W - LDA and S - LDA are displayed in Table 3 .
We can see that the two models almost tie in foucs and non - redundancy .
This is because two models use the same sentence selection strategy based on MMR for redundancy removal and propagation model to impose the query ' s influence on sentences .
S - LDA outperforms W - LDA in overall ranking and responsiveness ranking .
This implies that treating sentence as a unit in topic modeling would be preferable to just treating it as a series of independent words .
5 Related Work Graph - based ranking approaches have been hot these days for both generic and query - focused summarization ( Zhou et al , 2003 ; Zhou et al , 2004 ; Erkan and Radev , 2004 ; Wan et al , 2007 ; Wei et al , 2008 ).
Commonly used graph - based ranking algorithms are mainly inspired by the link analysis algorithm in web research such as PageRank ( Page et al , 1999 ).
( Wan et al , 2007 ) proposed the approach that treated the task of query - focused MDS as a semi - supervised learning task , in which the query is treated as a labeled node , and sentences as unlabeled nodes .
Then the scores of sentences are determined from the manifold learning algorithm proposed by ( Zhou et al , 2003 ) or the harmonic approach proposed by ( Zhu et al , 2003 ).
Table 2 : Performance comparison with baselines Table 3 : Manual evaluation for S - LDA and W - LDA .
It is worthy of noting that researchers have found that by considering topic level information , the summarization performance can be effectively improved ( Hardy et al , 2002 ; Wan and Yang , 2008 ; Harabagiu and Lacatusu , 2005 ).
For example , ( Otterbacher et al , 2005 ) models documents as a stochastic graph and calculates sentence ranking scores with a topic - sensitive version of PageRank .
( Wan and Yang , 2008 ) developed a two - layer graph by clustering sentences by using standard clustering algorithms such as K - means or agglomerate clustering .
However , his algorithm is for general summarization where the influence of query is not considered .
A significant portion of recent work incorporates LDA topic models ( Blei et al , 2008 ) in summarization tasks for their clear and rigorous probabilistic topic interpretations ( Daume and Marcu , 2006 ; Titov and McDonald , 2008 ; Haghighi and Vanderwende , 2009 ; Mason and Charniak , 2011 ; Li et al , 2013a ; Li et al , 2013b ).
( Haghighi and Vanderwende , 2009 ) introduced a LDA based model called Hiersum to find the subtopics or aspects by combining KL - divergence criterion for selecting relevant sentences .
AYESSUM ( Daume and Marcu , 2006 ) and the Special Words and Background model ( Chemudugunta et al , 2006 ) are very similar to Hiersum .
In the same way , ( Delort and Alfonseca , 2012 ) tried to use LDA to model different levels of information for novelty detection in update summarization .
Furthermore , ( Paul and Dredze , 2013 ) extends their f - LDA to jointly model combinations of drug , aspect and route of administration as an exploratory tool for extractive summarization .
6 Conclusions and Future Work In this paper , we propose a two - layer graph - based semi - supervised algorithm for query - focused MDS .
Topic modeling techniques are used for sentence clustering and further graph construction .
By considering different kinds of information such as background or document - specific information , our two LDA topic model extensions achieve better results than traditional clustering algorithms .
One primary disadvantage of our models is that it is hard to decide the topic number K in LDA models and how to define topic number is still a open problem in LDA topic models .
From Figure 7 , we can see that summarization performance is sensitive to topic number .
We train the value of topic number on TAC2008 dataset and test the model on TAC2009 .
Such process makes sense because the corpus sizes and contents of two datasets are similar .
But it would be hard to extend optimal topic number in TAC2008 to other datasets .
Using non - parametric topic modeling techniques where topic number does not have to be predefined is one of our future works .
Acknowledgements Thanks Jiwei Li for the insightful reviews and careful polishment .
We also thank the three anonymous reviewers for their helpful comments .
This work was partially supported by National High Technology Research and Development Program of China ( No .
2012AA011101 ), National Key Basic Research Program of China ( No .
2014CB340504 ), National Natural Science Foundation of China ( No .
61273278 ), and National Key Technology R & D Program ( No : 2011BAH10B04 - 03 ).
APPENDIX To optimize g ), shown in Equation ( 10 ), we set the partial derivative with respect to fm to 0 , for m G [ 1 , N ].
Let ömn denote the index function as follows : So we have : f = aUf + ^( l - a )( W + WT ) g A similar approach is used to obtain the second part of Equation ( 11 ).
