Leveraging Effective Query Modeling Techniques for Speech Recognition and Summarization Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .
In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo - relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .
This paper presents a continuation of such a general line of research and the main contribution is threefold .
First , we propose a principled framework which can unify the relationships among several widely - used query modeling formulations .
Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query - specific information cues to guide the model estimation .
Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .
A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .
1 Introduction Along with the rapidly growing popularity of the Internet and the ubiquity of social web communications , tremendous volumes of multimedia contents , such as broadcast radio and television programs , digital libraries and so on , are made available to the public .
Research on multimedia content understanding and organization has witnessed a booming interest over the past decade .
By virtue of the developed techniques , a variety of functionalities were created to help distill important content from multimedia collections , or provide locations of important speech segments in a video accompanied with their corresponding transcripts , for users to listen to or to digest .
Statistical language modeling ( LM ) ( Jelinek , 1999 ; Jurafsky and Martin , 2008 ; Zhai , 2008 ), which manages to quantify the acceptability of a given word sequence in a natural language or capture the statistical characteristics of a given piece of text , has been proved to offer both efficient and effective modeling abilities in many practical applications of natural language processing and speech recognition ( Ponte and Croft , 1998 ; Jelinek , 1999 ; Huang , et al ., 2001 ; Zhai and Lafferty , 2001a ; Jurafsky and Martin , 2008 ; Furui et al ., 2012 ; Liu and Hakkani - Tur , 2011 ).
The LM approach was first introduced for the information retrieval ( IR ) problems in the late 1990s , indicating very good potential , and was subsequently extended in a wide array of follow - up studies .
One typical realization of the LM approach for IR is to access the degree of relevance between a query and a document by computing the likelihood of the query generated by the document ( usually referred to as the query - likelihood approach ) ( Zhai , 2008 ; Baeza - Yates and Ribeiro - Neto , 2011 ).
A document is deemed to be relevant to a given query if the corresponding document model is more likely to generate the query .
On the other hand , the Kullback - Leibler divergence measure ( denoted by KLM for short hereafter ), which quantifies the degree of relevance between a document and a query from a more rigorous information - theoretic perspective , has been proposed ( bLafferty and Zhai , 2001 ; Zhai and Lafferty , 2001b ; Baeza - Yates and Ribeiro - Neto , 2011 ).
KLM not only can be thought as a natural generalization of the query - likelihood approach , but also has the additional merit of being able to accommodate extra information cues to improve the performance of document ranking .
For example , a main challenge facing such a measure is that since a given query usually consists of few words , the true information need is hard to be inferred from the surface statistics of a query .
As such , one emerging stream of thought for KLM is to employ the pseudo - relevance feedback process to construct an enhanced query model ( or representation ) so as to achieve better retrieval effectiveness ( Hi - emstra et al ., 2004 ; Lv and Zhai , 2009 ; Carpineto and Romano , 2012 ; Lee and Croft , 2013 ).
Following this line of research , the major contribution of this paper is three - fold : 1 ) we analyze several widely - used query models and then propose a principled framework to unify the relationships among them ; 2 ) on top of the successfully developed query models , we propose an extended modeling formulation by incorporating additional query - specific information cues to guide the model estimation ; 3 ) we explore a novel use of these query models by adapting them to the speech recognition and summarization tasks .
As we will see , a series of experiments indeed demonstrate the effectiveness of the proposed models on these two tasks .
2 Language Modeling Framework 2 . 1 Kullback - Leibler Divergence Measure A promising realization of the LM approach to IR is the Kullback - Leibler divergence measure ( KLM ), which determines the degree of relevance between a document and a query from a rigorous information - theoretic perspective .
Two different language models are involved in KLM : one for the document and the other for the query .
The divergence of the document model with respect to the query model is defined by KLM not only can be thought as a natural generalization of the traditional query - likelihood approach ( Yi and Allan , 2009 ; Baeza - Yates and Ribeiro - Neto , 2011 ), but also has the additional merit of being able to accommodate extra information cues to improve the estimation of its component models in a systematic way for better document ranking ( Zhai , 2008 ).
Due to that a query usually consists of only a few words , the true query model P ( w \ Q ) might not be accurately estimated by the simp le ML estimator ( Jelinek , 1991 ).
There are several studies devoted to estimating a more accurate query modeling , saying that it can be approached with the pseudo - relevance feedback process ( Lavren - ko and Croft , 2001 ; Zhai and Lafferty , 2001b ).
However , the success depends largely on the assumption that the set of top - ranked documents , DTop ={ D ;, D2 ,..., Dr ,...}, obtained from an initial round of retrieval , are relevant and can be used to estimate a more accurate query language model .
2 . 2 Relevance Modeling Under the notion of relevance modeling ( RM , often referred to as RM - 1 ), each query Q is assumed to be associated with an unknown relevance class Rq , and documents that are relevant to the semantic content expressed in query are samples drawn from the relevance class Rq .
Since there is no prior knowledge about Rq , we may use the top - ranked documents DTop to approximate the relevance class Rq .
The corresponding relevance model can be estimated using the following equation ( Lavrenko and Croft , 2001 ; Lavrenko , 2004 ): ' DreDTo 2 . 3 Simple Mixture Model Another perspective of estimating an accurate query model with the top - ranked documents is the simple mixture model ( SMM ), which assumes that words in DTop are drawn from a two - component mixture model : 1 ) One component is the query - specific topic model PSMM ( w \ Q ), and 2 ) the other is a generic background model P ( w \ BG ).
By doing so , the SMM model PSMM ( w \ Q ) can be estimated by maximizing the likelihood over all the top - ranked documents ( Zhai and Lafferty , 2001b ; Tao and Zhai , 2006 ): where a is a pre - defined weighting parameter used to control the degree of reliance between PSMM ( w \ Q ) and P ( w \ BG ).
This estimation will enable more specific words to receive more probability mass , thereby leading to a more discriminative query model PSMM ( w \ Q ).
Although the SMM modeling aims to extract extra word usage cues for enhanced query modeling , it may confront two intrinsic problems .
One is the extraction of word usage cues from DTop is not guided by the original query .
The other is that the mixing coefficient a is fixed across all top - ranked documents albeit that different documents would potentially contribute different amounts of word usage cues to the enhanced query model .
To mitigate these two problems , the regularized simple mixture model has been proposed and can be estimated by maximizing the likelihood function ( Tao and Zhai , 2006 ; Dillon and Collins - Thompson , 2010 ) DreDTop weV r where is a weighting factor indicating the confidence on the prior information .
3 The Proposed Modeling Framework Fundamentals It is obvious that the major difference among the representative query models mentioned above is how to capitalize on the set of top - ranked documents and the original query .
Several subtle relationships can be deduced through the following in - depth analysis .
First , a direct inspiration of the LM - based query reformulation framework can be drawn from the celebrated Rocchio ' s formulation , while the former can be viewed as a probabilistic counterpart of the latter ( Robertson , 1990 ; Ponte and Croft , 1998 ; Baeza - Yates and Ribeiro - Neto , 2011 ).
Second , after some mathematical manipulation , the formulation of the RM model ( c . f .
It becomes evident that the RM model is composed by mixing a set of document models P ( w \ D ).
As such , the RM model bears a close resemblance to the Rocchio ' s formulation .
Furthermore , based on Eq .
( 5 ), we can recast the estimation of the RM model as an optimization problem , and the likelihood ( or objective ) function is formulated as where the document models P ( w \ D ) are known in advance ; the conditional probability P ( D \ Q ) of each document D is unknown and leave to be estimated .
Finally , a principled framework can be obtained to unify all of these query models , and RSMM ( c . f .
( 4 ))), by using a generalized objective likelihood function : where E represents a set of observations which we want to maximize their likelihood , and M denotes a set of mixture components .
3 . 2 Query - specific Mixture Modeling The SMM model and the RSMM model are intended to extract useful word usage cues from DTop , which are not only relevant to the original query Q but also external to those already captured by the generic background model .
However , we argue in this paper that the " generic information " should be carefully crafted for each query due mainly to the fact that users ' information needs may be very diverse from one another .
To crystallize the idea , a query - specific background model Pq ( w \! BG ) for each query Q can be derived from DTop directly .
Another consideration is that since the original query model P ( w \ Q ) cannot be accurately estimated , it thus may not necessarily be the best choice for use in defining a conjugate Dirichlet prior for the enhanced query model to be estimated .
We propose to use the RM model as a prior to guide the estimation of the enhanced query model .
The enhanced query model is termed query - specific mixture model ( QMM ), and its corresponding training objective function can be expressed as i ( w \! Q ).
4 Applications 4 . 1 Speech Recognition Language modeling is a critical and integral component in any large vocabulary continuous speech recognition ( LVCSR ) system ( Huang et al ., 2001 ; Jurafsky and Martin , 2008 ; Furui et al ., 2012 ).
More concretely , the role of language modeling in LVCSR can be interpreted as calculating the conditional probability P ( w \ H ), in which H is a search history , usually expressed as a sequence of words H = hi , h2 ,..., hL , and w is one of its possible immediately succeeding words .
Once the various aforementioned query modeling methods are applied to speech recognition , for a search history H , we can conceptually regard it as a query and each of its immediately succeeding words w as a ( single - word ) document .
Then , we may leverage an IR procedure that takes H as a query and poses it to a retrieval system to obtain a set of top - ranked documents from a contemporaneous ( or in - domain ) corpus .
Finally , the enhanced query model ( that is P ( w \ H ) in speech recognition ) can be estimated by RM , SMM , RSMM or QMM , and further combined with the background Â«- gram ( e . g ., trigram ) language model to form an adaptive language model to guide the speech recognition process .
4 . 2 Speech Summarization On the other hand , extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio ( Carbonell and Goldstein , 1998 ; Mani and Maybury , 1999 ; Nenkova and McKeown , 2011 ; Liu and Hakkani - Tur , 2011 ).
Intuitively , this task could be framed as an ad - hoc IR problem , where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need .
Therefore , KLM can be used to quantify how close the document D and one of its sentences S are : the closer the sentence model P ( w \ S ) to the document model P ( w \! D ), the more n n ( aD ; ^ pqmm D ; eDTop weV likely the sentence would be part of the summary .
Due to that each sentence S of a spoken document D to be summarized usually consists of only a few words , the corresponding sentence model P ( w \ S ) might not be appropriately estimated by the ML estimation .
To alleviate the deficiency , we can leverage the merit of the above query modeling techniques to estimate an accurate sentence model for each sentence to enhance the summarization performance .
5 Experimental Setup The speech corpus consists of about 196 hours of Mandarin broadcast news collected by the Aca - demia Sinica and the Public Television Service Foundation of Taiwan between November 2001 publicly available and has been segmented into separate stories and transcribed manually .
Each story contains the speech of one studio anchor , as well as several field reporters and interviewees .
A subset of 25 - hour speech data compiled during November 2001 to December 2002 was used to bootstrap the acoustic model training .
The vocabulary size is about 72 thousand words .
The background language model was estimated from a background text corpus consisting of 170 million Chinese characters collected from the Chinese Gigaword Corpus released by LDC .
The dataset for use in the speech recognition experiments is compiled by a subset of 3 - hour speech data from the corpus within 2003 ( 1 . 5 hours for development and 1 . 5 hours for test ).
The contemporaneous ( in - domain ) text corpus used for training the various LM adaptation methods was collected between 2001 and 2003 from the corpus ( excluding the test set ), which consists of one million Chinese characters of the orthographic broadcast news transcripts .
In this paper , all the LM adaptation experiments were performed in word graph rescoring .
The associated word graphs of the speech data were built beforehand with a typical LVCSR system ( Ortmanns et al ., 1997 ; Young et al ., 2006 ).
In addition , the summarization task also employs the same broadcast news corpus as well .
A subset of 205 broadcast news documents compiled between November 2001 and August 2002 was reserved for the summarization experiments ( 185 for development and 20 for test ).
A subset of about 100 , 000 text news documents , compiled during the same period as the documents to be summarized , was employed to estimate the related summarization models compared in this paper .
We adopted three variants of the widely - used ROUGE metric ( i . e ., ROUGE - 1 , ROGUE - 2 and ROUGE - L ) for the assessment of summarization performance ( Lin , 2003 ).
The summarization ratio , defined as the ratio of the number of words in the automatic ( or manual ) summary to that in the reference transcript of a spoken document , was set to 10 % in this research .
6 Experimental Results In the first part of experiments , we evaluate the effectiveness of the various query models applied to the speech recognition task .
The corresponding results with respect to different numbers of top - ranked documents being used for estimating their component models are shown in Table 1 .
Also worth mentioning is that the baseline system with the background trigram language model , which was trained with the SRILM toolkit ( Stolcke , 2005 ) and Good - Turing smoothing ( Jelinek , 1999 ), results in a Chinese character error rate ( CER ) of 20 . 08 % on the test set .
Consulting Table 1 we notice two particularities .
One is that there is more fluctuation in the CER results of SMM than in those of RM .
The reason might be that , for SMM , the extraction of relevance information from the top - ranked documents is conducted with no involvement of the test utterance ( i . e ., the query ; or its corresponding search histories ), as elaborated earlier in Section 2 .
When too many feedback documents are being used , there would be a concern for SMM to be distracted from being able to appropriate model the test utterance , which is probably caused by some dominant distracting ( or irrelevant ) feedback documents .
The other interesting observation is that RSMM only achieves a comparable ( even worse ) result when compared to SMM .
A possible reason is that the prior constraint of the RSMM may contain too much noisy information so as to bias the model estimation .
Furthermore , it is evident that the proposed QMM is the best - performing method among all the query models compared in the paper .
Although the improvements made by QMM are not as pronounced as expected , we believe that QMM has demonstrated its potential to be applied to other related applications .
On the other hand , we compare the various query models with two well - practiced language models , namely the cache model ( Cache ) ( Kuhn and Mori , 1990 ; Jelinek et al .
, 1991 ) and the latent Dirichlet allocation ( LDA ) ( Liu and Liu , 2007 ; Tam and Schultz , 2005 ).
The CER results of these two models are also shown in Table 1 , respectively .
For the cache model , bigram cache was used since it can yield better results than the unigram and trigram cache models in our experiments .
It is worthy to notice that the LDA model was trained with the entire set of contemporaneous text document collection ( c . f .
Section 4 ), while all of the query models explored in the paper were estimated based on a subset of the corpus selected by an initial round of retrieval .
The results reveal that most of these query models can achieve superior performance over the two conventional language models .
In the second part of experiments , we evaluate the utilities of the various query models as applied to the speech summarization task .
At the outset , we assess the performance level of the baseline KLM method by comparison with two well - practiced unsupervised methods , viz .
the vector space model ( VSM ) ( Gong and Liu , 2001 ), and its extension , maximal marginal relevance ( MMR ) ( Carbonell and Goldstein , 1998 ).
The corresponding results are shown in Table 2 and can be aligned with several related literature reviews .
By looking at the results , we find that KLM outperforms VSM by a large margin , confirming the applicability of the language modeling framework for speech summarization .
Furthermore , MMR that presents an extension of VSM performs on par with KLM for the text summarization task ( TD ) and exhibits superior performance over KLM for the speech summarization task ( SD ).
We now turn to evaluate the effectiveness of the various query models ( viz .
RM , SMM , RSMM and QMM ) in conjunction with the pseudo - relevance feedback process for enhancing the sentence model involved in the KLM method .
The corresponding results are also shown in Table 2 .
Two noteworthy observations can be drawn from Table 2 .
One is that all these query models can considerably improve the summarization performance of the KLM method , which corroborates the advantage of using them for enhanced sentence representations .
The other is that QMM is the best - performing one among all the formulations studied in this paper for both the TD and SD cases .
Going one step further , we explore to use extra prosodic features that are deemed complementary to the LM cue provided by QMM for speech summarization .
To this end , a support vector machine ( SVM ) based summarization model is trained to integrate a set of 28 commonly - used prosodic features ( Liu and Hakkani - Tur , 2011 ) for representing each spoken sentence , since SVM is arguably one of the state - of - the - art supervised methods that can make use of a diversity of indicative features for text or speech summarization ( Xie and Liu , 2010 ; Chen et al ., 2013 ).
The sentence ranking scores derived by QMM and SVM are in turn integrated through a simple log - linear combination .
The corresponding results are shown in Table 2 , demonstrating consistent improvements with respect to all the three variants of the ROUGE metric as compared to that using either QMM or SVM in isolation .
We also investigate using SVM to additionally integrate a richer set of lexical and relevance features to complement QMM and further enhance the summarization effectiveness .
However , due to space limitation , we omit the details here .
As a side note , there is a sizable gap between the TD and SD cases , indicating room for further im - Table 1 .
The speech recognition results ( in CER (%)) achieved by various language models along with different numbers of latent topics / pseudo - relevance feedback documents .
Table 2 .
The summarization results ( in F - scores ) achieved by various language models along with text and spoken documents .
We may seek remedies , such as robust indexing schemes , to compensate for imperfect speech recognition .
7 Conclusion and Outlook In this paper , we have presented a systematic and thorough analysis of a few well - practiced query models for IR and extended their novel applicability to speech recognition and summarization in a principled way .
Furthermore , we have proposed an extension of this research line by introducing query - specific mixture modeling ; the utilities of the deduced model have been extensively compared with several existing query models .
As to future work , we would like to investigate jointly integrating proximity and other different kinds of relevance and lexical / semantic information cues into the process of feedback document selection so as to improve the empirical effectiveness of such query modeling .
Acknowledgements This research is supported in part by the " Aim for the Top University Project " of National Taiwan Normal university ( NTNu ), sponsored by the Ministry of Education , Taiwan , and by the Ministry of Science and Technology , Taiwan , under Grants MOST 103 - 2221 - E - 003 - 016 - MY2 , NSC 101 - 2221 - E - 003 - 024 - MY3 , NSC 102 - 2221 - E - 003 - 014 -, NSC 101 - 2511 - S - 003 - 057 - MY3 , NSC 101 - 2511 - S - 003 - 047 - MY3 and NSC 103 - 2911 - I - 003 - 301 .
