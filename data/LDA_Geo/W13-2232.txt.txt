Online Polylingual Topic Models for Fast Document Translation Detection Many tasks in NLP and IR require efficient document similarity computations .
Beyond their common application to exploratory data analysis , latent variable topic models have been used to represent text in a low - dimensional space , independent of vocabulary , where documents may be compared .
This paper focuses on the task of searching a large multilingual collection for pairs of documents that are translations of each other .
We present ( 1 ) efficient , online inference for representing documents in several languages in a common topic space and ( 2 ) fast approximations for finding near neighbors in the probability simplex .
Empirical evaluations show that these methods are as accurate as — and significantly faster than — Gibbs sampling and brute - force all - pairs search .
1 Introduction Statistical topic models , such as latent Dirichproven to be highly effective at discovering hidden structure in document collections ( Hall et al ., 2008 , e . g .).
Often , these models facilitate exploratory data analysis , by revealing which collocations of terms are favored in different kinds of documents or which terms and topics rise and fall over time ( Blei and Lafferty , 2006 ; Wang and McCallum , 2006 ).
One of the greatest advantages in using topic models to analyze and process large document collections is their ability to represent documents as probability distributions over a small number of topics , thereby mapping documents into a low - dimensional latent space — the T - dimensional probability simplex , where T is the number of topics .
A document , represented by some point in this simplex , is said to have a particular " topic distribution ".
Representing documents as points in a low - dimensional shared latent space abstracts away from the specific words used in each document , thereby facilitating the analysis of relationships between documents written using different vocabularies .
For instance , topic models have been used to identify scientific communities working on related problems in different disciplines , e . g ., work on cancer funded by multiple Institutes within the NIH ( Talley et al ., 2011 ).
While vocabulary mismatch occurs within the realm of one language , naturally this mismatch occurs across different languages .
Therefore , mapping documents in different languages into a common latent topic space can be of great benefit when detecting document translation pairs ( Mimno et al ., 2009 ; Platt et al ., 2010 ) .
Aside from the benefits that it offers in the task of detecting document translation pairs , topic models offer potential benefits to the task of creating translation lexica , aligning passages , etc .
The process of discovering relationship between documents using topic models involves : ( 1 ) representing documents in the latent space by inferring their topic distributions and ( 2 ) comparing pairs of topic distributions to find close matches .
Many widely used techniques do not scale efficiently , however , as the size of the document collection grows .
Posterior inference by Gibbs sampling , for instance , may make thousands of passes through the data .
For the task of comparing topic distributions , recent work has also resorted to comparing all pairs of documents ( Talley et al ., 2011 ) .
This paper presents efficient methods for both of these steps and performs empirical evaluations on the task of detected translated document pairs embedded in a large multilingual corpus .
Unlike some more exploratory applications of topic models , translation detection is easy to evaluate .
The need for bilingual training data in many language pairs and domains also makes it attractive to mitigate the quadratic runtime of brute force translation detection .
We begin in § 2 by extending the online variational Bayes approach of Hoffman et al .
( 2010 ) to polylingual topic models ( Mimno et al ., 2009 ).
Then , in § 3 , we build on prior work on efficient approximations to the nearest neighbor problem by presenting theoretical and empirical evidence for applicability to topic distributions in the probability simplex and in § 4 , we evaluate the combination of online variational Bayes and approximate nearest neighbor methods on the translation detection task .
2 Online Variational Bayes for Polylingual Topic Models Hierarchical generative Bayesian models , such as topic models , have proven to be very effective for modeling document collections and discovering underlying latent semantic structures .
Most current topic models are based on Latent Dirichearly work on the subject , Blei and Jordan ( 2003 ) showed the usefulness of LDA on the task of automatic annotation of images .
Hall et al .
( 2008 ) used LDA to analyze historical trends in the scientific literature ; Wei and Croft ( 2006 ) showed improvements on an information retrieval task .
More recently Eisenstein et al .
( 2010 ) modeled geographic linguistic variation using Twitter data .
Aside from their widespread use on monolingual text , topic models have also been used to model multilingual data ( Boyd - Graber and Blei , 2009 ; Platt et al ., 2010 ; Jagarlamudi and Daume , 2010 ; Fukumasu et al ., 2012 ), to name a few .
In this paper , we focus on the Polylingual Topic Model , introduced by Mimno et al .
Given a multilingual set of aligned documents , the PLTM assumes that across an aligned multilingual document tuple , there exists a single , tuple - specific , distribution across topics .
In addition , PLTM assumes that for each language - topic pair , there exists a distribution over words in that language ßl .
As such , PLTM assumes that the multilingual corpus is created through a generative process where first a document tuple is generated by drawing a tuple - specific distribution over topics 9 which , as it is the case with LDA , is drawn from a Dirich - let prior 9 ~ Dir ( a ) .
For each of the languages l in the tuple and for each of the N words wln in the document the generative process : first chooses a topic assignment zln ~ Multinomial ( 9 ) which is then followed by choosing a word wln from a multinomial distribution conditioned on the topic assignment and the language specific topics distribution over words ßl ~ Dir ( rji ).
Both a and rji ,..., L are symmetric priors , i . e .
the priors are exchangeable Dirichlet distributions .
Finally , each word is generated from a language - and topic - specific multinomial distribution ßlt as selected by the topic assignment variable znl : Figure 1 : Polylingual topic model ( PLTM ) Wn ~ p ( wl I Zn , ßl ) Figure 1 shows a graphical representation of the PLTM using plate notation .
In their original work Mimno et al .
( 2009 ) used the Gibbs sampling approach as a posterior inference algorithm to assign topics distributions over their test collection .
While more straightforward to implement , this sampling approach is inherently slow when applied to large collections which makes the original PLTM work practically infeasible to be used on real - world data sets .
In general , performing posterior inference over the latent variables of a Bayesian model is usually done with two of the three approximate approaches , Gibbs sampling , variational Bayes ( VB ) and expectation - propagation .
While Gibbs Sampling is a variation of Markov Chain Monte Carlo method ( MCMC ) which generates a sample from the true posterior after converging to a stationary distribution ; in VB , a set of free variational parameters characterizes a simpler family of probability distributions .
These variational parameters are then optimized by finding the minimum Kullback - Leibler ( KL ) divergence between the variational distribution q ( 9 , z , /\! y , 0 , A ) and the true posterior P ( 9 , z , ß \! w , a , n ).
From an algorithmic perspective , the variational Bayes approach follows the Expectation - Maximization ( EM ) procedure where for a given document , the E - step updates the per document variational parameters Ydand ( f ) d while holding the per words - topic distribution parameter A fixed .
It then updates the variational parameter A using the sufficient statistics computed in the E step .
In order to converge to a stationary point , both approaches require going over the whole collection multiple times which makes their time complexity to grown linearly with the size of the data collection .
The mere fact that they require continuous access to the whole collection makes both inference approaches impracticable to use on very large or streaming collections .
To alleviate this problem , several algorithms have been proposed that draws from belief propagation ( Zeng et al ., 2012 ), the Gibbs sampling approach such as ( Canini et al ., 2009 ), variational Bayes ( Hoffman et al ., 2010 ) as well as a combination of the latter two ( Hoffman et al ., 2012 ) to name a few .
In this paper we use Hoffman et al .
( 2010 ) approach .
Hoffman et al .
( 2010 ) proposed a new inference approach called Online LDA which relies on the stochastic gradient descent to optimize the variational parameters .
This approach can produce good estimates of LDA posteriors in a single pass over the whole collection .
In the traditional LDA model 9 is used to specify the document specific distribution over topics .
2 . 1 Algorithmic Implementation We now derive an online variational Bayes algorithm for PLTM to infer topic distributions over multilingual collections .
Figure 2 shows the vari - ational model and free parameters used in our approach .
As in the case of Hoffman et al .
( 2010 ), our algorithm updates the variational parameters Yd and 4 > ld on each batch of documents while the variational parameter A is computed as a weighted average of the value on the previous batch and its approximate version A .
Averaging is performed using a decay function whose parameters control the rate at which old values of A are forgotten .
Within the E step of the VB approach , we compute the updates over the variational parameter 0l Figure 2 : Graphical model representation of the free variational parameters for the online varia - tional Bayes approximation of the PLTM posterior for each language L present in our document tuple while the update on the Y parameter accumulates the language specific sufficient statistics : I ml nml wk nw We detail these steps in Algorithm 1 .
2 . 2 Performance Analysis To demonstrate the efficacy of online PLTM , we ran topic inference on a subset of the English - Spanish Europarl collection consisting of ~ 64k parallel speeches and compared the accuracy results vs . the training and inference speed against the original PLTM model using topic sets of T = 50 , 100 , 200 and 500 .
We explain in details the evaluation task and the performance metric used in § 4 .
Shown in Figure 3 are the results of these comparisons .
Our speed measurements were performed on Xeon quad processors with a clock speed of 2 . 66GHz and a total of 16GB of memory .
As we increase the number of topics we gain in accuracy over the evaluation task across both inference approaches .
When we increase the number of topics from 50 to 500 the speed improvement obtained by Online VB PLTM drops by a factor of 2 . 9 within the training step and by a factor of 4 . 45 in the test step .
Our total running time for the Online VB PLTM with T = 500 approaches the running time of the Gibbs sampling approach with T = 50 .
The gradual drop in speed improvement with the increase of the number topics is mostly attributed to the commutation of the Algorithm 1 Online variational Bayes for PLTM initialize Al randomly obtain the tth mini - batch of tuples Mtfor t = 1 to oo do initialize Yt randomly for each document tuple in mini - batch t for m in Mt do repeat until convergence end for Akw = n + d Em Ck end for end for Figure 3 : Speed vs . accuracy comparison between Online VB PLTM and Gibbs Sampling PLTM at T = 50 , 100 , 200 and 500 .
We used a Python implementation of Online VB and Mallet ' s Java implementation of PLTM with in - memory Gibbs Sampling using 1000 iterations .
Figure 4 : Collection size vs . training time comparison between Online VB PLTM and Gibbs Sampling PLTM using multilingual collections of50k , 100k , 250k , 500k , 750k and 1M speech pairs .
digamma function ( Asuncion et al ., 2009 ) whose time complexity increases linearly with the number of topics .
While a multilingual collection of ~ 64k document pairs is considered relatively big , our goal of deriving the Online VB PLTM approach was to be able to utilize PLTM on very large multilingual collections .
To analyze the potential of using Online VB PLTM on such collections we ran speed comparisons within the training step by creating multilingual collections of different lengths multiplying the original English - Spanish Europarl collection .
Speed comparisons using collections of length 50K , 100K , 250K , 500K , 750K and 1M are shown in Figure 4 .
Training was performed with the number of topics T set to T = 50 and T = 500 .
As we increase the collection size we observe the real benefit of using Online VB compared to Gibbs sampling .
This is mostly attributed to the fact that the Gibbs sampling approach requires multiple iterations over the whole collection in order to achieve a convergence point .
For collection sizes of 50k and 100k the training time for the Online VB PLTM with T = 500 approaches the training time of Gibbs sampling with T = 50 and as we increase the collection size this proximity dissipates .
In Figure 5 we show a sample set of the aligned topics extracted using Online VB PLTM with T = 400 on the English - Spanish Europarl collection .
For a given topic tuple words are ordered based on probability of occurrence within the given topic .
Figure 5 : Sample set of topics extracted from Europarl English - Spanish collection of 64k speeches using Online PLTM with T = 400 ordered based on their probability of occurrence within the topic .
3 Approximate NN Search in the Probability Simplex One of the most attractive applications for topic models has involved using the latent variables as a low - dimensional representation for document similarity computations ( Hall et al ., 2008 ; Boyd - Graber and Resnik , 2010 ; Talley et al ., 2011 ).
After computing topic distributions for documents , however , researchers in this line of work have almost always resorted to brute - force all - pairs similarity comparisons between topic distributions .
In this section , we present efficient methods for approximate near neighbor search in the probability simplex in which topic distributions live .
Measurements for similarity between two probability distributions are information - theoretic , and distance metrics , typical for the metric space , are not appropriate ( measurements such as Euclidean , cosine , Jaccard , etc .).
Divergence metrics , such as Kullback - Leibler ( KL ), Jensen - Shannon ( JS ), and Hellinger distance are used instead .
Shown in Figure 6 are the formulas of the divergence metrics along with the Euclidean distance .
When dealing with a large data set of N documents , the O ( N ) time complexity of all - pairs comparison makes the task practically infeasible .
With some distance measures , however , the time complexity on near neighbor tasks has been alleviated using approximate methods that reduce the time complexity of each query to a sub - linear number of comparisons .
For example , Euclidean distance ( 3 ) has been efficiently used on all - pairs comparison tasks in large data sets thanks to its approximate based versions developed using locality sensitive hashing ( LSH ) ( Andoni et al ., 2005 ) and k - d search trees ( Friedman et al ., 1977 ).
In order to alleviate the all - pairs computational complexity in the probability simplex , we will use a reduction of the Hellinger divergence measure ( 4 ) to Euclidean distance and therefore utilize preexisting approximation techniques for the Euclidean distance in the probability simplex .
This reduction comes from the fact that both measurements have similar algebraic expressions .
If we discard the square root used in the Euclidean distance , Hellinger distance ( 4 ) becomes equivalent to the Euclidean distance metric ( 3 ) between yfpl and y / qî .
The task of finding nearest neighbors for a given point ( whether in the metric space or the probability simplex ) involves ranking all nearest points discovered and as such not computing the square root function does not affect the overall ranking and the nearest neighbor discovery .
Moreover , depending on its functional form , the Hellinger distance is often defined as square root over the whole summation .
Aside from the Hellinger distance , we also approximate Jensen - Shannon divergence which is a symmetric version of the Kullback - Liebler divergence .
For the JS approximation , we will use a constant factor relationship between the Jensen - Shannon divergence an Hellinger distance previously explored by ( Tops0e , 2000 ).
More specifically , we will be using its more concise form ( 7 ) also presented by He ( p , q ) < JS ( p , q ) < 2ln ( 2 ) He ( p , q ) ( 7 ) Figure 6 : Distance measures and bounds Jensen - Shannon Divergence Figure 7 : Empirical evidence of the bounds presented in Eq .
7 on 1 million document pairs — zoomed section where nearest neighbors reside .
The lower bound is He ( p , q ) = jïh ^) JS ( p , q ) while the upper bound is He ( p , q ) = 2JS ( p , q ).
The constant factor relationship provides us with the theoretical guarantees necessary for this approximation .
In practice , we can often do much better than this theoretical bound .
Figure 7 shows the empirical relation of JS and Hellinger on a translation - detection task .
As will be described in § 4 , we computed the JS and Hellinger divergences between topic distributions of English and Spanish Europarl speeches for a total of 1 million document pairs .
Each point in the figure represents one Spanish - English document pair that might or might not be translations of each other .
In this figure we emphasize the lower left section of the plot where the nearest neighbors ( i . e ., likely translations ) reside , and the relationship between JS and Hellinger is much tighter than the theoretical bounds and from pratical perspective as we will show in the next section .
As a summary for the reader , using the above approaches , we will approximate JS divergence by using the Euclidean based representation of the Hellinger distance .
As stated earlier , the Euclidean based representation is computed using well established approximation approaches and in our case we will use two such approaches : the Exact Euclidean LSH ( E2LSH ) ( Andoni et al ., 2005 ) and the k - d trees implementation within the Approximate Nearest Neighbor ( ANN ) library ( Mount and Arya , 2010 ).
4 Efficient Approximate Translation Detection Mapping multilingual documents into a common , language - independent vector space for the purpose of improving machine translation ( MT ) and performing cross - language information retrieval ( CLIR ) tasks has been explored through various techniques .
Mimno et al .
( 2009 ) introduced polylingual topic models ( PLTM ), an extension of latent Dirichlet allocation ( LDA ), and , more recently , Platt et al .
( 2010 ) proposed extensions of principal component analysis ( PCA ) and probabilistic latent semantic indexing ( PLSI ).
Both the PLTM and PLSI represent bilingual documents in the probability simplex , and thus the task of finding document translation pairs is formulated as finding similar probability distributions .
While the nature of both works was exploratory , results shown on fairly large collections of bilingual documents ( less than 20k documents ) offer convincing argument of their potential .
Expanding these approaches to much large collections of multilingual documents would require utilizing fast NN search for computing similarity in the probability simplex .
While there are many other proposed approaches to the task of finding document translation pairs that represent documents in metric space , such as Krstovski and Smith ( 2011 ) which utilizes LSH for cosine distance , there is no evidence that they yield good results on documents of small lengths such as paragraphs and even sentences .
In this section , we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time .
We use PLTM representations of bilingual documents .
In addition , we show how the results as reported by Platt et al .
( 2010 ) can be obtained using the PLTM representation with a significant speed improvement .
As in ( Platt et al ., 2010 ) and ( Mimno et al ., 2009 ) the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs .
For this experimental setup , accuracy is defined as the number of times ( in percentage ) that the target language document was discovered at rank 1 ( i . e .
% @ Rank 1 .)
across the whole test collection .
4 . 1 Experimental Setup We use Mallet ' s ( McCallum , 2002 ) implementation of the PLTM to train and infer topics on the same data set used in Platt et al .
That paper used the Europarl ( Koehn , 2005 ) multilingual collection of English and Spanish sessions .
Their training collection consists of speeches extracted from all Europarl sessions from the years 1996 through 1999 and the year 2002 and a development set which consists of speeches from sessions in 2001 .
The test collection consists of Eu - roparl speeches from the year 2000 and the first nine months of 2003 .
While Platt et al .
( 2010 ) do offer absolute performance comparison between their JPLSA approach and previous results published by ( Mimno et al ., 2009 ), these performance comparisons are not done on the same training and test sets — a gap that we fill below .
We train PLTM models with number of topics T set to 50 , 100 , 200 , and 500 .
In order to compare exactly the same topic distributions when computing speed vs . accuracy of various approximate and exhaustive all - pairs comparisons we focus only on one inference approach - the Gibbs sampling and ignore the online VB approach as it yields similar performance .
For all four topic models , we use the same settings for PLTM ( hyperparameter values and number of Gibbs sampling iterations ) as in ( Mimno et al ., 2009 ).
Topic distributions were then inferred on the test collection using the trained topics .
We then performed all - pairs comparison using JS divergence , Hellinger distance , and approximate , LSH and kd - trees based , Hellinger distance .
We measured the total time that it takes to perform exhaustive all - pairs comparison using JS divergence , the LSH and kd - trees version on a single machine consisting of a core 2 duo quad processors with a clock speed of 2 . 66GHz on each core and a total of 8GB of memory .
Since the time performance of the E2LSH depends on the radius R of data set points considered for each query point ( Indyk and Motwani , 1998 ), we performed measurements with different values of R . For this task , the all - pairs JS code implementation first reads both source and target sets of documents and stores them in hash tables .
We then go over each entry in the source table and compute divergence against all target table entries .
We refer to this code implementation as hash map implementation .
4 . 2 Evaluation Task and Results Performance of the four PLTM models and the performance across the four different similarity measurements was evaluated based on the percentage of document translation pairs ( out of the whole test set ) that were discovered at rank one .
This same approach was used by ( Platt et al ., 2010 ) to show the absolute performance comparison .
As in the case of the previous two tasks , in order to evaluate the approximate , LSH based , Hellinger distance we used values of R = 0 . 4 , R = 0 . 6 and R = 0 . 8 .
Since in ( Platt et al ., 2010 ) numbers were reported on the test speeches whose word length is greater or equal to 100 , we used the same subset ( total of 14150 speeches ) of the original test collection .
Shown in Table 1 are results across the four different measurements for all four PLTM models .
When using regular JS divergence , our PLTM model with 200 topics performs the best with 99 . 42 % of the top one ranked candidate translation documents being true translations .
When using approximate , kd - trees based , Hellinger distance , we outperform regular JS and Hellinger divergence across all topics and for T = 500 we achieve the best overall accuracy of 99 . 61 %.
We believe that this is due to the small amount of error We start off by first replicating the results as in ( Mimno et al ., 2009 ) and thus verifying the functionality of our experimental setup .
JS divergence while maintaining the same performance compared to Hellinger distance metric and insignificant loss over all - pairs JS divergence .
From Table 2 it is evident that as we increase the radius R we reduce the relative speed of performance since the range of points that LSH considers for a given query point increases .
Also , as the number of topics increases , the speed benefit is reduced for both the LSH and k - d tree techniques .
5 Conclusion Hierarchical Bayesian models , such as Polylingual Topic Models , have been shown to offer great potential in analyzing multilingual collections , extracting aligned topics and finding document translation pairs when trained on sufficiently large aligned collections .
Online stochastic optimization inference allows us to generate good parameter estimates .
By combining these two approaches we are able to infer topic distributions across documents in large multilingual document collections in an efficient manner .
Utilizing approximate NN search techniques in the probability simplex , we showed that fast document translation detection could be achieved with insignificant loss in accuracy .
6 Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant # IIS - 0910884 .
Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor .
