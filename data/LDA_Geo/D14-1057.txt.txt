A Comparison of Selectional Preference Models for Automatic Verb Classification We present a comparison of different selectional preference models and evaluate them on an automatic verb classification task in German .
We find that all the models we compare are effective for verb clustering ; the best - performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner .
A very simple model based on lexical preferences is also found to perform well .
1 Introduction Selectional preferences ( Katz and Fodor , 1963 ; Wilks , 1975 ; Resnik , 1993 ) are the tendency for a word to semantically select or constrain which other words may appear in a direct syntactic relation with it .
Selectional preferences ( SPs ) have been a perennial knowledge source for nlp tasks such as word sense disambiguation ( Resnik , 1997 ; Stevenson and Wilks , 2001 ; McCarthy and Carroll , 2003 ) and semantic role labelling ( Erk , 2007 ); and recognising selectional violations is thought to play a role in identifying and interpreting metaphor ( Wilks , 1978 ; Shutova et al ., 2013 ).
We focus on the SPs of verbs , since determining which arguments are typical of a given verb sheds light on the semantics of that verb .
In this study , we present the first empirical comparison of different SP models from the perspective of automatic verb classification ( Schulte im Walde , 2009 ; Sun , 2012 ), the task of grouping verbs together based on shared syntactic and semantic properties .
We cluster German verbs using features capturing their valency or subcategorisation , following prior work ( Schulte im Walde , 2000 ; Esteve Ferrer , 2004 ; Schulte im Walde , 2006 ; Sun et al ., 2008 ; Korhonen et al ., 2008 ; Li and Brew , 2008 ), and investigate the effect of adding information about verb argument preferences .
SPs are represented by features capturing lexical information about the heads of arguments to the verbs ; we restrict our focus here to nouns .
We operationalise a selectional preference model as a function which maps such an argument head to a concept label .
We submit that the primary characteristic of such a model is its granularity .
In our baseline condition , all nouns are mapped to the same label ; this effectively captures no information about a verb ' s SPs ( i . e ., we cluster verbs using subcategorisation information only ).
On the other extreme , each noun is its own concept label ; we term this condition lexical preferences ( LP ).
Between the baseline and LP lie a spectrum of models , in which multiple concepts are distinguished , and each concept label can represent multiple nouns .
Our main hypothesis is that verb clustering will work best using a model of such intermediate granularity .
This follows the intuition that verbs would seem to select for classes of nouns ; for instance , we suppose that essen ' eat ' would tend to prefer as a direct object a noun from the absUact concept Essen (' food ').
We assume that these concepts can be expressed independently of particular predicates ; that is , there exist selectional preference models that will work for all verbs ( and all grammatical relations ).
Further benefits of grouping nouns into classes include combating data sparsity , as well as deriving models which can generalise to nouns unseen in training data .
Another parameter of a selectional preference model is the methodology used to induce the conceptual classes ; put another way , the success of an SP model hinges on how it represents concepts .
In this paper , we investigate the choice of noun categorisation method through an empirical comparison of selectional preference models previously used in the literature .
We set out to investigate the following questions : 1 .
What classes of nouns are effective descriptors of selectional preference concepts ?
For example , do they correspond to features such as animate ?
What is the appropriate granularity of selectional preference concepts ?
Which methods of classifying nouns into concepts are most effective at capturing selectional preferences for verb clustering ?
This paper is structured as follows : In Section 2 , we introduce our baseline method of clustering verbs using subcategorisation information and describe evaluation ; Section 3 lists the models of selectional preferences that we compare in this work ; Section 4 presents results and discussion ; Section 5 summarises related work ; and Section 6 concludes with directions for future research .
2 Automatic verb classification Verb classifications such as VerbNet ( Kipper - Schuler , 2005 ) allow generalisations about the syntax and semantics of verbs and have proven useful for a range of nlp tasks ; however , creation of these resources is expensive and time - consuming .
Automatic verb classification seeks to learn verb classes automatically from corpus data in a cheaper and faster way .
This endeavour is possible due to the link between a verb ' s semantics and its syntactic behaviour ( Levin , 1993 ).
Recent research has found that even automatically - acquired classifications can be useful for nlp applications ( Shutova et al ., 2010 ; Guo et al ., 2011 ).
In this section , we introduce the verb classification method used by our baseline model , which clusters verbs based on subcategorisation information .
Following this , Section 2 . 2 explains the gold standard verb clustering and cluster purity metric which we use for evaluation .
2 . 1 Baseline model In this work , we take subcategorisation to mean the requirement of a verb for particular types of argument or concomitant .
For example , the English verb put subcategorises for subject , direct object , and a prepositional phrase ( pp ) like on the shelf : ( 1 ) [ np Al ] put [ np the book ] [ pp on the shelf ].
A subcategorisation frame ( scf ) describes a combination of arguments required by a specific verb ; a description of the set of scfs which a verb may take is called its subcategorisation preference .
We acquire descriptions of verbal scf preferences on the basis of unannotated corpus data .
Our experiments use the SdeWaC corpus ( Faaß and Eckart , 2013 ), containing 880 million words in 45 million sentences ; this is a subset of deWaC ( Baroni et al ., 2009 ), a corpus of 10 words extracted from Web search results .
SdeWaC is filtered to include only those sentences which are maximally parsable .
We parsed SdeWaC with the mate - tools dependency parser ( Bohnet et al ., 2013 ), which performs joint POS and morphological tagging , as well as lemmatisation .
Our subcategorisation analyses are delivered by the rule - based scf tagger described by Roberts et al .
( 2014 ), which operates using the dependency parses and assigns each finite verb an scf type .
The scf tags are taken from the scf inventory proposed by Schulte im Walde ( 2002 ), which indicates combinations of nominal and verbal complement types , such as nap : f ür .
Acc ( transitive verb , with a PP headed by für ' for ').
Examples of complements are n for nominative subject , and a for accusative direct object ; in SCFs which include PPs ( p ), the SCF tag specifies the head of the PP and the case of the prepositional argument ( Acc in our example indicates the accusative case of the prepositional argument ).
The scf tagger undoes passivisation and analyses verbs embedded in modal and tense constructions .
We record 673 scf types in SdeWaC .
From SdeWaC , we extracted the first 3 , 000 , 000 verb instances assigned an scf tag by the scf tagger , where the verb lemma is one of the 168 listed in our gold standard clustering ( this requires approximately 270 million words of parsed text , or 25 % of SdeWaC ).
We refer to this as our test set .
In this set , each verb is seen on average 17 , 857 times ; the most common is geben (' give ', 328 , 952 instances ), and the least is grinsen (' grin ', 50 ).
We represent verbs as vectors , where each dimension represents a different scf type .
Vector entries are initialised with scf code counts over the test set , and each vector is then normalised to sum to 1 , so that a vector represents a discrete probability distribution over the scf inventory .
We use the Jensen - Shannon divergence as a dissimilarity measure between pairs of verb vectors .
The Jensen - Shannon divergence ( Lin , 1991 ) is an information - theoretic , symmetric measure ( Equation ( 2 )) re - ' The filtering used a rule - based dependency parser to estimate a per - token parse error rate for each sentence , and removed those sentences with very high error rates .
https :// code . google . com / p / mate - tools / lated to the Kullback - Leibler divergence ( Equation ( 3 )).
With this dissimilarity measure , we use hierarchical clustering with Ward ' s criterion ( Ward , Jr , 1963 ) to partition the verbs into K disjoint sets ( i . e ., hard clustering ), where we match K to the number of classes in our gold standard ( described below ).
2 . 2 Evaluation paradigm We evaluate the automatically induced verb clusterings against a manually - constructed gold standard , published by Schulte im Walde ( 2006 , page 162ff .).
This Levin - style classification groups 168 high - and low - frequency verbs into 43 semantic classes ; examples include Aspect ( e . g ., anfangen ' begin '), Prepositional Attitude ( e . g ., denken ' think '), and Weather ( e . g ., regnen ' rain ').
Some of the classes are further sub - classified ; for the purposes of our evaluation , we ignore the hierarchical structure of the classification and consider each class or subclass to be a separate entity .
In this way , we obtain classes of fairly comparable size and sufficient semantic consistency .
We evaluate a given verb clustering against the gold standard using the pairwise F - score ( Hatzivassiloglou and McKeown , 1993 ).
To calculate this statistic , we construct a contingency table over the pairs of verbs , the idea being that the gold standard provides binary judgements about whether two verbs should be clustered together or not .
If a clustering agrees with the gold standard as to whether a pair of verbs belong together or not , this is a " correct " answer .
Using the contingency table , the standard information retrieval measures of precision ( P ) and recall ( R ) can be computed ; the F - score is then the harmonic mean of these : F = 2PR /{ P + R ).
The random baseline is 2 . 08 ( calculated as the average score of 50 random partitions ), and the optimal score is 95 . 81 , calculated by evaluating the gold standard against itself .
As the gold standard includes polysemous verbs , which belong to more than one cluster , the optimal score is calculated by randomly picking one of their senses ; the average is then taken over 50 such trials .
In contrast , a top - level class like ' Transfer of Possession ( Obtaining )', not only covers 25 % of the gold standard , it also comprises the semantically very diverse subclasses ' Transfer of Possession ( Giving )', ' Manner of Motion ', and ' Emotion '.
The pairwise F - score is known to be somewhat nonlinear ( Schulte im Walde , 2006 ), penalising early clustering " mistakes " more than later ones , but it has the advantage that we can easily determine statistical significance using the contingency table and McNemar ' s test .
We use only one clustering algorithm and one purity metric , because our prior work shows that the most important choices for verb clustering are the distance measure used , and how verbs are represented .
These factors set , we expect similar performance trends from different algorithms , with predictable variation ( e . g ., spectral tends to outperform hierarchical clustering , which in turn outperforms fc - means ).
Combining Ward ' s criterion and F - score is a trade - off at this point ; the criterion is deterministic , giving reproducible results without computational complexity , but disallows estimates of density over our evaluation metric and is greedy ( see discussion in Section 4 . 3 ).
3 Selectional preference models In this section , we introduce the various SP models that we compare in this paper .
In all cases , we hold the verb clustering procedure described in the previous section unchanged , with the exception that SCF tags for verbs are parameterised for selectional preferences .
As an example , a verb instance observed in a simple Uansitive frame with a nominal subject and accusative object would receive the SCF tag na .
Assuming that a given SP model places the subject noun in the SP concept animate and the object noun in the concept concrete , the parameterised SCF tag would be na * subj -{ animate }* obj -{ concrete }.
This process captures argument co - occurrence information about verb instances , and has the effect of multiplying the SCF inventory size , making the verb vectors described in Section 2 . 1 both longer and sparser .
We evaluate various types of SP models : the simple lexical preferences model ; three models which perform automatic unsupervised induction of noun concepts from unlabelled data ; and one which uses a manually - built lexical resource .
As far as we are aware , two of these , the word space and LDA models , have never been applied to verb classification before .
Table 1 : Fraction of verb instances in the test set parameterised by lp as a function of the number of nouns N included in the lp model .
3 . 1 Lexical preferences The LP model is the simplest in our study after the baseline condition ; it simply maps a noun to its own lemma .
We include as a parameter of the LP model a maximum number of nouns N to admit as LP tags .
In this way , the LP model parameterises scfs using only the N most frequent nouns in SdeWaC ; nouns beyond rank N are treated as if they were unseen .
Table 1 indicates what fraction of the 3 million verb instances receive scf tags specifying one or more LPs as a function of this parameter .
Note that the coverage approaches an asymptote of around 60 %.
This is due to the fact that noun arguments are not observed for every verb instance ; many verbs ' arguments are pronominal or verbal and are not treated by our sp models .
Setting N allows a simple way of tuning the LP model : With increasing N , the LP model should capture more data about verb instances , but after a point this benefit should be cancelled out by the increasing sparsity in the verb vectors .
3 . 2 Sun and Korhonen model The sp model described in this section ( sun ) was first used by Sun and Korhonen ( 2009 ) to deliver state - of - the - art verb classification performance for English ; more recently , the technique was applied to successfully identify metaphor in free text ( Shutova et al ., 2010 ; Shutova et al ., 2013 ).
It uses co - occurrence counts that describe which nouns are found with which verbs in which grammatical relations ; this information is used to sort the nouns into classes in a procedure almost identical to our verb clustering method described in Section 2 . 1 .
We extract all verb instances in SdeWaC which are analysed by the scf tagger , and count all ( verb , grammatical relation , nominal argument head ) triples , where the grammatical relation is subject , direct ( accusative ) object , indirect ( dative ) object , or prepositional object , and is listed in the verb instance ' s scf tag ; we undo passivisation , remove instances of auxiliary and modal verbs , and filter out those triples seen less than 10 times in the corpus .
These observations cover 60 , 870 noun types and 33 , 748 , 390 tokens , co - occurring with 6 , 705 verb types ( 11 , 426 verb - grammatical - relation types ); an example is ( sprechen , ob ], Wort ) (' speak ' with direct object ' word ', occurring 1 , 585 times ).
We represent each noun by a vector whose 11 , 426 dimensions are the different verb - grammatical - relation pairs ; coordinates in the vector indicate the observed corpus counts .
The vectors are then normalised to sum to 1 , such that each represents some particular noun ' s discrete probability distribution over the set of verb - grammatical - relation pairs .
The distance between two noun vectors is defined to be the Jensen - Shannon divergence between their probability distributions , and we partition the set of nouns into M groups using hierarchical Ward ' s clustering .
The sp model then maps a noun to an arbitrary label indicating which of the M disjoint sets that noun is to be found in ( i . e ., all nouns in the first noun class map to the concept label concept 1 ); we employ the parameter M to model sp concept granularity .
As with the lp model , we use the parameter N to indicate how many nouns are included in the sun model ; we search the parameter values N = { 300 , 500 , 1000 , 5000 , 10000 } and $ = { 5 , 10 , 15 , 20 , 30 , 50 }.
3 . 3 Word space model Word space models ( wsms , ( Sahlgren , 2006 ; Turney and Pantel , 2010 )) use word co - occurrence counts to represent the disUibutional semantics of a word .
This strategy makes possible a clustering of nouns that does not depend on verbal dependencies in the first place .
We have also experimented with adding features for each noun showing nominal modification features ( e . g ., ( schwarz , nmod , Haar ), ' hair ' modified by ' black '), but these seem to hurt performance .
Triples representing prepositional object relations are distinguished by preposition ( e . g ., the triple ( geben , prep - in , Auftrag ), ' give ' with pp headed by ' in ' with argument head ' contract ', an idiomatic expression meaning ' to commission ' something ).
( 1999 ) address the problem of data sparseness for the automatic determination of word co - occurrence probabilities , which includes selectional preferences .
They introduce the idea of estimating the probability of hitherto unseen word combinations using available information on words that are closest w . r . t .
distributional word similarity .
Following this idea , Erk ( 2007 ) and Padö et al .
( 2007 ) describe a memory - based SP model , using a WS m similarity measure to generalise the model to unseen data .
We build a wsm of German nouns and use it to partition nouns into disjoint sets , which we then employ as with the sun model .
We compute word co - occurrence counts across the whole SdeWaC corpus , using as features the 50 , 000 most common words in SdeWaC , skipping the first 50 most common words ( i . e ., we use words 50 through 50 , 050 ), with sentences as windows .
We lemmatise the corpus and remove all punctuation ; no other normalisation is performed .
Co - occurrence counts between a word Wi and a feature Cj are weighted using the t - test scheme : We use a recent technique called context selection ( Polajnar and Clark , 2014 ) to improve the word space model , whereby only the C most highly weighted features are kept for each word vector .
We set C by optimising the correlation between the word space model ' s cosine similarity and a data set of human semantic relatedness judgements for 65 word pairs ( Gurevych and Niederlich , 2005 ); at C = 380 , we obtain Spearman p = 0 . 813 and Pearson r = 0 . 707 ( human inter - annotator agreement for this data set is given as r = 0 . 810 ).
After this , we build a similarity matrix between all pairs of nouns using the cosine similarity , and then partition the set of N nouns into M disjoint classes using spectral clustering with the MNCut algorithm ( Meilä and Shi , 2001 ).
As with the SUN model , this SP model assigns labels to nouns indicating which noun class they belong to .
We search the same parameter space for N and M as for the sun model .
3 . 4 GermaNet Statistical models of SPs have often used WordNet as a convenient and well - motivated inventory of concepts ( e . g ., Resnik ( 1997 ), Li and Abe ( 1998 ), Clark and Weir ( 2002 )).
Typically , such models make use of probabilistic treatments to determine an appropriate concept granularity separately for each predicate ; we opt here for a simple model that allows more direct control over concept granularity .
We take the set of concepts relevant to describing selectional preferences to be a target set of synsets in GermaNet ( Hamp and Feldweg , 1997 ), and represent the target set as the set of synsets which are at some depth d or less in the GermaNet noun hierarchy : { s I depth ( s ) < d } where depth ( s ) counts the number of hypernym links separating s from the root of the hierarchy .
We model concept granularity by varying d = 1 ... 6 ; at d = 1 , the target set is of size 5 , and at d = 6 , it is of size 17 , 125 .
Nouns are atttibuted to concepts as follows : Given a noun belonging to a synset s , either s is in the target set , or we take s ' s lowest hypernym in the target set .
For polysemous nouns , each synset listing a sense of the noun votes for a member of the target set ; the noun observation is then spread over the target set using the votes as weights .
This procedure makes our GermaNet SP model a soft clustering over nouns ( i . e ., a noun can belong to more than one SP concept ); a consequence of this is that a single verb occurrence in the corpus can contribute fractional counts to multiple SCF types .
Latent Dirichlet allocation ( Blei et al ., 2003 ) is a generative model that discovers similarities in data using latent variables ; it is frequently used for topic modelling .
LDA models of SPs have been proposed by Ö Séaghdha ( 2010 ) and Ritter et al .
( 2010 ); previous to this , Rooth et al .
( 1999 ) also described a latent variable model of SPs .
We implement the LDA model of selectional preferences described by Ö Séaghdha ( 2010 ).
Gener - atively , the model produces nominal arguments to verbs as follows : For a given ( verb , grammatical relation ) pair ( v , r ), ( 1 ) Sample a noun class z from a from a multinomial distribution «&^> r with a Dirichlet prior parameterised by a ; ( 2 ) Sample a noun n from a multinomial disUibution @ z with a Dirichlet prior parameterised by ß .
Like Ö Séaghdha , we use an asymmetric Dirichlet prior for «&^> r ( i . e ., a can differ for each noun class ) and a symmetric prior for @ z ( ß is the same for each @ z ).
We estimate the LDA model using the MALLET software ( Mc - Callum , 2002 ) using the same ( verb , grammatical relation , argument head ) co - occurrence statistics used for the sun model .
We train for 1 , 000 iterations using the software ' s default parameters , allowing the lda hyperparameters a and ß to be re - estimated every 10 iterations .
We build models with 50 or 100 topics as a proxy to concept granularity ; models include number of nouns N of { 500 , 1000 , 5000 , 10000 , 50000 , 100000 }.
As with the GermaNet - based model , the lda model creates a soft clustering of nouns ; the ability of a noun to have degrees of membership in multiple concepts might be a good way to model polysemy .
We also experiment with a hard clustering version of the lda model ; to do this , we assign each noun n its most likely class label z using the model ' s estimate for P ( z \ n ).
4 Results We experimented with applying the sp models to different combinations of grammatical relations ( e . g ., only subject , only object , subject + object , etc .
), but generally obtained better results by para - meterising scf tags for all grammatical relations .
Table 2 summarises the evaluation scores and parameter settings for the best - performing sp models , applied to verb arguments in all four grammatical relations ( subject , direct , indirect and prepositional object ).
The table also indicates the number of scf types constructed by each sp model ( i . e ., the number of dimensions of the vectors representing verbs ).
All the sp models we compare help with automatic verb clustering .
Using McNemar ' s test on the contingency tables underlying the F - scores , all models score better than the baseline at at least the p < 0 . 01 level .
lda - hard is better than the GermaNet , lda - soft , ws m and lp models at at least the p < 0 . 05 level ; sun is better ( p < 0 . 05 ) than all models except lda - hard .
All other performance differences are not statistically significant .
We can also demonstrate the effectiveness of the sp models with a regression analysis on the models ' coverage of the test set .
By varying the number of nouns N included in the sp models which use this parameter ( lp , sun , wsm , lda ), or by paramet - erising scf tags with sp information only for particular combinations of grammatical relations , different numbers of the verb instances in the test data will end up with sp information in their scf tags ( this is the " coverage " statistic in Table 1 ); with the exception of the GermaNet model , all of the s p models we examine here show positive correlation between the number of verb instances tagged for sp information and verb clustering performance .
This effect is independent of parameter settings , indicating the performance benefit conferred by the sp models is robust .
Due to space constraints , we do not present here a detailed per - model study of performance as a function of parameter settings ; we feel a summary to be adequate , since the relative performances of the models reflect trends across a range of parameter settings .
Using a significance criterion of p < 0 . 05 .
4 . 1 Comparison of sp models The GermaNet model is the least successful in our study .
It achieves its best performance with a depth of 5 ; after this , verb clustering performance drops off again .
Verb clustering using the GermaNet sp model is only slightly better than the baseline condition .
Against our expectations , the hard clustering lda models perform better than the soft clustering ones , achieving the second highest score in our evaluation ; also , in contrast to the other sp models studied in this paper , lda performs best with fewer , coarser - grained topics .
We observe that the soft clustering models produce verb vectors more than an order of magnitude longer than the hard clustering models , and suggest that simple soft clustering may be causing problems with data sparsity that interfere with verb clustering .
We have also observed that the topics found by lda do not represent polysemy as we had hoped .
While some of the topics discovered by the lda models can be easily assigned labels ( e . g ., body parts , people , quantities , emotions , places , buildings , tools , etc .
), others are less cohesive .
We found that frequent words ( e . g ., time , person ) are generated with high probability by multiple topics in ways that do not appear to reflect multiple word senses , and that the 100 - topic models exhibit this property to a greater extent .
For instance , Zeit ' time ' is highly predictive of three topics in the 50 - topic models , of which only the highest - weighted topic groups time expressions together ; in the 100 - topic models , Zeit is found in six topics .
Again , of these six , only the topic with the highest a consists of time expressions .
In the 50 - topic models , we find 11 topics that we cannot assign a coherent label ; in the 100 - topic models , there are 38 of these mismatched topics .
In our work to date , we have not found that lda models with greater numbers of topics find more Figure 1 : Verb clustering performance ( black ) and test set coverage ( grey ) of the LP model as a function of the number of nouns N included in the model .
specific concepts ; it is possible that this problem might be alleviated by careful filtering of the ( verb , grammatical relation , noun ) triples , but we leave this question to future research .
The LP model is very effective , which is surprising given its simplicity .
As expected , with increasing N , we do observe sparsity effects which hurt verb clustering performance ( see Figure 1 ).
Our best performing model is SUN .
Our best result is obtained with 10 , 000 nouns ( the maximum value of N that we tried ) in 1 , 000 classes , giving relatively fine - grained classes ( on average 10 nouns per class ).
Table 3 shows some example noun classes learned by the SUN model .
These include : groups with synonyms or near synonyms , often including alternate spellings of the same word ( such as in the truck grouping ); and groups of closely - related co - hyponyms , such as the body part grouping and the clothing grouping .
In the latter , bill , joint responsibility , complicity and inscription are also included as things which can be borne , this is due to the fact that the SUN noun clustering is based on triples of verbs , grammatical relations , and nouns .
LKW ( truck ), Lkw ( truck ), Lastwagen ( truck ), Castor ( container for highly radioactive material ), Laster ( truck ), Krankenwagen ( ambulance ), Transporter ( van ), Traktor ( tractor ) Hand ( hand ), Kopf ( head ), Fuß ( foot ), Haar ( hair ), Bein ( leg ), Arm ( arm ), Zahn ( tooth ), Fell ( fur ) _ Leiche ( corpse ), Leichnam ( body ), Schädel ( skull ), Skelett ( skeleton ), Wrack ( wreck ), Mumie ( mummy ), Trümmer ( debris ) Sauna ( sauna ), Badezimmer ( bathroom ), Schwimmbad ( swimming pool ), Nachbildung ( replica ), Kamin ( fireplace ), Aufenthaltsraum ( common room ), Mens a ( cafeteria ) Rechnung ( bill ), Kopftuch ( headscarf ), Uniform ( uniform ), Anzug ( suit ), Helm ( helmet ), Gewand ( garment ), Handschuh ( glove ), Mitverantwortung ( joint responsibility ), Bart ( beard ), Rüstung ( armour ), Mitschuld ( complicity ), Socke ( sock ), Jeans ( jeans ), Sonnenbrille ( sunglasses ), Aufschrift ( inscription ), Pullover ( sweater ), Weste ( vest ), Handschellen ( handcuffs ), Horner ( horns ), Kennzeichen ( marking ), Tracht ( traditional costume ), Korsett ( corset ), Schuhwerk ( footwear ), Kopfbedeckung ( headgear ), Pelz ( fur ), Maulkorb ( muzzle ) Missionar ( missionary ), Weihnachtsmann ( Santa Claus ), Selbstmordattentäter ( suicide bomber ), Bote ( messenger ), Nikolaus ( Nicholas ), Killer ( killer ), Bomber ( bomber ), Osterhase ( Easter bunny ) Table 3 : Example noun clusters in the SUN SP model .
Table 2 : Evaluation of the best SP models .
Furthermore , there are thematically related groups { corpse , body , etc ., and sauna , bathroom , etc .).
All months are placed together in one 12 - word group .
Some classes can be easily subdivided into separate groups , and sometimes the source for this can be guessed : For example , sports { football , golf , tennis ) are lumped together with musical instruments { guitar , piano , violin ) and film roles { starring role , supporting role ), these all being things that can be played .
Many groups of personal roles ( such as various kinds of government ministers ) are distinguished , as are diseases and medications ; other groupings contain proper names or geographical locations , sometimes of surprising specificity ( e . g ., authors , Biblical names , philosophers , NGOs , Eastern European countries , foreign currencies , German male first names , newspapers , television channels ).
The last group in Table 3 shows a grouping which appears to combine two of these semantic - ally narrow categories , in which Santa Claus and the Easter bunny are united with killers and suicide bombers .
4 . 2 Noun classes as sp concepts The wsm sp model is not as successful as sun , but , due to the methodological similarity between these two ( sp concepts modelled as hard partitions of nouns ), it affords us an opportunity to investigate the question of what properties might make for an effective noun partition .
The wsm model partitions nouns based on paradigmatic information ( which sentence contexts a noun appears in ), rather than sun ' s use of syntagmatic information ( which grammatical contexts a noun appears in ).
Therefore , it is perhaps not surprising that the noun classes derived by the wsm are organised thematically , and the synonym / co - hyponym structure observed in the sun noun classes is in many cases absent ( e . g ., { Pferd ( horse ), Reiter ( rider ), Stall ( stable ), Sattel ( saddle ), Stute ( mare )}; these classes can easily conflate semantic roles ( e . g ., Agent for rider and Location for stable ), which is presumably unhelpful for representing selectional preferences .
The distribution of noun classes also differs between sun and wsm .
The largest noun class in the wsm model contains 1 , 076 high - frequency nouns which are semantically unrelated { day , question , case , part , reason , kind , form , week , person , month ,...).
We suppose that these nouns are them - 5 -......... -......... -.........
Number of verb instances atically " neuUal " and are classed together by virtue of their usage in a wide variety of sentences .
This one noun class by itself subsumes 13 . 6 % of all noun tokens in SdeWaC .
wsm also includes 56 singleton noun classes ; the variance in noun class size is 2800 .
For comparison , in SUN , the largest noun class has 73 words , and the smallest , 2 ( there are 12 of these two - word classes ); noun class size variance is 37 .
The 73 - word class in SUN does indeed appear to be a grab bag ( including gas , taboo , pioneer , mustard , spy , mafia , and skinhead ), but these are uncommon words and account for only 0 . 1 % of noun tokens in SdeWaC .
The next two most common classes ( with some 40 nouns each ) are lists of names ( politicians ' surnames , and male first names ).
The noun class in the SUN model containing the largest number of high - frequency nouns ( 28 nouns : human , child , woman , man , people , Mr ., mother , father , ...) only covers 3 . 6 % of noun usages in SdeWaC and is both semantically cohesive and intuitively useful as a SP concept .
Figure 2 : Verb clustering performance of SP models as a function of number of verb instances .
These issues raise the question of why the WSM model is effective at all for verb classification .
We think that the larger less - related noun classes neither help nor hurt verb clustering , and we find that some of the thematic classes represent abstractions that should be useful for describing SPs .
Examples include lists of body parts , counUies ( separate classes for Europe , Africa , Asia , etc .
), diseases , human names , articles of clothing , and the group { fruit , apple , banana , pear , strawberry }.
4 . 3 Effects of test set size We were curious if the success of the LP model might be due to the size of the test set preventing sparsity from becoming a problem .
To pursue this question , we take the four best performing sp models and run the verb clustering evaluation with the number of verb instances in the test set varying between 10 , 000 and the full SdeWaC corpus ( 11 million ).
The results are displayed in Figure 2 .
This graph indicates that below 3 x 10 verb instances , sparsity seems to become a problem for all models on this task , and the baseline delivers the best performance .
Above this threshold , it seems that sparsity is not a major issue : lp performs fairly consistently , and is competitive with the sun model .
We attribute this to our use of the Jensen - Shannon divergence as a verb dissimilarity measure , which seems relatively robust to data sparsity .
The lda - hard model with its fewer topics seems to do quite well with fewer data ; as the test set size increases , it drops off in the rankings .
At the maximum number of verb instances , the best - performing models are sun , wsm and the lexical preferences .
The figure also shows that our evaluation metric is not smooth ( note , e . g ., the fluctuations in the baseline score ).
We believe that this reflects a degree of instability in the Ward ' s hierarchical clustering algorithm ; this clustering method is greedy , and clustering errors can be expected to propagate , which might explain the jaggedness of the plot .
4 . 4 Conclusions To conclude , we summarise the results of our analysis , using the questions formulated in the Introduction as guidelines .
First , we wanted to compare the efficiency of different classes of nouns as descriptors of selectional preference concepts .
Our findings suggest that noun classes are most effective when they are semantically highly consistent , representing groups of strongly related nouns .
It seems reasonable that sp concepts representing collections of synonyms would be useful for generalising observations , and should represent arguments better than simple lp .
A classification of proper names ( e . g ., as human , corporation , country , medication ) is also useful .
This implies that we can expect features such as animate to be shared by all members of a noun cluster .
Second , we were interested in the appropriate granularity of selectional preference concepts .
In our evaluation , we have observed a tendency for smaller , more specific noun classes to be superior ; this holds because data sparsity is not a problem in our experiment .
Beyond this finding , we would have liked to present a direct juxtaposition of different models on " granularity " but this is difficult : We have not yet identified a sttong abstraction of granularity from the proxies we use ( e . g ., GermaNet depth , or sun ' s N / M ).
Finally , which methods of classifying nouns into concepts are most effective at capturing selectional preferences for verb clustering ?
In our experiments , the sun and LDA - hard models proved to be more effective than lexical preferences , supporting our primary hypothesis that some level of SP concept granularity above the lexical level is desirable for verb clustering .
On the other hand , the LP model is only slightly worse than SUN and LDA - hard , making it attractive because it is so simple .
As we have shown , the potential data sparsity issues with LP can be alleviated by judiciously choosing the value of the N parameter that controls the number of nouns included in the model .
In addition , comparing the sun and wsm models , and observing the performance of the LDA - hard method , we conclude that inducing noun classes using syntagmatic information is more effective than using paradigmatic relations .
5 Related work In this study , we have looked at the utility of selectional preferences for automatic verb classification .
Some previous research has followed this line of inquiry , though prior studies have not compared alternative methods of modelling SPs .
Schulte im Walde ( 2006 ) presented a detailed examination of parameters for fc - means - based verb clustering in German , using the same gold standard that we employ here .
She reports on the effects of adding SP information to a SCF - based verb clustering using 15 high - level GermaNet synsets as SP concepts ; SP information for some combinations of grammatical relations improves clustering performance slightly , but neither are the effects consistent , nor is the improvement delivered by the SP model over the SCF - based baseline statistically significant .
Schulte im Walde et al .
( 2008 ) used expectation maximisation to induce latent verb clusters from the British National Corpus while simultaneously building a tree cut model of SPs on the WordNet hierarchy using a minimum description length method ; their evaluation focuses on the induced soft verb clusters , reporting the model ' s estimated perplexity of ( verb , grammatical relation , argument head ) triples .
The SPs are described qualitatively by presenting two example cases .
Sun and Korhonen ( 2009 ) study the effect of adding selectional preferences to a subcategorisation - based verb clustering in English using the sun model ( see Section 3 . 2 ).
They demonstrate that adding SPs to the scf preference data leads to the best results on their two clustering evaluations ; overall , their best results come from using SP information only for the subject grammatical relation .
They employ coarse SP concepts ( 20 or 30 noun clusters ) which capture general semantic categories ( Human , Building , Idea , etc .).
Selectional preferences are usually evaluated either from a word sense disambiguation standpoint using pseudo - words ( Chambers and Juraf - sky , 2010 ), or in terms of how acceptable an argument is with a verb , via regression against human plausibility judgements .
Several studies have compared SP methodologies from the latter perspective .
These include Brockmann and Lapata ( 2003 ), who compared three GermaNet - based models of SP , showing that different models were most effective for describing different grammatical relations ; Ö Séaghdha ( 2010 ), who compared different LDA - based models of SP , showing these to be effective for a variety of grammatical relations ; and Ö Séaghdha and Korhonen ( 2012 ), who show that WordNet tree cut models , LDA , and a hybrid LDA - WordNet model are effective for describing verb - object relations .
6 Future work Our GermaNet model delivered disappointing performance in this study ; we would be interested in seeing whether a more sophisticated implementation such as the tree cut model of Li and Abe ( 1998 ) would be more competitive .
We also would like to explore alternative noun clustering methods such as CBC ( Pantel and Lin , 2002 ) and Brown clusters ( Brown et al ., 1992 ), which were not covered in this work ; these would fit easily into our SP evaluation paradigm .
More challenging would be a verb classification - based evaluation of the s P models of ( Rooth et al ., 1999 ) and ( Schulte im Walde et al ., 2008 ), which use expectation maximisation to simultaneously cluster verbs into verb classes and nominal arguments into noun classes ; these approaches are not compatible with the evaluation framework we have used here .
Finally , the SP model of Bergsma et al .
( 2008 ) has also achieved impressive results on a number of tasks , but has not been investigated for use in verb classification .
Our verb clustering evaluation in this work has matched K , the number of clusters found by Ward ' s method , to the number of classes in the gold standard .
Since the number of clusters has an influence on the quality of the ensuing semantic classification ( Schulte im Walde , 2006 , page 180f ), we will also be running our experiments with different settings of K to explore whether this also influences the overall results of our evaluation .
