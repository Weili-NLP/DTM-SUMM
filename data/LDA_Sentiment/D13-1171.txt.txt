Open Domain Targeted Sentiment We propose a novel approach to sentiment analysis for a low resource setting .
The intuition behind this work is that sentiment expressed towards an entity , targeted sentiment , may be viewed as a span of sentiment expressed across the entity .
This representation allows us to model sentiment detection as a sequence tagging problem , jointly discovering people and organizations along with whether there is sentiment directed towards them .
We compare performance in both Spanish and English on microblog data , using only a sentiment lexicon as an external resource .
By leveraging linguistically - informed features within conditional random fields ( CRFs ) trained to minimize empirical risk , our best models in Spanish significantly outperform a strong baseline , and reach around 90 % accuracy on the combined task of named entity recognition and sentiment prediction .
Our models in English , trained on a much smaller dataset , are not yet statistically significant against their baselines .
1 Introduction Sentiment analysis is a multi - faceted problem .
Determining when a positive or negative sentiment is being expressed is a large part of the challenge , but identifying other attributes , such as the target of the sentiment , is also crucial if the ultimate goal is to pinpoint and extract opinions .
Consider the examples below , all of which contain a positive sentiment : ( 1 ) So happy that Kentucky lost to Tennessee !
( 2 ) Kentucky versus Kansas I can hardly wait ... ( 3 ) Kentucky is the best alley - oop throwing team since Sherman Douglas ' Syracuse squads !!
The entities in these examples are college basketball teams , and the events referred to are games .
In ( 1 ), although there is a positive sentiment , the target of the sentiment is an event ( Kentucky losing to Tennessee ).
However , from the positive sentiment toward this event , we can infer that the speaker has a negative sentiment toward Kentucky and a positive sentiment toward Tennessee .
In ( 2 ), the positive sentiment is toward a future event , but we are not given enough information to infer a sentiment toward the mentioned entities .
In ( 3 ), Kentucky is the direct target of the positive sentiment .
We can also infer a positive sentiment toward Douglas ' s Syracuse teams , and even toward Douglas himself .
These examples illustrate the importance of the target when interpreting sentiment in context .
If we are looking for sentiments toward Kentucky , for example , we would want to identify ( 1 ) as negative , ( 2 ) as neutral ( no sentiment ) and ( 3 ) as positive .
However , if we are looking for sentiment toward Tennessee , we would want to identify ( 1 ) as positive , and ( 2 ) and ( 3 ) as neutral .
The expression of these and other kinds of sentiment can be understood as involving three items : ( 1 ) An experiencer ( 2 ) An attitude ( 3 ) A target ( optionally ) Research in sentiment analysis often focuses on ( 2 ), predicting overall sentiment polarity ( Agarwal et al ., 2011 ; Bora , 2012 ).
Recent work has begun to combine ( 2 ) with ( 3 ), examining how to automatically predict the sentiment polarity expressed towards a target entity ( Jiang et al ., 2011 ; Chen et al ., 2012 ) for a fixed set of targets .
This topic - dependent sentiment classification requires that the target entity be Figure 1 : Sentiment expressed across an entity .
given , and returns statements expressing sentiment towards the given entity .
In this paper , we take a step towards open - domain , targeted sentiment analysis by investigating how to detect both the named entity and the sentiment expressed toward it .
We observe that sentiment expressed towards a target entity may be possible to learn in a graphical model along the span of the entity itself : Similar to how named entity recognition ( NER ) learns labels along the span of each word in an entity name , sentiment may be expressed along the entity as well .
A small example is shown in Figure 1 .
We focus on people and organizations ( volitional named entities ), which are the primary targets of sentiment in our microblog data ( see Table 1 ).
Both NER and opinion expression extraction have achieved impressive results using conditional random fields ( CRFs ) ( Lafferty et al ., 2001 ) to define the conditional probability of entity categories ( Mc - Callum and Li , 2003 ; Choi et al ., 2006 ; Yang and Cardie , 2013 ).
We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training ( Stoyanov and Eisner , 2012 ).
We learn our models on informal Spanish and English language taken from the social network Twitter , where the language variety makes NLP particularly challenging ( see Figure 2 ).
Our ultimate goal is to develop models that will be useful for low resource languages , where a sentiment lexicon may be known or bootstrapped , but more sophisticated linguistic tools may not be readily available .
We therefore do not rely on an external part - of - speech tagger or parser , which are often used for features in fine - grained sentiment analysis ; such tools are not available in many languages , and if they are , are not usually adapted for noisy social media .
Instead , we use information from sentiment lexicons and some simple hand - written features , and otherwise use only features of the word that can be @[ user ] le dijo erralo muy por lo bajo jaja un grande juancito grandes amigos mios www . twitter . com @ [ user ] he told him it was very on the dl haha a great juancito great friends of mine @[ user ] buenos dfas Profe !!
Nos quedamos acciden - tados otra vez en la carretera vieja guarenas echando gasoil , estamos a la interperie @ [ user ] good morning , Prof !!
We were wrecked again on the oldguarenas highway while getting diesel , we ' re out in the open Sin animo de ofender a los Militares , que realmente se merecen ese aumento y mas .
I do not intend to offend the military in the slightest , they truly deserve the raise and more .
However , I ' m wondering whether doctors will ever receive a similar compensation .
Figure 2 : Messages on Twitter use a wide range of formality , style , and errors , which makes extracting information particularly difficult .
Examples from Spanish ( screen names anonymized ), with approximate translations in English .
extracted without supervision .
These include features based on unsupervised word tags ( Brown clusters ) and a method that automatically syllabifies a word based on the orthography of the language .
All tools and code used for this research are released with this paper .
2 Related Work As the scale of social media has grown , using sources such as Twitter to mine public sentiment has become increasingly promising .
Commercial systems include Sentiment140 ( products and brands ) and tweetfeel ( suggests searching for popular movies , celebrities and companies ).
The majority of academic research has focused on supervised classification of message sentiment irrespective of target ( Barbosa and Feng , 2010 ; Pak and Paroubek , 2010 ; Bifet and Frank , 2010 ; Davidov et al ., 2010 ; Kouloumpis et al ., 2011 ; Agarwal et al ., 2011 ).
Large datasets are collected for this work by leveraging the sentiment inherent in emoticons ( e . g ., smilies and frownies ) and / or select Twitter hashtags ( e . g ., # bestdayever , # fail ), resulting in noisy collections appropriate for initial exploration .
Prior work includes : the use of a social network ( Speriosu et al ., 2011 ; Tan et al ., 2011 ; Calais Guerra et al ., 2011 ; Jiang et al ., 2011 ; Li et al ., 2012 ; Hu et al ., 2013 ); user - adapted models based on collaborative online - learning ( Li et al ., 2010b ); unsuper - vised , joint sentiment - topic modeling ( Saif et al ., 2012 ); tracking changing sentiment during debates ( Diakopoulos and Shamma , 2010 ); and how orthographic conventions such as word - lengthening can be used to adapt a Twitter - specific sentiment lexicon ( Brody and Diakopoulos , 2011 ).
www . m - mitchell . com / code 3 www . sentiment140 . com www . tweetfeel . com Efforts in targeted sentiment ( Bermingham and Smeaton , 2010 ; Jin and Ho , 2009 ; Li et al ., 2010a ; Jiang et al ., 2011 ; Tan et al ., 2011 ; Wang et al ., 2011 ; Li et al ., 2012 ; Chen et al ., 2012 ), have mostly focused on topic - dependent analysis .
In these approaches , messages are collected on a fixed set of topics / targets , such as products or sports teams , and sentiment is learned for the given set .
In contrast , we aim to predict sentiment in tweets for any named person or organization .
We refer to this task as open domain targeted sentiment analysis .
Within topic - dependent sentiment analysis , several approaches have explored applying CRFs or HMMs to extract sentiment and target words from text ( Jin and Ho , 2009 ; Li et al ., 2010a ).
In these approaches , opinion expressions are extracted , and polarity is annotated across the opinion expression .
However , as noted by many researchers in sentiment , opinion orientation towards a specific target is often not equal to the orientation of a neighboring opinion expression ; and opinion expressions in one context may not be opinion expressions in another ( Kim and Hovy , 2006 ), making open domain approaches particularly challenging .
The above work by Jiang et al .
( 2011 ) is most similar to our own .
They do not use joint learning , but they do incorporate a number of parse - based features designed to capture relationships between sentiment terms and topic references .
In our work these relationships are captured by the CRF model , and we compare against their approach in Section 6 .
Recent work by Yang and Cardie ( 2013 ) is similar in spirit to our own , where the identification of opinion holders , opinion targets , and opinion expressions is modeled as a sequence tagging problem using a CRF .
However , similar to previous work applying CRFs to extract sentiment , Yang and Cardie use syntactic relations to connect an opinion target to an opinion expression .
In contrast , we model the expression of sentiment polarity across the sentiment target itself , extracting both the sentiment target and the sentiment expressed towards it within the same span of words .
This allows us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target .
Most work in targeted sentiment outside the mi - croblogging domain has been in relation to product review mining ( e . g ., Yi et al .
( 2003 ), Hu and Liu ( 2004 ), Popescu and Etzioni ( 2005 ), Qiu et al .
Rather than identify named entities ( NEs ), this work seeks to identify products and their features mentioned in reviews , and classify these for sentiment .
Recent work by Qui et al .
jointly learns targets and opinion words , and Jakob and Gurevych ( 2010 ) use CRFs to extract the targets of opinions , but do not attempt to classify the sentiment toward these targets .
To the best of our knowledge , this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment .
Twitter Collection We use the Spanish / English Twitter dataset of Etter et al .
( 2013 ) to train and test our models .
Approximately 30 , 000 Spanish tweets and 10 , 000 English were labeled for named entities in BIO encoding : The start of an NE is labeled B -{ NE } and the rest of the NE is labeled i -{ NE }.
The Table 1 : Distribution of named entities in our Spanish Twitter corpus .
Targeted sentiment percentages are based on expert annotations from a random sample of 10 ( or all ) of of each entity .
Most entities are not sentiment targets ( neutral ).
person and organization are most frequent , and among the top recipients of sentiment .
full set of NE categories are shown in Table 1 .
For example , the sequence " Mark Twain " would be labeled b - person , i - person .
We are interested in both person and organization entities , which make up the majority of named entities in this data , and we evaluate these using the more general entity category volitional .
Removing retweets , 7 , 105 Spanish tweets contained a total of 9 , 870 volitional entities and 2 , 350 English tweets contained a total of 3 , 577 volitional entities .
Sentiment Lexicons We use two sentiment lexicon sources in each language .
For English , we use the MPQA lexicon ( Wilson et al ., 2005 ), which identifies 12 , 296 manually and semi - automatically produced subjective terms along with their polarity .
For the second lexicon , we use SentiWordNet 3 . 0 ( Baccianella et al ., 2010 ), which assigns positive and negative polarity scores to WordNet synsets .
We use the majority polarity of all words with a subjectivity score above 0 . 5 .
For Spanish , the first lexicon is obtained from Volkova et al .
( 2013 ), who automatically translated strongly subjective terms from the MPQA lexicon ( Wilson et al ., 2005 ) into Spanish .
The resulting Spanish lexicon contains about 65K words .
The second lexicon is available from Perez - Rosas et al .
This contains approximately 1000 sentiment - bearing words collected leveraging manual resources and 2000 collected leveraging automatic resources .
Annotation To collect sentiment labels , we use crowdsourcing through Amazon ' s Mechanical Turk .
Annotators (" Turkers ") were shown six tweets at a time , each with a single highlighted named entity .
Turkers were instructed to ( 1 ) select the sentiment being expressed towards the entity ( positive , negative , or no sentiment ); and ( 2 ) rate their level of confidence in their selection .
Following best practices on collecting language data with Mechanical Turk ( Callison - Burch and Dredze , 2010 ), two controls were placed among each set of six tweets to screen out unreliable judgments .
An example prompt is shown in Figure 3 .
Each ( tweet , NE ) pair was shown to three Turkers , and those with majority consensus on sentiment polarity were extracted .
Tweets without sentiment ORGANIZATION PERSON Named Entity Figure 4 : Targeted sentiment annotated for Spanish .
Majority www . mturk .
com / mturk Table 2 : Number of targeted sentiment instances where at least two of the three annotators ( Majority ) agreed .
Common disagreements with a third annotator ( Minority ) were over whether no sentiment or positive sentiment was expressed , and whether no sentiment or negative sent - ment was expressed .
consensus on all NEs were removed .
In Spanish , this yielded 6 , 658 unique ( tweet , NE ) pairs .
In English , which is a smaller data set , this yielded 3 , 288 unique pairs .
We split the data into folds for 10 - fold cross - validation , developing on the data from one fold and reporting results for the remaining nine .
The distribution of sentiment for the named entities annotated by Turkers is shown in Figure 4 .
Neutral ( no targeted sentiment ) dominates , followed by positive sentiment for both organizations and people .
As shown in Table 2 , common disagreements were over whether or not there was targeted positive sentiment , and whether or not there was targeted negative sentiment .
This is in line with previous research showing that distinguishing positive sentiment from no sentiment ( and distinguishing negative sentiment from no sentiment ) is often more challenging than distinguishing between positive and negative sentiment ( Wilson et al ., 2009 ).
Indeed , we see that it was more common for annotators to disagree than to agree on targeted sentiment , particularly for negative targeted sentiment , where more instances had neutral / negative disagreement than negative three - way agreement .
TWEET 2 Marque si el mensaje no esta en espanol o la entidad no es una persona ni una organizaciön .
Päse al siguiente Tweet .
Viendo todos dicen te quiero del gran WOODY ALLEN 1 .
La persona twitteando ( expresa un sentimiento positive ;] con relaciön a WOODY ALLEN en este mensaje .
Elige su nivel de confianza del sentimiento en relaciön a WOODY ALLEN .
Figure 3 : Example Tweet shown to Turkers .
Variable Possible values Sentiment ( s ) not - targ , sent - targ ( Pipe & Joint models ) Named Entity ( l ) o , b - volitional , i - volitional ( Pipe & Joint models ) Combined Sent / NE ( y ) o , b + not - targ , i + not - targ ( Coll models ) b + sent - targ , i + sent - targ Table 3 : Possible values for random variables , targeted subjectivity ( is / is not sentiment target ).
Coll models collapse targeted subjectivity and NE label into one node .
Sentiment ( s ) not - targ , pos , neg Combined Sent / NE ( y ) o , b + not - targ , i + not - targ ( Coll models ) b + pos , i + pos b + neg , i + neg Table 4 : Possible values for random variables , targeted sentiment .
The Coll models collapse both targeted sentiment and NE label into one node .
4 Targeted Subjectivity and Sentiment Formally , we define the problem as follows : Given an observed message w = ( w1 ... wn ), where n is the number of words in the message and Wj ( 1 < j < n ) is a word , we learn the probability of a label sequence l = ( l1 ... ln ), where lj G the set of named entity values ; and a sentiment sequence s = ( s1 ... sn ), where sj G the set of sentiment values .
We additionally explore simpler linear - chain models that learn the probability of a single label sequence y = ( y1 ... yn ), where yj G the set of conjoined entity + sentiment values ( Tables 3 and 4 ).
our basic model is a linear conditional random field , an undirected graph that represents the conditional distribution p ( l , s \! w ).
Sentiment towards a named entity may be modeled in a CRF as a sequence of random variables for sentiment s connected to named entities l . In all models , entity variables are connected by a factor to their neighbors in sequence , and we include skip - chains ( Finkel and Manning , 2010 ) connecting identical words where at least one is capitalized .
Our model strategies include : a pipeline that first learns volitional entities then sentiment directed towards them ( Pipe ); one that jointly learns volitional entities along with sentiment directed towards them ( Joint ); and one that learns volitional entities and targeted sentiment with combined labels ( Coll ) ( Figure 5 ).
For the Coll models , this is instead the conditional distribution p ( y \!
w ), where entity and sentiment labels are conjoined in one sequence assignment y .
Using these models , we explore two primary tasks : ( 1 ) the task of detecting whether sentiment is targeted at an entity , which we refer to as targeted subjectivity ; and ( 2 ) the task of detecting whether positive , negative , or neutral sentiment ( no sentiment ) is targeted at an entity , which we refer to as targeted sentiment .
Moving from targeted subjectivity prediction to targeted sentiment prediction is possible by changing the sentiment target ( sent - targ ) variable into two variables , one for positive targeted sentiment ( pos ) and one for negative ( neg ).
Possible values for targeted subjectivity are shown in Table 3 , and possible values for targeted sentiment are shown in Table 4 .
In the pipeline models ( Pipe ), we first build a CRF where each word is connected by a factor to an entity label lj G l . In a second model , every observed volitional entity node is connected by a factor to a sentiment label sj G s . An example is shown in Figure 5 ( 1 ).
In the joint models ( Joint ), each sj G s is connected by a factor to the corresponding entity label in the sequence , lj G l . Sentiment in this model is partially observed : All sentiment variables are treated as latent except for the sentiment connected to the volitional entity .
An example is shown in Figure 5 ( 2 ).
In the collapsed models ( Coll ), we combine sentiment and named entity into one label sequence ( e . g ., o , b + sent - targ , i + sent - targ ).
An example is shown in Figure 5 ( 3 ).
The Joint and Pipe models therefore predict named entity sequences , their category labels , and the sentiment expressed towards volitional named entities .
The collapsed models predict volitional labels and targeted sentiment as combined categories .
The Coll and Pipe models are considerably faster than Joint models , where exact inference is intractable .
Pipeline model ( Pipe ) Step 1 : Volitional Named Step 2 : Sentiment Entity Recognition 2 .
Joint model 3 .
Collapsed model Figure 5 : Example CRFs for targeted subjectivity with observed variables ( dark nodes ), predicted variables ( white nodes ) and hidden variables ( light grey nodes ).
5 Training Minimum - Risk CRF Training We use the ERMA system ( Stoyanov et al ., 2011 ) to learn our models .
ERMA ( Empirical Risk Minimization under Approximations ) learns parameters to minimize loss on the training data .
Predicting NE labels using a linear - chain CRF trained with empirical risk minimization has been shown to result in a statistically significant improvement over the common approach of maximum likelihood estimation ( Stoyanov and Eisner , 2012 ).
All models are trained to optimize log likelihood using 20 iterations of stochastic gradient descent , and a maximum of 100 iterations of belief propagation to compute the marginals for each example .
We found that learning the VOLITIONAL categories during training rather than maintaining beliefs about separate named entities during inference ( ORGANIZATION , PERSON ) and then post - processing to VOLITIONAL leads to slightly better accuracy .
sites . google . com / site / ermasoftware Features Features of the models are shown in Table 5 .
For an observed word , features are extracted for the word itself as well as within a context window of three words in either direction .
Words seen only once are treated as out - of - vocabulary .
Surface features and linguistic features are concatenated in groups of two and three to create further features .
All algorithms and code that we have developed for feature extraction are available online .
Because we aim to develop models that do not heavily rely on language - specific resources , we are interested in exploring unsupervised and lightly supervised methods for learning relevant features .
Rather than use part - of - speech tags , we therefore use Brown cluster labels as unsupervised word tags ( Brown et al ., 1992 ; Koo et al ., 2008 ).
Brown clustering is a distributional similarity method that merges pairs of word clusters in the training datato create the smallest decrease in corpus likelihood , using a bigram language model on the clusters .
For our task , we cut clusters at length 3 and length 5 , and these serve as rough part - of - speech tags without the need to train additional models .
For example , the word hello is tagged as belonging to cluster 011 ( length 3 ) and 01111 ( length 5 ).
During development , we found that being able to syllabify the word ( break the word into syllables ) was a positive indicator of people names , but a negative indicator of organization names .
This observation can be approximated automatically using constraints from the sonority sequencing principle ( Hooper , 1976 ; Clements , 1990 ; Blevins , 1996 ; Morelli , 2003 ) on a language ' s orthography .
This is a phonotactic principle that states that syllables will tend to have a sonority peak , usually a vowel , in the center of the syllable , followed on either side by consonants with decreasing sonority .
Although languages may violate this principle , the core idea that a vowel forms the nucleus of a syllable with optional consonants before ( the onset ) and after ( the coda ) can be used to begin to automatically learn syllable structure .
We learn this in an unsupervised way , using the most frequent ( seen more than 1 , 000 times ) word - initial non - vowel sequences from the Brown cluster data as allowable syllable onset consonants .
Similarly , the most frequent word - final non - vowel sequences are learned as possible syllable codas .
For each word , we then attempt to segment syllables using the learned onsets and codas around each vowel .
If a word cannot be syllabified , it is often an initialism ( e . g ., CND , Isat ).
www . m - mitchell . com / code For Spanish , we train on a sample of ~ 7 million Spanish tweets .
For English , we train on the essays ( Pennebaker et al ., 2007 ) and Facebook data ( Kosinskia et al ., 2013 ) available from ICWSM 2013 .
We follow the approach from the out - of - vocabulary assignment in the Berkeley parser ( Petrov et al ., 2006 ) to encode common surface patterns such as capitalization and lexical patterns such as verb endings as a single feature for words we have seen once or less .
We also use the Jerboa toolkit ( Van Durme , 2012 ) to extract further language - independent features from the data , such as features for emoticons and binning for repeated characters ( like !!!).
In addition , we include features for whether the word is three or four letters , which is often used for acronyms and initialisms in several languages ( including Spanish and English ); whether the word is neighbored by a punctuation mark ; word identity ; word length ; message length ; and position in the sentence .
We utilize a speaker of each language to simply list word forms for sentiment features that may be indicative of sentiment , totaling less than two hours of annotation time .
This set includes intensifiers ( e . g ., hella , freakin ' in English ; e . g ., muy , suma - mente in Spanish ), positive / negative abbreviations ( WTF , pso ), positive / negative slang words , and positive / negative prefix and suffixes ( e . g ., anti - in English and Spanish , - ito in Spanish ).
6 Experiments We are interested in both person and organization entities , and evaluate these in the collapsed category volitional .
This suggests that the data may be pre - processed to label all volitional entities as volitional NEs , or the models may be learned with the traditional named entities in place , and post - Further development is necessary to extend a similar idea to languages that do not ordinarily mark all vowels in their orthography , such as Hebrew and Arabic .
Surface features binned word length , message length , and sentence position ; Jerboa features ; word identity ; word lengthening ; punctuation characters , has digit ; has dash ; is lower case ; is 3 or 4 letters ; first letter capitalized ; more than one letter capitalized , etc .
Linguistic features function words ; can syllabify ; curse words ; laugh words ; words for good , bad , no , my ; slang words ; abbreviations ; intensifiers ; subjective suffixes and prefixes ( such as diminutive forms ); common verb endings ; common noun endings Brown clustering features cluster at length 3 ; cluster at length 5 Sentiment features is sentiment - bearing word ; prior sentiment polarity Table 5 : Features used in model .
processed to identify those that are volitional .
We explored results using both methods , and found that training models on volitional tags yielded the best performance overall ; we report numbers for this approach below .
We compare against a baseline ( Base - NS ) where we use our volitional entity labels and assign no sentiment directed towards the entity ( the majority case ).
This is a strong baseline to isolate how our methods perform specifically for the task of identifying sentiment targeted at an entity .
We report on precision , recall , and sensitivity for the tasks of NER and targeted subjectivity / sentiment prediction in isolation ; and we report on accuracy for the targeted subjectivity and targeted sentiment models .
For sentiment , a true positive is an instance where the label has sentiment , and a true negative is an instance where the label has no sentiment ( neutral ).
For NER , a true positive is an instance where the label is a b - or i - label ; a true negative is an instance where the label is o .
The three systems are evaluated against one another for NER , subjectivity ( entity has / does not have sentiment expressed towards it ), and sentiment ( positive / negative / no sentiment ) using paired t - tests across folds , with a Bon - ferroni correction to set a to 0 . 02 .
NER We include results for the isolated task of volitional named entity recognition in Table 6 . in both Spanish and English , all three models are roughly comparable for precision , recall , and specificity .
The task of finding o tags - spans that are not named entities - works especially well ( NE spec ).
Common Table 6 : Average precision , recall , and specificity for volitional entity NER ( in %).
mistakes include confusing b - labels with i - labels .
Subjectivity and Sentiment Table 7 shows results for the isolated task of predicting the presence of sentiment about a volitional entity .
In Spanish , the pipeline models ( Pipe ) perform optimally for subjectivity recall ( Subj rec ), and significantly above the Coll models ( p <.
Precision and specificity are comparable across models .
In English as in Spanish , the collapsed model is particularly poor at subjectivity recall .
As discussed in Section 2 , the subtask of predicting whether subjectivity is expressed towards an entity is comparable to the main task of Jiang et al .
( 2011 ), and so we compare our approach here .
The Jiang et al .
study is similar to the current study in that they aim to detect targeted sentiment , but it differs from the current study in that they focus exclusively on subjectivity towards five manually selected entities : { Obama , Google , iPad , Lakers , Lady Gaga }.
They also evaluate on artificially balanced evaluation data , and evaluate sentiment polarity ( positive / negative ) separately from subjectivity ( has / does not have sentiment ).
Our dataset includes any entity labeled as person or organization , and is not balanced ( most targets have no sentiment expressed towards them ; see Table 1 ), thus we can only roughly compare against their approach .
Lakers and Lady Gaga are rare in our collection ( appearing less than 3 times ), and so we updated the comparison set prior to evaluation to : { Obama , Google , iPad , BBC , Tebow }.
On this set , a baseline that always guesses no sentiment reaches an accuracy of 66 . 9 %, compared to Jiang et al .
' s 65 . 5 % accuracy on a balanced set ( not strictly comparable , but provided for reference ).
The joint models reach an accuracy of 71 . 04 % on this set , demonstrating this approach as potentially useful for topic - dependent targeted sentiment .
Table 8 shows results for the task of predicting the polarity of the sentiment expressed about an entity .
In Spanish , the Pipe models significantly out - Spanish English Model Joint Pipe Coll \!\!
Joint Pipe Coll Subj prec 583 58 . 8 58 . 9 46 . 6 52 . 2 45 . 9 Subj rec 40 . 1 50 . 9 19 . 1 44 . 5 48 . 5 16 . 4 Subj spec [ I 79 . 6 77 . 5 77 . 8 \!\!
77 . 6 80 . 8 74 . 0 Table 7 : Average precision , recall , and specificity ( in %) for subjectivity prediction ( has / does not have sentiment ) along the target entity .
Spanish English Model Joint Pipe Coll \!\!
Joint Pipe Coll Table 8 : Average precision , recall , and specificity ( in %) for sentiment prediction ( positive / negative / no sentiment ) along the target entity .
perform the Coll models on sentiment recall , and the Joint models on sentiment precision ( p <.
In English , Pipe significantly outperforms Joint on precision ( p <.
Targeted Subjectivity and Targeted Sentiment The Joint and Pipe models work reasonably well for the isolated tasks of NER and subjectivity / sentiment prediction .
We now examine results for targeted subjectivity - labeling an entity and predicting whether there is sentiment directed towards it - in Table 9 ; and targeted sentiment - labeling an entity and predicting what the sentiment directed towards it is - in Table 10 .
We evaluate using two accuracy metrics : Acc - all , which measures the accuracy of the entire named entity span along with the sentiment span ; and Acc - Bsent , which measures the accuracy of identifying the start of a named entity ( b - labels ) along with the sentiment expressed towards it .
Acc - all primarily measures the correctness of o labels , while Acc - Bsent focuses on the beginning of named entities .
For the targeted subjectivity task , our Joint models perform optimally in Spanish , and significantly above their baselines .
For the Acc - Bsent task , Joint models perform best , significantly outperforming their baseline for subjectivity prediction .
In English , where our data is half the size , we do not see a statistically significant difference between the predictive models and the no sentiment baselines .
For the targeted sentiment task , the Joint models again perform relatively well in Spanish ( Table 10 ), labeling volitional entities , predicting whether or not there is sentiment targeted towards them , and Table 9 : Average accuracy on Targeted Subjectivity Prediction : Identifying volitional entities and whether they are a sentiment target .
In the core task , Acc - Bsent , the best model in Spanish is JOINT , significantly outperforming the baseline .
In English , the best model ( PIPE ) does not significantly improve over its baseline .
Table 10 : Average accuracy on Targeted Sentiment Prediction : Identifying volitional entities and the polarity of the sentiment expressed towards them .
The Spanish Joint models significantly improve over their baseline for the core task .
In English , no models outperform their baseline .
the sentiment polarity above their no sentiment baselines .
We find this to be the most difficult task : It may be clear that sentiment is being expressed towards an entity , but it is not always clear what the polarity of that sentiment is .
Error analysis is given below in this section .
In the smaller English set , the models do not outperform the no sentiment baseline .
7 Discussion Feature Analysis Examples of some of the top - weighted features in the Spanish models are shown in Table 11 .
In addition to lexical identity and Brown cluster , we find that positive indicators include positive suffixes such as diminutive forms , whether the word can be syllabized ( Section 5 ), and whether it is three or four letters .
Error Analysis Because it is relatively common for there not to be sentiment targeted at a named entity , it is difficult to tease out the polarity in instances where there is targeted sentiment .
Similarly , our predictions are most reliable for detecting the absence of a named entity ( O labels ).
Label confusions are shown in Table 12 .
Mistakes are often made by confusing B - labels ( the start of B - VOLITIONAL FEATURES Negative is a function word ; jerboa tags ; followed by a word with 3 or 4 letters that cannot be syllabified Positive ends in - a , - o , or - s ; is capitalized ; has one noninitial capital letter ; is 3 or 4 letters B - VOLITIONAL , POS FEATURES Negative preceded by a curse word ; followed by a word with a positive suffix ; immediately preceded by a word with a negative prefix Positive not in a sentiment lexicon ; preceded by a happy emoticon ; followed by an exclamation or a ' my ' word ; immediately preceded by a laugh ; has two or more sentiment - bearing words in the sentence B - VOLITIONAL , NEG FEATURES Negative is immediately followed by a question mark or positive abbreviation word Positive preceded by a ' bad ' word or curse word ; has four or more sentiment lexicon items B - VOLITIONAL , NOT - TARG FEATURES Negative immediately followed by a ' no ' word or word with a negative prefix ; is preceded by a question mark ; is immediately preceded by a curse word or laugh ; is followed by an exclamation mark Positive not followed by sentiment lexicon word Table 11 : Example strongly weighted features for a Spanish joint sentiment model .
In addition to lexical identity , we find that curse words and positive and negative prefixes are used to detect volitional entities and the sentiment directed towards them .
an entity ) with I - labels ( inside an entity ); and by predicting sentiment polarity when the gold annotations say there is not sentiment targeted at the entity .
Some example errors are shown in Figure 13 .
In ( 1 ), " CANSADO " (" TIRED ") was predicted to be volitional , while " Matthew " was not .
In ( 2 ), " Ma - tias del rlo " was not predicted to be an entity , likely due to the fact that the capitalization patterns we see in this sentence are indicative of the start of a sentence rather than a proper name ( similar to 1 ).
Observed Observed Table 12 : Predicted vs . observed values for a joint model .
( a ) For named entities , most common confusions were between b - volitional and o labels .
( b ) For sentiment , most common mistakes were to predict that a positive sentiment was neutral ( no sentiment ), and that a neutral sentiment was negative .
NE prediction errors 2 .
Sentiment prediction errors 3 .
Sentiment and NE prediction errors 5 . tion : " dio " should be " dios ", meaning " God "; otherwise , " dio " is the word for " gave ".
Humans can easily fix the spelling error , which changes the overall reading of the expression .
In ( 4 ), the positive polarity item " verdad " (" believe ") and the exclamation marks (!!!)
were likely used as indicators of positive sentiment ; however , in this case the annotators marked the targeted sentiment as neutral .
In ( 5 ), the " Humala " entity was predicted to be longer than it is (" Hamala dos " or " Hamala two " ).
It was also predicted that both " Giesecke " and " Eiguiguren " had no sentiment expressed towards them ; annotators disagreed , with the majority of those who annotated " Giesecke " marking negative sentiment , and the majority of those who annotated " Eiguiguren " marking no sentiment .
This highlights some of the difficulty in predicting sentiment discussed in Section 3 , where annotators will often disagree as to whether there is no sentiment or positive / negative sentiment .
During development , we found that the collapsed model ( COLL ) performed best on small amounts of data .
However , as we scaled up the amount of data we trained on , the PIPE and JOINT models significantly improved , while the COLL models did not have significant performance gains .
Table 13 : Example errors made by joint models .
8 Conclusion We have introduced the task of open domain targeted sentiment : predicting sentiment directed towards an entity along with discovering the entity itself .
Our approach is developed to find targeted sentiment towards both person and organization named entities by modeling sentiment as a span along the entity .
We find that by modeling targeted sentiment in this way , we can reliably detect entities and whether or not they are sentiment targets above a no sentiment baseline .
How best to determine the polarity of the sentiment expressed towards the entity , however , is still an open issue .
Our data suggests that it is usually not clear - cut whether sentiment is being expressed or not ; the strong disagreement between annotators suggests that detecting sentiment polarity in microblogs is difficult even for humans .
In future work , we hope to explore further methods for teasing apart sentiment polarity expressed towards a target .
This research has achieved promising results for detecting sentiment targets without relying on external supervised models , and we hope that the features and approaches developed here can aid in sentiment analysis in noisy text and languages without rich linguistic resources .
1 . sentiment may not be clear with
