Explaining the Stars : Weighted Multiple - Instance Learning for Aspect - Based Sentiment Analysis This paper introduces a model of multiple - instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user - contributed texts such as product reviews .
Each variable - length text is represented by several independent feature vectors ; one word vector per sentence or paragraph .
For learning from texts with known aspect ratings , the model performs multiple - instance regression ( MIR ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings .
Next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions .
We evaluate the model on seven multi - aspect sentiment analysis data sets , improving over four MIR baselines and two strong bag - of - words linear models , namely SVR and Lasso , by more than 10 % relative in terms of MSE .
1 Introduction Sentiment analysis of texts provides a coarsegrained view of their overall attitude towards an item , either positive or negative .
The recent abundance of user texts accompanied by real - valued labels e . g .
on a 5 - star scale has contributed to the development of automatic sentiment analysis of reviews of items such as movies , books , music or other products , with applications in social computing , user modeling , and recommender systems .
The overall sentiment of a text towards an item often results from the ratings of several specific aspects of the item .
For instance , the author of a review might have a rather positive sentiment about a movie , having particularly liked the plot and the music , but not too much the actors .
Determining the ratings of each aspect automatically is a challenging task , which may seem to require the engineering of a large number of features designed to capture each aspect .
Our goal is to put forward a new feature - agnostic solution for analyzing aspect - related ratings expressed in a text , thus aiming for a finer - grained , deeper analysis of text meaning than overall sentiment analysis .
Current state - of - the - art approaches to sentiment analysis and aspect - based sentiment analysis , attempt to go beyond word - level features either by using higher - level linguistic features such as POS tagging , parsing , and knowledge infusion , or by learning features that capture syntactic and semantic dependencies between words .
Once an appropriate feature space is found , the ratings are typically modeled using a linear model , such as Support Vector Regression ( SVR ) with £ 2 norm for regularization or Lasso Regression with l \ norm .
By treating a text globally , these models ignore the fact that the sentences of a text have diverse contributions to the overall sentiment or to the attitude towards a specific aspect of an item .
In this paper , we propose a new learning model which answers the following question : " To what extent does each part of a text contribute to the prediction of its overall sentiment or the rating of a particular aspect ?"
The model uses multiple - instance regression ( MIR ), based on the assumption that not all the parts of a text have the same contribution to the prediction of the rating .
Specifically , a text is seen as a bag of sentences ( instances ), each of them modeled as a word vector .
The overall challenge is to learn which sentences refer to a given aspect , and how they contribute to the text ' s attitude towards it , but the model applies to overall sentiment analysis as well .
For instance , Figure 1 displays a positive global comment on a TED talk and the weights assigned to two of its sentences by MIR .
Output Y S2 ( w = 0 . 05 ): However , I think our worst risk is not technological , but political .
Figure 1 : Analysis of a comment ( bag of sentences { s \, Sj }) annotated by humans with the maximal positive sentiment score ( 5 stars ).
The weights assigned by MIR reveal that s \ has the greatest relevance to the overall sentiment .
Using regularized least squares , we formulate an optimization objective to jointly assign instance weights and regression hyperplane weights .
Then , an instance relevance estimation method is used to predict aspect ratings , or global ones , in previously unseen texts .
The parameters of the model are learned using an alternating optimization procedure inspired by Wagstaff and Lane ( 2007 ).
Our model requires only text with ratings for training , with no particular assumption on the word features to be extracted , and provides interprétable explanations of the predicted ratings through the relevance weights assigned to sentences .
We also show that the model has reasonable computational demands .
The model is evaluated on aspect and sentiment rating prediction over seven datasets : five of them contain reviews with aspect labels about beers , audiobooks and toys ( McAuley et al ., 2012 ), and two contain TED talks with emotion labels , and comments on them with sentiment labels ( Pappas and Popescu - Belis , 2013 ).
Our model outperforms previous MIR models and two strong linear models for rating prediction , namely SVR and Lasso by more than 10 % relative in terms of MSE .
The improvement is observed even when the sophistication of the feature space increases .
The paper is organized as follows .
Section 2 shows how our model innovates with respect to previous work on MIR and rating prediction .
Section 3 formulates the problem while Section 4 describes previous MIR models .
Section 5 presents our MIR model and learning procedure .
Section 6 presents the datasets and evaluation methods .
Section 7 reports our results on rating prediction tasks , and provides examples of rating explanation .
2 Related Work 2 . 1 Multiple - Instance Regression Multiple - instance regression ( MIR ) belongs to the class of multiple - instance learning ( MIL ) problems for real - valued output , and it is a variant of multiple regression where each data point may be described by more than one vectors of values .
Many MIL studies focused on classification ( Andrews et al ., 2003 ; Bunescu and Mooney , 2007 ; Settles et al ., 2008 ; Foulds and Frank , 2010 ; Wang et al ., 2011 ) while fewer focused on regression ( Ray and Page , 2001 ; Davis and others , 2007 ; Wagstaff et al ., 2008 ; Wagstaff and Lane , 2007 ).
Related to document analysis , several MIR studies have focused on news categorization ( Zhang and Zhou , 2008 ; Zhou et al ., 2009 ) or web - index recommendation ( Zhou et al ., 2005 ) but , to our knowledge , no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real - valued labels .
MIR was firstly introduced by Ray et al .
( 2001 ), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label .
Wagstaff and Lane ( 2007 ) proposed to simultaneously learn a regression model and estimate instance weights per bag for crop yield modeling ( not applicable to prediction ).
A similar method which learns the internal structure of bags using clustering was proposed by Wagstaff et al .
( 2008 ) for crop yield prediction , and we will use it for comparison in the present study .
Later , the method was adapted to map bags into a single - instance feature space by Zhang et al .
Wang et al .
( 2008 ) assumed that each bag is generated by random noise around a primary instance , while Wang et al .
( 2012 ) represented bag labels with a probabilistic mixture model .
( 2010 ) concluded that various assumptions are differently suited to different tasks , and should be stated clearly when describing an MIR model .
2 . 2 Rating Prediction from Text Sentiment analysis aims at analyzing the polarity of a given text , either with classification ( for discrete labels ) or regression ( for real - valued labels ).
Early studies introduced machine learning techniques for sentiment classification , e . g .
Pang et al .
( 2002 ), including unsupervised techniques based on the notion of semantic orientation of phrases , e . g .
Turney et al .
Other studies focused on subjectivity detection , i . e .
whether a text span expresses opinions or not ( Wiebe et al ., 2004 ).
Rating inference was defined by Pang et al .
( 2005 ) as multi - class classification or regression with respect to rating scales .
Pang and Lee ( 2008 ) discusses the large range of features engineered for this task , though several recent studies focus on feature learning ( Maas et al ., 2011 ; Socher et al ., 2011 ), including the use of a deep neural network ( Socher et al ., 2013 ).
In contrast , we do not make any assumption about the nature or dimensionality of the feature space .
The fine - grained analysis of opinions regarding specific aspects or features of items is known as multi - aspect sentiment analysis .
This task usually requires aspect - related text segmentation , followed by prediction or summarization ( Hu and Liu , 2004 ; Zhuang et al ., 2006 ).
Most attempts to perform this task have engineered various feature sets , augmenting words with topic or content models ( Mei et al ., 2007 ; Titov and McDonald , 2008 ; Sauper et al ., 2010 ; Lu et al ., 2011 ), or with linguistic features ( Pang and Lee , 2005 ; Baccianella et al ., 2009 ; Qu et al ., 2010 ; Zhu et al ., 2012 ).
Other studies have advocated joint modeling of multiple aspects ( Snyder and Barzilay , 2007 ) or multiple reviews for the same product ( Li et al ., 2011 ).
McAuley et al .
( 2012 ) introduced new corpora of multi - aspect reviews , which we also partly use here , and proposed models for aspect detection , sentiment summarization and rating prediction .
Lastly , joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al .
None of the above studies considers the multiple - instance property of text in their modeling .
3 MIR Definition Let us consider a set B of m bags with numerical labels Y as input data D = {({ bijjt ^ Vi ), -, ({ bmj } tm , ym )}, where b ^ G Rd ( for 1 < j < m ) and yt G R . Each bag Bi consists of rii data points ( called ' instances '), hence it is a matrix of rii d - dimensional vectors , e . g .
word vectors .
The challenge is to infer the label of the bag given a variable number of instances rii .
This requires finding a set of bag representations X = { x \,..., xm } of size m where Xi G Rd , from which the class labels can be computed .
The goal is then to find a mapping from this representation , noted $ : Rd —> R , which is able to predict the label of a given bag .
Ideally , assuming that X is the best bag representation for our task , we look for the optimal regression hyper - plane $ which minimizes a loss function C plus a regularization term Q as follows : loss reg .
Since the best set of representations X for a task is generally unknown , one has to make assumptions to define it or compute it jointly with the regression hyperplane $.
Thus , the main difficulty lies in finding a good assumption for X , as we will now discuss .
4 Previous MIR Assumptions We describe here three assumptions frequently made in past MIR studies , to which we will later compare our model : aggregating all instances , keeping them as separate examples , or choosing the most representative one ( Wang et al ., 2012 ).
For each assumption , we will experiment with two state - of - the - art regression models ( noted abstractly as /), namely SVR ( Drucker et al ., 1996 ) and Lasso ( Tibshirani , 1996 ) with respectively the £ 2 and £\ norms for regularization .
The Aggregated algorithm assumes that each bag is represented as a single d - dimensional vector , which is the average of its instances ( hence Xi G Rd ).
Then , a regression model / is trained on pairs of vectors and class labels , Dagg = {( xi , yi ) I i = 1 ,..., m }, and the predicted class of an unlabeled bag Bi = { bij \ j = 1 ,... , rii } is computed as follows : In fact , a simple sum can also be used instead of the mean , and we observed in practice that with an appropriate regularization there is no difference on the prediction performance between these options .
This baseline corresponds to the typical approach for text regression tasks , where each text sample is represented by a single vector in the feature space ( e . g .
BOW with counts or TF - IDF weights ).
The Instance algorithm considers each of the instances in a bag as separate examples , by labeling each of them with the bag ' s label .
A regression model / is learned over the training set made of all vectors of all bags , Dms = {{ bij , yi ) \ j = 1 ,... , m ; i = 1 ,..., m }, assuming that there are m labeled bags .
To label a new bag Bi , given that there is no representation Xi , the method simply averages the predicted labels of its instances : Instead of the average , the median value can also be used , which is more appropriate when the bags contain outlying instances .
The Prime algorithm assumes that a single instance in each bag is responsible for its label ( Ray and Page , 2001 ).
This instance is called the primary or prime one .
The method is similar to the previous one , except that only one instance per bag is used as training data : Dpri = {( b ^, yi ) \ i = 1 ,..., m }, where b ^ is the prime instance of the ith bag Bi and m is the number of bags .
The prime instances are discovered through an iterative algorithm which refines the regression model /.
Given an initial model /, in each iteration the algorithm selects from each bag a prime candidate which is the instance with the lowest prediction error .
Then , a new model is trained over the selected prime candidates , until convergence .
For a new bag , the target class is computed as in Eq .
5 Proposed MIR Model We propose a new MIR model which assigns individual relevance values ( weights ) to each instance of a bag , thus making fewer simplifying assumptions than previous models .
We extend instance - relevance algorithms such as ( Wagstaff and Lane , 2007 ) by supporting high - dimensional feature spaces , as required for text regression , and by predicting both the class label and the content structure of previously unseen ( hence unlabeled ) bags .
The former is achieved by minimizing a regularized least squares loss ( RLS ) instead of solving normal equations , which is prohibitive in large spaces .
The latter represents a significant improvement over Aggregated and Instance algorithms , which are unable to pinpoint the most relevant instances with respect to the label of each bag , being thus applicable only to bag label prediction .
Similarly , Prime only identifies the prime instance when the bag is already labeled .
Instead , our model learns an optimal method to aggregate instances , rather than a pre - defined one , and allows more degrees of freedom in the regression model than previous ones .
Moreover , the weight of an instance is interpreted as its relevance both in training and prediction .
5 . 1 Instance Relevance Assumption Each bag defines a bounded region of a hyper - plane orthogonal to the y - axis ( the envelope of all its points ).
The goal is to find a regression hyperplane that passes through each bag Bi and to predict its label by using at least one data point Xi within that bounded region .
Thus , the point Xi is a convex combination of the points in the bag , in other words Bi is represented by the weighted average of its instances bij : where ipij is the weight of the jth instance of the ith bag .
Each weight ipij indicates the relevance of an instance j to the prediction of the class yi of the ith bag .
The constraint forces Xi to fall within the bounded region of the points in bag i and guarantees that the ith bag will influence the regressor .
5 . 2 Modeling Bag Structure and Labels Let us consider a set of m bags , where each bag Bi is represented by its rii d - dimensional instances , i . e .
along with the set of target class labels for each bag , Y = { yi } N , yi S M . The representation set of all Bi in the feature space , X = { x \,..., xm }, Xi G M . d , is obtained using the rii instance weights associated to each bag Bi , ■ 0i = {^ i ^ ni , ipij e [ 0 , 1 ] which are initially unknown .
Thus , we look for a linear regression model / that is able to model the target values using the regression coefficients $ G Md , where X and Y are respectively the sets of training bags and their labels : Y = f ( X ) = $ TX .
We define a loss function according to the least squares objective dependent on X , Y , $ and the set of weight vectors \ I > = { ip \,..., ipm } using Eq .
4 as follows : Using the above loss function , accounting for the constraints of our assumption in Eq .
4 and assuming ^ 2 - norm for regularization with ei and ê2 terms for each and $ respectively , we obtain the following least squares objective from Eq .
1 : fx loss fx re : h loss subject to ipij > 0 \/ i , j and Y ^ Li ipij = 1 Vi The selection of the .
£ 2 - norm was based on preliminary results showing that it outperforms ^ i - norm .
Other combinations of p - norm regularization can be explored for f \ and / 2 , e . g .
to learn sparser instance weights and denser regression coefficients or vice versa .
The above objective is non - convex and difficult to optimize because the minimization is with respect to all ^ i ,..., ipm and $ at the same time .
As indicated in Eq .
6 above , we will note f \ a model that is learned from the minimization only with respect to ip \,..., ipm and / 2 a model obtained from the minimization with respect to $ only .
6 , we can observe that if one of the two is known or held fixed , then the other one is convex and can be learned with the well - known least squares solving techniques .
In Section 5 . 3 , we will describe an algorithm that is able to exploit this observation .
Having computed ipi ,..., ipm and $, we could predict a label for an unlabeled bag using Eq .
3 , but would not be able to compute the weights of the instances .
Moreover , information that has been learned about the instances during the training phase would not be used during prediction .
For these reasons , we introduce a third regression model fs with regression coefficients O G M . d assuming a ^ 2 - norm for the regularization with es term , which is trained on the relevance weights obtained from the Eq .
6 , Dw = {{ bij , ipij ) \ i = l ,..., m ; j = l ,..., rii }.
The optimization objective for the / 3 model is the following : ».
fs loss function This minimization can be easily performed with the well - known least squares solving techniques .
The learned model is able to estimate the weights of the instances of an unlabeled bag during prediction time as : ipi = f : i ( Bi ) = QTBi .
The ipi weights are estimations which are influenced by the relevance weights learned in our minimization objective of Eq .
6 but they are not constrained at prediction time .
To obtain interprétable weights , we can convert the estimated scores to the [ 0 , 1 ] interval as follows : ipi = ipi / sum ( ipi ).
Finally , the prediction of the label for the ith bag using the estimated instance weights ipi is done as follows : 5 . 3 Learning with Alternating Projections Algorithm 1 solves the non - convex optimization problem of Eq .
6 by using a powerful class of methods for finding the intersection of convex sets , namely alternating projections ( AP ).
The problem is firstly divided into two convex problems , namely f \ loss function and / 2 loss function , which are then solved in an alternating fashion .
Like EM algorithms , AP algorithms do not have general guarantees on their convergence rate , although , in practice , we found it acceptable at generally fewer than 20 iterations .
Initialize ( i /> i ,..., ipN , <&, X ) while not converged do for Bi in B do ipi = cRLS ( ß > TBi , Yi , ex ) # fx model Xi = Biipf end for Instance weighting Figure 2 : Visual representation for the training and testing procedure of Algorithm 1 .
The algorithm takes as input the bags Bi , their target class labels Y and the regularization terms ei , Ê2 , £ 3 and proceeds as follows .
First , under a fixed regression model ( J2 ), it proceeds with f \ to the optimal assignment of weights to the instances of each bag ( projection of $ vectors on the ipi space which is a n ^- simplex ) and computes its new representation set X .
Second , given the fixed instance weights , it trains a new regression model ( J2 ) using X ( projection back to the $ Table 1 : Description of the seven datasets used for aspect , sentiment and emotion rating prediction .
space ).
This procedure repeats until convergence , i . e .
when there is no more decrease on the training error , or until a maximum number of iterations has been reached .
The regression model fs is trained on the weights learned from the previous steps .
5 . 4 Complexity Analysis The overall time complexity T of Algorithm 1 in terms of the input variables , noted h = { m , n , d }, with m being the number of bags , n the average size of the bags , and d the dimensionality of the feature space ( here , the size of word vectors ), is derived as follows : where Tap and Tj3 are respectively the time complexity of the AP procedure and of training the fs model .
9 shows that when n < C m , the model complexity is linear with the input bags m and always quadratic with the number of features d . Previous works on relevance assignment for MIR have prohibitive complexity for high - dimensional feature spaces or numerous bags and hence they are not most appropriate for text regression tasks .
Wagstaff and Lane ( 2007 ) have cubic time complexity with the average bag size n and number of features d ; Zhou et al .
( 2009 ) use kernels , thus their complexity is quadratic with the number of bags m ; and Wang et al .
( 2011 ) have cubic time wrt .
d . Our formulation is thus competitive in terms of complexity .
6 Data , Protocol and Metrics 6 . 1 Aspect Rating Datasets We use seven datasets summarized in Table 1 .
Five publicly available datasets were built for aspect prediction by McAuley et al .
( 2012 ) - Beer - Advocate , Ratebeer ( ES ), RateBeer ( FR ), Audio - books and Toys & Games - and have aspect ratings assigned by their creators on the respective websites .
On the set of comments on TED talks from Pappas and Popescu - Belis ( 2013 ), we aim to predict two things : talk - level emotion dimensions assigned by viewers through voting , and comment polarity scores assigned by crowdsourc - ing .
The distributions of aspect ratings per dataset are shown in Figure 3 .
Five datasets are in English , one in Spanish ( Ratebeer ) and one in French ( RateBeer ), so our results will also demonstrate the language - independence of our method .
From every dataset we kept 1 , 200 texts as bags of sentences , but we also used three full - size datasets , namely Ratebeer ES ( 1 , 259 labeled reviews ), Ratebeer FR ( 17 , 998 ) and Audiobooks ( 10 , 989 ).
The features for each of them are word vectors with binary attributes signaling word presence or absence , in a traditional bag - of - words model ( BOW ).
The word vectors are provided with the first five datasets and we generated them for the latter two , after lowercasing and stopword removal .
Moreover , for TED comments , we computed TF - IDF scores using the same dimensionality as with BOW to experiment with a different feature space .
The target class labels were normalized by the maximum rating in their scale , except for TED talks where the votes were normalized by the maximum number of votes over all the emotion classes for each talk , and two emotions , ' informative ' and ' ok ', were excluded as they are neutral ones .
6 . 2 Evaluation Protocol We compare the proposed model , noted AP - Weights , with four baseline ones - Aggregated , Instance , Prime ( Section 4 ) and Clus - Illj li . ll II I » Figure 3 : Distributions of rating values per aspect rating class for the seven datasets .
tering ( from github . com / garydoranjr / mcr ), which is an instance relevance method proposed by Wagstaff et al .
( 2008 ) for aspect rating prediction .
First , for each aspect class , we optimize all methods on a development set of 25 % of the data ( 300 randomly selected bags ).
Then , we perform 5 - fold cross - validation for every aspect on each entire data set and report the average error scores using the optimal hyper - parameters per method .
In addition , we report for comparison the scores of Average Rating , which always predicts the average rating over the training set .
We report standard error metrics for regression , namely the Mean Absolute Error ( MAE ) and the Mean Squared Error ( MSE ).
The former measures the average magnitude of errors in a set of predictions while the latter measures the average of their squares , which are defined over the test set of bags Bi respectively as MAE = Ç £ ti and MSE = (£ i = i - Vi ))/ k .
The cross - validation scores are obtained by averaging the MAE and MSE scores on each fold .
To find the optimal hyper - parameters for each model , we perform 3 - fold cross - validation on the development set using exhaustive grid - search over a fine - grained range of possible values and select the ones that perform best in terms of MAE .
The hyper - parameters to be optimized for the baselines ( except AverageRating ) are the regularization terms A2 , Ai of their possible regression model /, namely SVR which uses the £ 2 norm and Lasso which uses the £\ norm .
As for AP - Weights , it relies on three regularization terms , namely £ 1 , 62 , 63 of the ^ 2 - norm for f \, / 2 and fs regression models .
Lastly , for the Clustering baseline , we use the / 2 regression model , which relies on £ 2 and the number of clusters k , optimized over { 5 , 50 } with step 5 , for its clustering algorithm , here k - Means .
All the regularization terms are optimized over the same range of possible values , noted a ■ 10b with a G { 1 ,..., 9 } and b G {— 4 ,..., + 4 }, hence 81 values per term .
For the regression models and evaluation protocol , we use the scikit - learn machine learning library ( Pedregosa et al ., 2012 ).
Our code and data are available in the first author ' s website .
7 Experimental Results 7 . 1 Aspect Rating Prediction The results for aspect rating prediction are given in Table 2 .
The proposed APWeights method outperforms Aggregated (£ 2 ) and Aggregated {£{) i . e .
SVR and Lasso along with all other baselines on each case .
The SVR baseline has on average 11 % lower performance than APWeights in terms of MSE and about 6 % in terms of MAE .
Similarly , the Lasso baseline has on average 13 % lower MSE and 8 % MAE than APWeights .
As shown in Figure 4 , APWeights also outperforms them for each aspect in the five review datasets .
The Instance method with £\ performed well on BeerAdvocate and Toys & Games ( for MSE ), and with £ 2 performed well on Ratebeer ( ES ), RateBeer ( FR ) and Toys & Games ( for MAE ).
Therefore , the instance - as - example assumption is quite appropriate for this task , however both options score below APWeights - by about 5 % MAE , and 8 %/ 9 % MSE , respectively .
The Prime method with £\ performed well only on the BeerAdvocate dataset and Prime with £ 2 only on the Toys & Games dataset , always with lower scores than APWeights , namely about 9 % MAE for both and 15 %/ 18 % MSE respectively .
This suggests that the primary - instance Table 2 : Performance of aspect rating prediction ( the lower the better ) in terms of MAE and MSE ( x 100 ) with 5 - fold cross - validation .
All scores are averaged over all aspects in each dataset .
The scores of the best method are in bold and the second best ones are underlined .
Significant improvements ( paired t - test , p < 0 . 05 ) are in italics .
Fig .
4 shows MSE scores per aspect for three methods on five datasets .
assumption is not the most appropriate for this task .
Lastly , even though Clustering is an instance relevance method , it has similar scores to Prime , presumably because the relevances are assigned according to the computed clusters and they are not directly influenced by the task ' s objective .
To compare with the state - of - the - art results obtained by McAuley et al .
( 2012 ), we experimented with three of their full - size datasets .
Splitting each dataset in half for training vs . testing , and using the optimal settings from our experiments above , we measured the average MSE over all aspects .
APWeights improved over Lasso by 10 %, 26 % and 17 % MSE respectively on each dataset - the absolute MSE scores are .
038 for Lasso vs . .
034 for APWeights on Ratebeer SP ; .
017 on Ratebeer FR ; .
Similarly , when compared to the best SVM baseline provided by the McAuley et al ., our method improved by 32 %, 43 % and 35 % respectively on each dataset , though it did not use their rating model .
Moreover , the best model proposed by McAuley et al ., which uses a joint rating model and an aspect - specific text segmenter trained on hand - labeled data , reaches MSE scores of .
03 , which is comparable to our model that does not use these features (.
052 ), though it could benefit from them in the future .
Lastly , as mentioned by the same authors , predictors which use segmented text , for example with topic models as in ( Lu et al ., 2011 ), do not neces - sarly outperform SVR baselines ; instead they have marginal or even no improvements , therefore , we did not further experiment with them .
Interes - Table 3 : MAE and MSE ( x 100 ) on sentiment and emotion prediction with 5 - fold c .- v . Scores on TED talks are averaged over the 12 emotions .
The scores of the best method are in bold and the second best ones are underlined .
Significant improvements ( paired t - test , p < 0 . 05 ) are in italics .
tignly , multiple - instance learning algorithms under several assumptions go beyond SVR baselines with BOW and even more sophisticated features such as TF - IDF ( see below ).
7 . 2 Sentiment and Emotion Prediction Our method is also competitive for sentiment prediction over comments on TED talks , as well as for talk - level emotion prediction with 12 dimensions from subsets of 10 comments on each talk ( see Table 3 ).
APWeights outperforms SVR and Lasso , as well as all other methods for each task .
For sentiment prediction , SVR is outperformed by 11 % MSE and Lasso by 5 %.
For emotion pre - b ) RateBeer ( ES ) c ) RateBeer ( FR ) 1 . d ) AudioBooks Figure 4 : MSE scores of SVR , Lasso and APWeights for each aspect over the five review datasets .
diction ( averaged over all 12 aspects ), differences are smaller , at 1 . 6 % and 2 . 9 % respectively .
These smaller differences could be explained by the fact that among the 10 most recent comments for each talk , many are not related to the emotion that the system tries to predict .
As mentioned earlier , the proposed model does not make any assumption about the feature space .
Thus , we examined whether the improvements it brings remain present even with a different feature space , for instance based on TF - IDF instead of BOW with counts .
For sentiment prediction on TED comments , we found that by changing the feature space to TF - IDF , strong baselines such as Aggregated (£\) and ( li ), i . e .
SVR and Lasso , improve their performance ( 16 . 25 and 16 . 59 MAE ; 4 . 16 and 3 . 97 MSE respectively ).
However , APWeights still outperforms them on both MAE and MSE scores ( 15 . 35 and 3 . 63 ), improving over SVR by 5 . 5 % on MAE and 12 . 5 % on MSE , and over Lasso by 7 . 4 % on MAE and 8 . 5 % on MSE .
These promising results suggest that improvements with APWeights could be observed also on more sophisticated feature spaces .
7 . 3 Interpreting the Relevance Weights Apart from predicting ratings , the MIR scores assigned by our model reflect the contribution of each sentence to these predictions .
To illustrate the explanatory power of our model ( until a dataset for quantitative analysis becomes available ), we provide examples of predictions on test data taken from the cross - validation folds above .
Table 5 displays the most relevant com - Table 4 : Predicted sentiment for TED comments : Hi is the actual sentiment , yi the predicted one , and ■ ipi the estimated relevance of each sentence .
ment for two correctly predicted emotions on two TED talks , based on the ipi relevance scores , along with the ipi scores of the other comments , for two emotion classes : ' beautiful ' and ' courageous '.
These comments appear to reflect correctly the fact that the respective emotion is the majority one in each of the comments .
As noted earlier , this task is quite challenging since we use only the ten most recent comments for each talk .
Table 4 displays four TED comments selected Table 5 : Two examples of top comments ( according to weights ipi ) for correctly predicted emotions in four TED talks ( score 1 . 0 ) and the distribution of weights over the 10 most recent comments in each talk .
r concepts " company oa ^ Toura9ing ' nsP ' rabional ey e5eacuceorganizabion movina learned conversation - !
"., v *", y poor inspirationdew courage negabive oears a ) inspiring images *-™" culture hGarbplanerjDei lxP ^ M ?
- nvoice , ovely till ICIOplrlosophytoiia I IUwIw warming alone _ pM , ebeaubySong 9 ^ 3 b ) beautiful enberbaining ™ bhabSremfs immluarihl „, presented .
W & hUlTIOr creabiviby UIUÜU160 abtibude impressive äs courage , peace admire .
Vj < new helped crazy iar loving SpiriO professional wafr & esnpired governmenb *™ serioSiu bless " i '^ a d - fn9er2 " L ? eaChwomanwornenfamily , s <* 9 c ) funny d ) courageous Figure 5 : Top words based on $ for predicting four emotions from comments on TED talks .
from the test set of a given fold , for the comment - level sentiment prediction task .
The table also shows the ipi relevance scores assigned to each of the composing sentences , the predicted polarity scores fji and the actual ones in .
We observe that the sentences that convey the most sentiment are assigned higher scores than sentences with less sentiment , always with respect to the global polarity level .
These examples suggest that , given that APWeights has more degrees of freedom for interpretation , it is able to assign relevance to parts of a text ( here , sentences ) and even to words , while other models can only consider words .
Hence , the assigned weights might be useful for other NLP tasks mentioned below .
8 Conclusion and Future Work This paper introduced a novel MIR model for aspect rating prediction from text , which learns instance relevance together with target labels .
To the best of our knowledge , this has not been considered before .
Compared to previous work on MIR , the proposed model is competitive and more efficient in terms of complexity .
Moreover , it is not only able to assign instance relevances on labeled bags , but also to predict them on unseen bags .
Compared to previous work on aspect rating prediction , our model performs significantly better than BOW regression baselines ( SVR , Lasso ) without using additional knowledge or features .
The improvements persist even when the sophistication of the features increases , suggesting that our contribution may be orthogonal to feature engineering or learning .
Lastly , the qualitative evaluation on test examples demonstrates that the parameters learned by the model are not only useful for prediction , but they are also interprétable .
In the future , we intend to test our model on sentiment classification at the sentence - level , based only on document - level supervision ( Täckström and McDonald , 2011 ).
Moreover , we will experiment with other model settings , such as regularization norms other than £ 2 and feature spaces other than BOW or TF - IDF .
In the longer term , we plan to investigate new methods to estimate instance weights at prediction time , and to evaluate the impact of assigned weights on sentence ranking , segmentation or summarization .
Acknowledgments The work described in this article was supported by the European Union through the inEvent project FP7 - ICT n . 287872 ( see http : / / www .
inevent - pro ject .
