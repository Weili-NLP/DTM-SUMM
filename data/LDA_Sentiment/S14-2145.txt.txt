UWB : Machine Learning Approach to Aspect - Based Sentiment Analysis This paper describes our system participating in the aspect - based sentiment analysis task of Semeval 2014 .
The goal was to identify the aspects of given target entities and the sentiment expressed towards each aspect .
We firstly introduce a system based on supervised machine learning , which is strictly constrained and uses the training data as the only source of information .
This system is then extended by unsupervised methods for latent semantics discovery ( LDA and semantic spaces ) as well as the approach based on sentiment vocabularies .
The evaluation was done on two domains , restaurants and laptops .
We show that our approach leads to very promising results .
1 Introduction The majority of current sentiment analysis approaches tries to detect the overall polarity of a sentence ( or a document ) regardless of the target entities ( e . g .
restaurants ) and their aspects ( e . g .
food , price ).
By contrast , the ABSA ( aspect based sentiment analysis ) task is concerned with identifying the aspects of given target entities and estimating the sentiment polarity for each mentioned aspect .
The aspect scenario can be decomposed into two tasks : aspect extraction and aspect sentiment classification ( Liu , 2012 ).
The task of aspect extraction is to recognize aspects of the entity and more generally can be seen as an information extraction task .
The basic approach is finding frequent nouns and noun This work is licensed under a Creative Commons Attribution 4 . 0 International Licence .
Page numbers and proceedings footer are added by the organisers .
Licence details : http :// creativecommons . org / licenses / by / 4 . 0 / phrases ( Liu et al ., 2005 ; Blair - Goldensohn et al ., 2008 ; Moghaddam and Ester , 2010 ; Long et al ., 2010 ).
Aspect extraction can be also seen as a special case of the general information extraction problem .
The most dominant methods are based on sequential learning ( e . g .
HMM - Hidden Markov Models ( Rabiner , 2010 ) or CRF - Conditional Random Fields ( Lafferty et al ., 2001 )).
Another group of methods use topic models ( Mei et al ., 2007 ; Titov and McDonald , 2008 ; Blei et al ., 2003 ).
Aspect sentiment classification determines whether the opinions on different aspects are positive , negative , or neutral .
While lexicon - based approaches use a list of aspect - related sentiment phrases as the core resource ( Ding et al ., 2008 ; Hu and Liu , 2004 ), the key issue for learning methods is to determine the scope of each sentiment expression , i . e ., whether it covers the aspect in the sentence ( Jiang et al ., 2011 ; Boiy and Moens , 2009 ).
The most of the research in aspect - level sentiment analysis has been done in English , however , there were some attempts to tackle the aspect - level task in other languages ( e . g .
in Czech ( Steinberger et al ., 2014 )).
The rest of the article is organized as follows .
In Section 2 , we summarize the ABSA shared task ( Pontiki et al ., 2014 ).
Then , we give a description of our participating system ( Section 3 ).
In Section 4 , we discuss our results in the task .
We participated with both the constrained and the unconstrained variants of the system .
2 The ABSA task Datasets consisting of customer reviews with human - authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect were provided .
The experiments were run in two domains : restaurant and laptop reviews .
Each team could submit two versions of systems - constrained and unconstrained .
The constrained system uses only the training data and other resources ( such as lexicons ) for training .
The unconstrained system can use additional data .
We use another definition of these types , which is not against the rules .
Our constrained systems are based purely on ABSA training data , without any external knowledge such as dictionaries or rules .
Our unconstrained systems use additional dictionaries , rule - based extensions and unlabeled data .
From our point of view , hand - crafted dictionaries and rules are external knowledge and thus it is the same as adding external data .
The task consists of the four subtasks .
2 . 1 Subtask 1 : Aspect term extraction Given a set of sentences with pre - identified entities ( restaurants or laptops ), the task is to identify the aspect terms present in the sentence and return a list containing all the distinct aspect terms .
/ liked the service and the staff , but not the food .
—> { service , staff , food } 2 . 2 Subtask 2 : Aspect term polarity For a given set of aspect terms within a sentence , the task is to determine the polarity of each aspect term : positive , negative , neutral or conflict ( i . e ., both positive and negative ).
/ hated their fajitas , but their salads were great .
—> { fajitas : negative , salads : positive } 2 . 3 Subtask 3 : Aspect category detection Given a predefined set of aspect categories , the task is to identify the aspect categories discussed in a given sentence .
Aspect categories are typically coarser than the aspect terms of Subtask 1 , and they do not necessarily occur as terms in the given sentence .
For example , the following categories were defined for the restaurants ' domain : food , service , price , ambience and anecdotes / miscellaneous .
The restaurant was expensive , but the menu was great .
—> { price , food } 2 . 4 Subtask 4 : Aspect category polarity Given a set of pre - identified aspect categories , the task is to determine the polarity ( positive , negative , neutral or conflict ) of each aspect category .
The restaurant was expensive , but the menu was great .
—> { price : negative , food : positive } 3 System description We use machine learning approach to all subtasks .
For aspect term extraction we use CRF .
For the other three tasks we use the Maximum Entropy classifier .
We use the Brainy ( Konkol , 2014 ) implementation of these algorithms .
During the data preprocessing , we use simple word tokenizer based on regular expressions .
All tokens are lowercased for tasks 2 and 4 .
We will firstly describe all the features used in this paper because the tasks share some of them .
These features are then referenced in the descriptions of individual subtasks .
Words ( W ) - Word occurrence on a given position in the context window .
Bag of Words ( BoW ) - Occurrence of a word in a sentence ( or context window ).
Bigrams ( B ) - Bigram occurrence on a given position in the context window .
Bag of Bigrams ( BoB ) - Occurrence of a bigram in a sentence ( or context window ).
Tf - idf - Term frequency - inverse document frequency for all tokens in the sentence .
Learned Dictionary ( LD ) - Dictionary of terms based on training data .
Suffixes ( S ) - Suffix of a word ( 2 - 4 characters ).
Sentiment Dictionary ( SD ) - Dictionary created using semi - automatic triangulation method ( Steinberger et al ., 2012 ).
The score is normalized .
Senti Wordnet ( SW ) - See ( Baccianella et al ., 2010 ).
LDA - See Section 3 . 1 .
Word Clusters ( WC ) - See Section 3 . 2 .
Cluster occurrence on a given position in the context window .
Bag of Clusters ( BoC ) - Same as word clusters , but without information about position .
We use two features that are not in common use in similar tasks - Latent Dirichlet Allocation and word clusters based on semantic spaces .
Both these features use large amount of unlabeled data to discover latent semantics .
We downloaded the restaurant reviews from http : // opentable .
com .
This corpus consists of 409 , 665 reviews ( documents ) with about 27 million words .
The opentable corpus is used as the training data for these features .
Unfortunately , we did not find any large corpus for laptop domain , thus presented unsupervised features are used in restaurant domain only .
We devote the following two subsections to describe these features .
Then we introduce our approach to the individual tasks .
3 . 1 Latent Dirichlet Allocation The Latent Dirichlet Allocation ( LDA ) ( Blei et al ., 2003 ) is a topic model that is assumed to provide useful information for particular subtasks .
We use LDA implementation from the MALLET ( McCal - lum , 2002 ) software package .
For each experiment we always train the 400 topics LDA ( no significant difference was observed between different numbers of topics ) with 1 , 000 iterations of Gibbs sampling .
The hyperparameters of Dirichlet distributions were initially set to a = 50 / K , where K is the number of topics and ß = 0 . 1 .
This setting is recommended by ( Griffiths and Steyvers , 2004 ) .
The topic probabilities are directly used as new features to the classifier .
3 . 2 Word clusters We use same approach as presented in ( Brychcm and Konopfk , 2014 ), where word clusters derived from semantic spaces improved language modeling .
As recommended by these authors , we use COALS ( Correlated Occurrence Analogue to Lexical Semantics ) ( Rohde et al ., 2004 ) and HAL ( Hyperspace Analogue to Language ) ( Lund and Burgess , 1996 ) for representing the word meaning and the Repeated Bisection algorithm for clustering .
Similar approach has been already used for sentiment analysis in ( Habernal and Brychcm , 2013 ) and ( Brychcm and Habernal , 2013 ).
The parameters of semantic spaces are set as follows .
For both semantic spaces we use a four - word context window ( in both directions ).
HAL uses a matrix consisting of 50 , 000 columns , which keeps the largest amount of information .
COALS uses a matrix with only 14 , 000 columns ( as recommended by the authors of the algorithm ).
The SVD reduction was not used in our experiments .
Implementation of the HAL , COALS algorithms is available in an open source package S - Space ( Jürgens and Stevens , 2010 ).
For clustering , we use the implementation from the CLUTO software package ( Karypis , 2003 ).
As a measure of the similarity between two words , we use the cosine similarity of word vectors .
For both semantic spaces the word vectors are clustered into four different depths : 100 , 500 , 1 , 000 , and 5 , 000 clusters ( i . e .
eight different cluster sets ).
The occurrences of particular clusters represent additional features to the classifiers .
3 . 3 Aspect term extraction Our approach for aspect term extraction is based on Conditional Random Fields ( CRF ).
The choice was based on similarity with the named entity recognition task , where CRF are regarded as the current state of the art ( Konkol and Konopfk , 2013 ).
We use the BIO model for representing aspect terms ( Ramshaw and Marcus , 1999 ).
The constrained feature set consists of : W , BoW , B , LD , S . It is extended by WC for the unconstrained case .
3 . 4 Aspect term polarity During the detection of the aspect term polarities , the words affecting the sentiment of the aspect term are assumed to be close in most of cases .
Thus we use a context window of 10 words in both directions around the target aspect term .
We assume the further the word or bigram is from the target aspect term , the lower impact it has on the polarity label .
To model this assumption we use a weight for each word and bigram feature taken from the Gaussian distribution according to the distance from the aspect term .
The mean is set to 0 and the variance is optimized on training data .
As a feature set for the constrained approach we use only BoW , BoB and for the unconstrained approach we use BoC , SD , SW above that .
3 . 5 Aspect category detection Aspect category detection is based on a set of binary Maximum Entropy classifiers , one for each class .
The final decision is simply assembled from decisions of individual classifiers .
For this task we use BoW , Tf - Idf for the constrained approach and add LDA , BoC for unconstrained approach .
Table 1 : Comparison of our constrained ( C ) and unconstrained ( U ) system with Semeval baseline , best and average results .
P , R , and F \ denote the precision , recall and F - measure , respectively , used for measuring aspect term and category detection .
ACC denotes the accuracy , used for measuring aspect term and category sentiment polarity detection .
3 . 6 Aspect category polarity For this task we always take the whole sentence into account .
We cannot take a limited window as we do not know where exactly the category is mentioned in the sentence .
Moreover , it can be at several positions .
To distinguish between different categories we again use standalone Maximum Entropy classifier for each category .
The constrained feature set consists of : BoW , BoB , Tf - Idf .
It is extended by BoC , LDA , SD , SW for the unconstrained case .
4 Results The ABSA task was a competition between research teams from around the world .
There were 21 to 32 submitted systems for individual tasks .
We have submitted both constrained ( no external knowledge , dictionaries or rules ) and unconstrained systems for all tasks , except unconstrained system for aspect term extraction in the laptops domain .
Table 1 shows results of our systems ( UWB ) and compares them with the best and average systems as well as with the Semeval baseline .
The average system is not any particular system .
It is represented by average rank and metrics ( metrics are averaged separately ).
Our systems performed quite well .
In all tasks , we outperform the Semeval baseline system .
Moreover , we are always above average ( F - measure and accuracy ) in all tasks .
We were three times in the fourth place and our unconstrained systems were always in top ten .
Table 2 presents the 10 - fold cross - validation results on restaurant training data .
We can clearly see , that any of our extension ( LDA , clusters , sentiment vocabularies ) brings at least some improvement .
5 Conclusion This paper covers our participation in the ABSA task of Semeval 2014 .
The ABSA task consists of 4 subtasks .
For each subtask we propose both constrained ( no external knowledge ) and unconstrained approach .
The constrained versions of our system are based purely on machine learning techniques .
The unconstrained versions extend the constrained feature set by LDA , semantic spaces and sentiment dictionaries .
The proposed approaches achieved very good results .
The constrained versions were always above average , often by a large margin .
The unconstrained versions were ranked among the best systems .
( c ) Aspect category extraction ( d ) Aspect category polarity Table 2 : 10 fold cross - validation results on the restaurants training data for individual features .
P , R , and Fi denote the precision , recall and F - measure , respectively , used for measuring aspect term and category detection .
ACC denotes the accuracy , used for measuring aspect term and category sentiment polarity detection .
Acknowledgements This work was supported by grant no .
SGS - 2013 - 029 Advanced computing and information systems , by the European Regional Development Fund ( ERDF ) and by project " NTIS - New Technologies for Information Society ", European Centre of Excellence , CZ .
1 . 05 / 1 . 1 . 00 / 02 . 0090 , and by project MediaGist , EU ' s FP7 People Programme ( Marie Curie Actions ), no .
