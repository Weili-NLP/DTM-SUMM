Clustering Aspect - related Phrases by Leveraging Sentiment Distribution Consistency Clustering aspect - related phrases in terms of product ' s property is a precursor process to aspect - level sentiment analysis which is a central task in sentiment analysis .
Most of existing methods for addressing this problem are context - based models which assume that domain synonymous phrases share similar co - occurrence contexts .
In this paper , we explore a novel idea , sentiment distribution consistency , which states that different phrases ( e . g .
" price ", " money ", " worth ", and " cost ") of the same aspect tend to have consistent sentiment distribution .
Through formalizing sentiment distribution consistency as soft constraint , we propose a novel unsupervised model in the framework of Posterior Regularization ( PR ) to cluster aspect - related phrases .
Experiments demonstrate that our approach outperforms baselines remarkably .
1 Introduction Aspect - level sentiment analysis has become a central task in sentiment analysis because it can aggregate various opinions according to a product ' s properties , and provide much detailed , complete , and in - depth summaries of a large number of reviews .
Aspect finding and clustering , a precursor process of aspect - level sentiment analysis , has attracted more and more attentions ( Mukherjee and Liu , 2012 ; Chen et al ., 2013 ; Zhai et al ., 2011a ; Zhai et al ., 2010 ).
Aspect finding and clustering has never been a trivial task .
People often use different words or phrases to refer to the same product property ( also called product aspect or feature in the literature ).
Some terms are lexically dissimilar while seman - tically close , which makes the task more challenging .
For example , " price ", " money " , " worth " and " cost " all refer to the aspect " price " in reviews .
In order to present aspect - specific summaries of opinions , we first of all , have to cluster different aspect - related phrases .
It is expensive and time - consuming to manually group hundreds of aspect - related phrases .
In this paper , we assume that the aspect phrases have been extracted in advance and we keep focused on clustering domain synonymous aspect - related phrases .
Existing studies addressing this problem are mainly based on the assumption that different phrases of the same aspect should have similar cooccurrence contexts .
In addition to the traditional assumption , we develop a new angle to address the problem , which is based on sentiment distribution consistency assumption that different phrases of the same aspect should have consistent sentiment distribution , which will be detailed soon later .
Pros : LCD , nice touch screen , longer battery life Cons : Horrible picture quality Review : The touch screen was the selling feature for me .
The LCD touch screen is nice and large .
This camera also has very impressive battery life .
However the picture quality is very grainy .
Figure 1 : A semi - structured Review .
This new angle is inspired by this simple observation ( as illustrated in Fig .
1 ): two phrases within the same cluster are not likely to be simultaneously placed in Pros and Cons of the same review .
A straightforward way to use this information is to formulate cannot - link knowledge in clustering algorithms ( Chen et al ., 2013 ; Zhai et al ., 2011b ).
However , we have a particularly different manner to leverage the knowledge .
Due to the availability of large - scale semi - structured customer reviews ( as exemplified in Fig .
1 ) that are supported by many web sites , we can easily get the estimation of sentiment distribution for each aspect phrase by simply counting how many times a phrase appears in Pros and Cons respectively .
As illustrated in Fig .
2 , we can see that the estimated sentiment distribution of a phrase is close to that of its aspect .
The above observation suggests the sentiment distribution consistency assumption : different phrases of the same aspect tend to have the same sentiment distribution , or to have statistically close distributions .
This assumption is also verified by our data : for most ( above 91 . 3 %) phrase with relatively reliable estimation ( whose occurrence > 50 ), the KL - divergence between the sentiment distribution of a phrase and that of its corresponding aspect is less than 0 . 05 .
Figure 2 : The sentiment distribution of aspect " battery " and its related - phrases on nokia 5130 with a large amount of reviews .
It is worth noting that , the sentiment distribution of a phrase can be estimated accurately only when we obtain a sufficient number of reviews .
When the number of reviews is limited , however , the estimated sentiment distribution for each phrase is unreliable ( as shown in Fig .
A key issue , arisen here , is how to formulate this assumption in a statistically robust manner .
The proposed model should be robust when only a limited number of reviews are available .
Figure 3 : The sentiment distribution of aspect " battery " and its related - phrases on nokia 3110c with a small mumber of reviews .
To deal with this issue , we model sentiment distribution consistency as soft constraint , integrated into a probabilistic model that maximizes the data likelihood .
We design the constraint to work in the following way : when we have sufficient observations , the constraint becomes tighter , which plays a more important role in the learning process ; when we have limited observations , the constraint becomes very loose so that it will have less effect on the model .
In this paper , we propose a novel unsupervised model , Sentiment Distribution Consistency Regularized Multinomial Naive Bayes ( SDC - MNB ).
The context part is modeled by Multinomial Naive Bayes in which aspect is treated as latent variable , and Sentiment distribution consistency is encoded as soft constraint within the framework of Posterior Regularization ( PR ) ( Graca et al ., 2008 ).
The main contributions of this paper are summarized as follows : • We study the problem of clustering phrases by integrating both context information and sentiment distribution of aspect - related phrases .
• We explore a novel concept , sentiment distribution consistency { SDC ), and model it as soft constraint to guide the clustering process .
• Experiments show that our model outperforms the state - of - art approaches for aspect clustering .
The rest of this paper is organized as follows .
We introduce the SDC - MNB model in Section 2 .
We present experiment results in Section 3 .
In Section 4 , we survey related work .
We summarize the work in Section 5 .
2 Sentiment Distribution Consistency Regularized Multinomial Naive Bayes In this section , we firstly introduce our assumption sentiment distribution consistency formally and show how to model the above assumption as soft constraint , which we term SDC - constraint .
Secondly , we show how to combine SDC - constraint with the probabilistic context model .
Finally , we present the details for context and sentiment extraction .
2 . 1 Sentiment Distribution Consistency We define aspect as a set of phrases that refer to the same property of a product and each phrase is termed aspect - related phrase ( or aspect phrase in short ).
For example , the aspect " battery " contains aspect phrases such as " battery ", " battery life ", " power ", and so on .
Let us consider the sentiment distribution on a certain aspect c ^.
In a large review dataset , aspect cii could receive many comments from different reviewers .
For each comment , we assume that people either praise or complain about the aspect .
So each comment on the aspect can be seen as a Bernoulli trial , where the aspect receives positive comments with probability pai .
We introduce a random variable Xai to denote the sentiment on aspect cii , where Xai = 1 means that aspect ai receives positive comments , Xai = 0 means that aspect cii receives negative comments .
Obviously , the sentiment on aspect ai follows the Bernoulli distribution , Pr ( Xai )= paOr in short , Bernoulli { pa Let us see the case for aspect phrase fj , where fj G aspect ai .
Similarly , each comment on an aspect phrase fj can also be seen as a Bernoulli trial .
We introduce a random variable Xfj to denote the sentiment on aspect phrase fj , where Xf .
= 1 means that aspect fj receives positive comments , Xf .
= 0 means that aspect fj receives negative comments .
As just discussed , we assume that each aspect phrase follows the same distribution with the corresponding aspect .
This leads to the following formal description : • Sentiment Distribution Consistency : The sentiment distribution of aspect phrase is the same as that of the corresponding aspect .
Formally , for all aspect phrase fj e aspect Bernov , lli ( pai Sentiment Distribution Consistency Constraint Assuming the sentiment distribution of aspect ai is given in advance , we need to judge whether an aspect phrase fj belongs to the aspect ai with limited observations for fj .
Let ' s consider the example in Fig .
For aspect phrase 3 , we have no definite answer due to the limited number of observations .
For aspect phrase 1 , it seems that the sentiment distribution is consistent with that of the left aspect .
However , we can not say that the phrase belongs to the aspect because the distribution may be the same for two different aspects .
For aspect phrase 2 , we are confident that its sentiment distribution is different from that of the left aspect , given sufficient observations .
positive comment means that an aspect term is observed in Pros of a review .
Figure 4 : Sentiment distribution of an aspect , and observations on aspect phrases .
To be concise , we judge an aspect phrase doesn ' t belong to certain aspect only when we are confident that they follow different sentiment distributions .
Inspired by the intuition , we conduct interval parameter estimation for parameter pfj ( sentiment distribution for phrase fj ) with limited observations , and thus get a confidence interval for pfyIf pai ( sentiment distribution for aspect a {) is not in the confidence interval of pfj , we then are confident that they follow different distributions .
In other words , if aspect phrase fj e aspect ai , we are confident that pai is in the confidence interval More formally , we use Uik to denote the sentiment distribution parameter of aspect ai on product pu , and assume that Uik is given in advance .
Table 1 : Notations We want to know whether the sentiment distribution on aspect phrase fj is the same as that of aspect cii on product Pk given a limited number of observations ( samples ).
It ' s straightforward to calculate the confidence interval for parameter Sjk in the Bernoulli distribution function .
Let the sample mean of rijk samples be Sjk , and the sample standard deviation be âjk - Since the sample size is small here , we use the Student - t distribution to calculate the confidence interval .
According to our assumption , we are confident that Uik is in the confidence interval if fj G c ^.
where we look for t - table to find C corresponding to a certain confidence level ( such as 95 %) with the freedom of rijk — 1 .
For simplicity , we represent the above confidence interval by [ êjk — cljk , § jk + djk ], where djk = C -^§=.
We introduce an indicator variable Zij to represent whether the aspect phrase fj belongs to aspect cii , as follows : This leads to our SDC - constraint function .
SDC - constraint are flexible for modeling Sentiment Distribution Consistency .
The more observations we have , the smaller djk is .
For frequent aspect phrase , the constraint can be very informative because it can filter unrelated aspects for aspect phrase fj .
The less observations we have , the larger djk is .
For rare aspect phrases , the constraint can be very loose , and will not have much effect on the clustering process for aspect phrase fj .
In this way , the model can work very robustly .
SDC - constraints are data - driven constraints .
Usually we have many reviews about hundreds of products in our dataset .
For each aspect phrase , there are \ A \ * \ P \ constraints ( the number of aspects times the number of product ).
With thousands of constraints about which aspect it is not likely to belong to , the model learns to which aspect a phrase fj should be assigned .
Although most constraints may be loose because of the limited observations , SDC - constraint can still play an important role in the learning process .
2 . 3 Sentiment Distribution Consistency Regularized Multinomial Naive Bayes ( SDC - MNB ) In this section , we present our probabilistic model which employs both context information and sentiment distribution .
First of all , we extract a context document d for each aspect phrase , which will be described in Section 2 . 5 .
In other word , a phrase is represented by its context document .
Assuming that the documents in D are independent and identically distributed , the probability of generating D is then given by : where ijj is a latent variable indicating the aspect label for aspect phrase fj , and 6 is the model parameter .
In our problem , we are actually more interested in the posterior distribution over aspect , i . e ., pe { yj \ dj ).
Once the learned parameter 9 is obtained , we can get our clustering result from Pe { yj \ dj ), by assigning aspect ai with the largest posterior to phrase fj .
We can also enforce SDC - constraint in expectation ( on posterior pe ).
We use q { Y ) to denote the valid posterior distribution that satisfy our SDC - constraint , and Q to denote the valid posterior distribution space , as follows : Since posterior plays such an important role in joining the context model and SDC - constraint , we formulate our problem in the framework of Posterior Regularization ( PR ).
PR is an efficient framework to inject constraints on the posteriors of latent variables .
Instead of restricting pe directly , which might not be feasible , PR penalizes the distance of pe to the constraint set Q .
The posterior - regularized objective is termed as follows : By trading off the data likelihood of the observed context documents ( as defined in the first term ), and the KL divergence of the posteriors to the valid posterior subspace defined by SDC - constraint ( as defined in the second term ), the objective encourages models with both desired posterior distribution and data likelihood .
In essence , the model attempts to maximize data likelihood of context subject ( softly ) to SDC - constraint .
2 . 3 . 1 Multinomial Naive Bayes In spirit to ( Zhai et al ., 2011a ), we use Multinomial Naive Bayes ( MNB ) to model the context document .
Let Wdhk denotes the kth word in document dj , where each word is from the vocabulary V = { w \, W2 , W \! y \!}.
For each aspect phrase fj , the probability of its latent aspect being ai and generating context document di is where p { ai ) and p { wdj , k \ ai ) are parameters of this model .
Each word Wdjtk is conditionally independent of all other words given the aspect c ^.
Although MNB has been used in existing work for aspect clustering , all of the studies used it in a semi - supervised manner , with labeled data or pseudo - labeled data .
In contrast , MNB proposed here is used in an unsupervised manner for aspect - related phrases clustering .
2 . 3 . 2 SDC - constraint As mentioned above , the constraint posterior set Q is defined by We can see that Q denotes a set of linear constraints on the projected posterior distribution q .
Note that we do not directly observe - u ^, the sentiment distribution of aspect ai on product pk - For aspect phrase fj that belongs to aspect ai , we estimate Uik by counting all sentiment samples .
We use the posterior pe { ai \ dj ) to approximately represent how likely phrase fj belongs to aspect c ^.
EUi « jfcPe ( ai \! rfj ) j = i where p $( ai \ dj ) is short for po { vj = ai \ dj ), the probability that aspect phrase fj belongs to ai given the context document dj .
We estimate - u ^ in this way because observations for aspect are relatively sufficient for a reliable estimation since observations for an aspect are aggregated from those for all phrases belonging to that aspect .
2 . 4 The Optimization Algorithm The optimization algorithm for the objective ( see Eq .
7 ) is an EM - like two - stage iterative algorithm .
In E - step , we first calculate the posterior distribution pe { ai I dj ), then project it onto the valid posterior distribution space Q .
Given the parameters 9 , the posterior distribution can be calculated by Eq .
11 . p ( aor7 . fciiKwri3 -, fciaQ We use the above posterior distribution to update the sentiment parameter for each aspect by Eq .
The projected posterior distribution q is calculated by For each instance , there are \ A \ * \ P \ constraints .
However , we can prune a large number of useless constraints derived from limited observations .
All constraints with djk > 1 can be pruned , due to the fact that the parameter - u ^, Sjk is within [ 0 , 1 ], and the difference can not be larger than 1 .
This optimization problem in Eq .
12 is easily solved via the dual form by the projected gradient algorithm ( Boyd and Vandenberghe , 2004 ): where e controls the slack size for constraint .
After solving the above optimization problem and obtaining the optimal A , we can calculate the projected posterior distribution q by where Z is the normalization factor .
Note that sentiment distribution consistency is actually modeled as instance - level constraint here , which makes it very efficient to solve .
In M - step , the projected posteriors q ( Y ) are then used to compute sufficient statistics and update the models parameters 9 .
Given the projected posteriors q ( Y ), the parameters can be updated by Eq .
+ EjJl liVj = a i p { wt \ cii where Ntj is the number of times that the word wt occurs in document dj .
The parameters are initialized randomly , and we repeat E - step and M - step until convergence .
2 . 5 Data Extraction 2 . 5 . 1 Context Extraction In order to extract the context document d for each aspect phrase , we follow the approach in Zhai et al .
For each aspect phrase , we generate its context document by aggregating the surrounding texts of the phrase in all reviews .
The preceding and following t words of a phrase are taken as the context where we set t = 3 in this paper .
Stop - words and other aspect phrases are removed .
For example , the following review contains two aspect phrases , " screen " and " picture ", The LCD screen gives clear picture .
For " screen ", the surrounding texts are { the , LCD , gives , clear , picture }.
We remove stop - words " the ", and the aspect term " picture ", and the resultant context of " screen " in this review is context (. ycreen ) ={ LCD , screen , gives , clear }.
Similarly , the context of " picture " in this review is context ( picture ) ={ gives , clear }.
By aggregating the contexts of all the reviews that contain aspect phrase fj , we obtain the corresponding context document dj .
2 . 5 . 2 Sentiment Extraction Since we use semi - structured reviews , we obtain the estimated sentiment distribution by simply counting how many times each aspect phrase appears in Pros and Cons reviews for each product respectively .
So for each aspect phrase fj , let rijk denotes the times that fj appears in Pros of all reviews for product pk , and let n ~ k denotes the times that fj appears in Cons of all reviews for product pk .
So the total number of occurrence of a phrase is rijk = n ^ k + n ~ k .
We have samples like ( 1 , 1 , 1 , 0 , 0 ) where 1 means a phrase occurs in Pros of a review , and 0 in Cons .
Given a sequence of such observations , the sample mean is easily computed as Sjk = + 3_ .
And the sample standard deviation is âjk = 3 Experiments 3 . 1 Data Preparation The details of our review corpus are given in Table 2 .
This corpus contains semi - structured customer reviews from four domains : Camera , Cellphone , Laptop , and MP3 .
These reviews were crawled from the following web sites : www . amazon . cn , www . 360buy . com , www . newegg . com . cn , and www . zol . com .
The aspect label of each aspect phrases is annotated by human curators .
3 . 2 Evaluation Measures We adapt three measures Purity , Entropy , and Rand Index for performance evaluation .
These measures have been commonly used to evaluate clustering algorithms .
Given a data set DS , suppose its gold - standard partition is G = { gi ,§ j , gk }, where k is the number of clusters .
A clustering algorithm partitions DS into k disjoint subsets , say DS1 , DS2 ,..., DSk .
Entropy : For each resulting cluster , we can measure its entropy using Eq .
17 , where P %{ gj ) is the proportion of data points of class gj in DSi .
The entropy of the entire clustering result is calculated byEq .
Purity : Purity measures the extent that a cluster contains only data from one gold - standard partition .
The cluster purity is computed with Eq .
The total purity of the whole clustering result ( all clusters ) is computed with Eq .
20 . purity ( DSi ) = max Pi ( gj ) j RI : The Rand Index ( RI ) penalizes both false positive and false negative decisions during clustering .
Let TP ( True Positive ) denotes the number of pairs of elements that are in the same set in DS and in the same set in G . TN ( True Negative ) denotes number of pairs of elements that are in different sets in DS and in different sets in G . FP ( False Table 2 : Statistics of the review corpus .
# denotes the size .
Table 3 : Comparison to unsupervised baselines .
( P is short for purity , E for entropy , and RI for random index .)
Positive ) denotes number of pairs of elements in S that are in the same set in DS and in different sets in G . FN ( False Negative ) denotes number of pairs of elements that are in different sets in DS and in the same set in G . The Rand Index ( RI ) is computed with Eq .
3 . 3 Evaluation Results 3 . 3 . 1 Comparison to unsupervised baselines We compared our approach with several existing unsupervised methods .
Some of the methods augmented unsupervised models by incorporating lexical similarity and other domain knowledge .
All of them are context - based models .
We list these models as follows .
• Kmeans : Kmeans is the most popular clustering algorithm .
Here we use the context distributional similarity ( cosine similarity ) as the similarity measure .
• L - EM : This is a state - of - the - art unsupervised method for clustering aspect phrases ( Zhai et al ., 2011a ).
L - EM employed lexical knowledge to provide a better initialization for EM .
• LDA : LDA is a popular topic model ( Blei et al ., 2003 ).
Given a set of documents , it outputs groups of terms of different topics .
In our case , each aspect phrase is processed as a term .
Each sentence in a review is considered as a document .
Each aspect is considered as a topic .
In LDA , a term may belong to more than one topic / group , but we take the topic / group with the maximum probability .
In our method , we collect context document for each aspect phrase .
This process is conducted for L - EM and K - means .
But for LDA and Constraint - LDA , we take each sentence of reviews as a document .
This setting for the LDA baselines is adapted from previous work .
Each aspect phrase is pre - processed as a single word ( e . g ., " battery life " is treated as battery - life ).
Other words are normally used in LDA .
• Constraint - LDA : Constraint - LDA ( Zhai et al ., 2011b ) is a state - of - the - art LDA - based method that incorporates must - link and cannot - link constraints for this task .
We set the damping factor A = 0 . 3 and relaxation factor r ] = 0 . 9 , as suggested in the original reference .
For all methods that depend on the random initiation , we use the average results of 10 runs as the final result .
For all LDA - based models , we choose a = 50 / T , ß = 0 . 1 , and run 1000 iterations .
Experiment results are shown in Table 3 .
We can see that our approach almost outperforms all unsupervised baseline methods by a large margin on all domains .
In addition , we have the following observations : • LDA and Kmeans perform poorly due to the fact that the two methods do not use any prior knowledge .
It is also shown that only using the context distributional information is not sufficient for clustering aspect phrases .
• Constraint - LDA and L - EM that utilize prior knowledge perform better .
We can see that Constraint - LDA outperforms LDA in terms of RI ( Rand Index ) on all domains .
L - EM achieves the best results against the baselines .
This demonstrates the effectiveness to incorporate prior knowledge .
• SDC - MNB produces the optimal results among all models for clustering .
Methods that use must - links and cannot - links may suffer from noisy links .
For L - EM , we find that it is sensitive to noisy must - links .
As L - EM assumes that must - link is transitive , several noisy must - links may totally mislabel the softly annotated data .
For Constraint - LDA , it is more robust than L - EM , because it doesn ' t assume the transitivity of must - link .
However , it only promotes the RI ( Rand Index ) consistently by leveraging pair - wise prior knowledge , but sometimes it hurts the performance with respect to purity or entropy .
Our method is consistently better on almost all domains , which shows the advantages of the proposed model .
• SDC - MNB is remarkably better than baselines , particularly for the cellphone domain .
We argue that this is because we have the largest number of reviews for each product in the cellphone domain .
The larger dataset gives us more observations on each phrase , so that we obtain more reliable estimation of model parameters .
3 . 3 . 2 Comparison to supervised baselines We further compare our methods with two supervised models .
For each supervised model , we provide a proportion of manually labeled data for training , which is randomly selected from gold - standard annotations .
However , we didn ' t use any labeled data for our approach .
• MNB : The labeled seeds are used to train a MNB classifier to classify all unlabeled aspect phrases into different classes .
• L - Kmeans : In L - Kmeans , the clusters of the labeled seeds are fixed at the initiation and remain unchanged during iteration .
We experiment with several settings : taking 5 %, 10 % and 15 % of the manually labeled aspect phrases for training , and the remainder as unlabeled data .
Experiment results is shown in Table 4 ( the results are averaged over 4 domains ).
We can see that our unsupervised approach is roughly as good as the supervised MNB with 10 % labeled data .
Our unsupervised approach is also slightly better than L - Kmeans with 15 % labeled data .
This result further demonstrates the effectiveness of our model .
3 . 3 . 3 Influence of parameters We vary the confidence level from 90 % to 99 . 9 % to see how it impacts on the performance of SDC - MNB .
The results are presented in Fig .
5 ( the results are averaged over 4 domains ).
We can see that the performance of clustering is fairly stable when changing the confidence level , which implies the robustness of our model .
A entropy Figure 5 : Influence of the confidence level on SDC - MNB .
3 . 3 . 4 Analysis of SDC - constraint As mentioned in Section 2 . 2 , SDC - constraint is dependent on the number of observations .
More observations we get , more informative the constraint is , which means the constraint is tighter and djk ( see Eq . 4 ) is smaller .
For all k , we count how many djk is less than 0 . 2 ( and 1 ) on average for each aspect phrase fj .
djk is calculated with a confidence level of 99 %.
The statistics of constraints is given in Table 5 .
We can see that the cellphone domain has the most informative and largest constraint set , that may explain why SDC - MNB achieves the largest purity gain ( over L - EM ) in cellphone domain .
Table 5 : Constraint statistics on different domains .
4 Related Work Our work is related to two important research topics : aspect - level sentiment analysis , and constraint - driven learning .
For aspect - level sentiment analysis , aspect extraction and clustering are key tasks .
For constraint - driven learning , a variety of frameworks and models for sentiment analysis have been studied extensively .
There have been many studies on clustering aspect - related phrases .
Most existing studies are based on context information .
Some works also encoded lexical similarity and synonyms as prior knowledge .
Carenini et al .
( 2005 ) proposed a method that was based on several similarity metrics involving string similarity , synonyms , and lexical distances defined with WordNet .
Table 4 : Comparison to supervised baselines .
MNB - 5 % means MNB with 5 % labeled data .
( 2009 ) proposed a multi - level latent semantic association model to capture expression - level and context - level topic structure .
Zhai et al .
( 2010 ) proposed an EM - based semi - supervised learning method to group aspect expressions into user - specified aspects .
They employed lexical knowledge to provide a better initialization for EM .
In Zhai et al .
( 2011a ), an EM - based unsupervised version was proposed .
The so - called L - EM model first generated softly labeled data by grouping feature expressions that share words in common , and then merged the groups by lexical similarity .
Zhai et al .
( 2011b ) proposed a LDA - based method that incorporates must - link and cannot - link constraints .
Another line of work aimed to extract and cluster aspect words simultaneously using topic modeling .
Titov and McDonald ( 2008 ) proposed the multi - grain topic models to discover global and local aspects .
Branavan et al .
( 2008 ) proposed a method which first clustered the key - phrases in Pros and Cons into some aspect categories based on distributional similarity , then built a topic model modeling the topics or aspects .
Zhao et al .
( 2010 ) proposed the MaxEnt - LDA ( a Maximum Entropy and LDA combination ) hybrid model to jointly discover both aspect words and aspect - specific opinion words , which can leverage syntactic features to separate aspects and sentiment words .
Mukherjee and Liu ( 2012 ) proposed a semi - supervised topic model which used user - provided seeds to discover aspects .
Chen et al .
( 2013 ) proposed a knowledge - based topic model to incorporate must - link and cannot - link information .
Their model can adjust topic numbers automatically by leveraging cannot - link .
Our work is also related to general constraint - driven ( or knowledge - driven ) learning models .
Several general frameworks have been proposed to fully utilize various prior knowledge in learning .
Constraint - driven learning ( Chang et al ., 2008 ) ( CODL ) is an EM - like algorithm that incorporates per - instance constraints into semi - supervised learning .
Posterior regularization ( Graca et al ., 2007 ) ( PR ) is a modified EM algorithm in which the E - step is replaced by the projection of the model posterior distribution onto the set of distributions that satisfy auxiliary expectation constraints .
Generalized expectation criteria ( Druck et al ., 2008 ) ( GE ) is a framework for incorporating preferences about model expectations into parameter estimation objective functions .
( 2009 ) developed a Bayesian decision - theoretic framework to learn an exponential family model using general measurements on the unlabeled data .
In this paper , we model our problem in the framework of posterior regularization .
Many works promoted the performance of sentiment analysis by incorporating prior knowledge as weak supervision .
Li and Zhang ( 2009 ) injected lexical prior knowledge to non - negative matrix tri - factorization .
Shen and Li ( 2011 ) further extended the matrix factorization framework to model dual supervision from both document and word labels .
Vikas Sindhwani ( 2008 ) proposed a general framework for incorporating lexical information as well as unlabeled data within standard regularized least squares for sentiment prediction tasks .
Fang ( 2013 ) proposed a structural learning model with a handful set of aspect signature terms that are encoded as weak supervision to extract latent sentiment explanations .
5 Conclusions Aspect finding and clustering is an important task for aspect - level sentiment analysis .
In order to cluster aspect - related phrases , this paper has explored a novel concept , sentiment distribution consistency .
We formalize the concept as soft constraint , integrate the constraint with a context - based probabilistic model , and solve the problem in the posterior regularization framework .
The proposed model is also designed to be robust with both sufficient and insufficient observations .
Experiments show that our approach outperforms state - of - the - art baselines consistently .
Acknowledgments This work was partly supported by the following grants from : the National Basic Research Program ( 973 Program ) under grant No . 2012CB316301 and 2013CB329403 , the National Science Foundation of China project under grant No . 61332007 and No .
61272227 , and the Beijing Higher Education Young Elite Teacher Project .
