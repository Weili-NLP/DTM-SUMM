Feature Selection for Sentiment Analysis Based on Content and Syntax Models Recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon - based approaches where the set of features are generated by humans , to approaches that use general statistical measures where features are selected solely on empirical evidence .
The advantage of statistical approaches is that they are fully automatic , however , they often fail to separate features that carry sentiment from those that do not .
In this paper we propose a set of new feature selection schemes that use a Content and Syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities .
By focusing only on the subjective expressions and ignoring the entities , we can choose more salient features for document - level sentiment analysis .
The results obtained from using these features in a maximum entropy classifier are competitive with the state - of - the - art machine learning approaches .
1 Introduction As user generated data become more commonplace , we seek to find better approaches to extract and classify relevant content automatically .
This gives users a richer , more informative , and more appropriate set of information in an efficient and organized manner .
One way for organizing such data is text classification , which involves mapping documents into topical categories based on the occurrences of particular features .
Sentiment Analysis ( SA ) can be framed as a text classification task where the categories are polarities such as positive and negative .
However , the similarities end here .
Whereas general text classification is concerned with features that distinguish different topics , sentiment analysis deals with features about subjectivity , affect , emotion , and points - of - view that describe or modify the related entities .
Since user - generated review documents contain both kinds of features , SA solutions ultimately face the challenge of separating the factual content from the subjective content describing it .
For example , taking a segment from a randomly chosen document in Pang et al .
' s movie review corpus , we see how entities and modifiers are related to each other : ... Of course , it helps that Kaye has an actor as talented as Norton to play this part .
It ' s astonishing how frightening Norton looks with a shaved head and a swastika on his chest .
... Visually , the film is very powerful .
Kaye indulges in a lot of interesting artistic choices , and most of them work nicely .
Indeed , most of the information about an entity that relates it to a particular polarity comes from the modifying words .
In the example above , these words are adjectives such as talented , frightening , interesting , and powerful .
They can also be verbs such as work and adverbs such as nicely .
The entities are represented by various nouns and pronouns such as : Kaye , Norton , actor and them .
http :// www . cs . cornell . edu / people / pabo / movie - review - data / Therefore , the task of classifying a review document can be explored by taking into account a mixture of entities and their modifiers .
An important characteristic of review documents is that the reviewers tend to discuss the whole set of entities throughout the entire document , whereas the modifiers for those entities tend to be more localized at the sentence or phrase level .
In other words , each entity can be polymorphous within the document , with a long - range semantic relationship between its forms while the modifiers in each case are bound to the entity in a short - range , syntactic relationship .
Generalizing a single entity to all the entities that are found in a document , and taking all their respective modifiers into account , we can start to infer the polarity of the entire document based on the set of all the modifiers .
This reduces to finding all the syntactic words in the document and disregarding the entities .
Taking another look at the example modifiers , we might assume that all of the relevant indicators for SA come from specific parts of speech categories such as adjectives and adverbs , while other parts of speech classes such as nouns are more relevant for general text classification , and can be discarded .
However , as demonstrated by Pang et al .
( 2002 ), Pang and Lee ( 2004 ), Hu and Liu ( 2004 ), and Riloff et al .
( 2003 ), there are some nouns and verbs that are useful sentiment indicators as well .
Therefore , a clear distinction cannot be made along parts of speech categories .
To address this issue , we propose a feature selection scheme in which we can obtain important sentiment indicators that : 1 .
Do not rely on specific parts of speech classes while maintaining the focus on syntax words .
Separate semantic words that do not indicate sentiment while keeping nouns that do .
Reflect the domain for the set of documents .
By using feature selection schemes that focus on the outlined sentiment indicators as a basis for our machine learning approach , we should achieve competitive accuracy results when classifying document polarities .
The rest of this paper is organized as follows .
Section 2 discusses some important work and results for SA and outlines the modelling and classification techniques used by our approach .
Section 3 provides details about our feature selection methods .
Our experiments and analyses are given in section 4 , and conclusions and future directions are presented in section 5 .
2 Related Work 2 . 1 Feature Selection in Sentiment Analysis The majority of the approaches for SA involve a two - step process : 1 .
Identify the parts of the document that will likely contribute to positive or negative sentiments .
Combine these parts of the document in ways that increase the odds of the document falling into one of these two polar categories .
The simplest approach for ( 1 ) by Pang et al .
( 2002 ) is to use the most frequently - occurring words in the corpus as polarity indicators .
This approach is commonly used with general text classification , and the results achieved indicate that simple document frequency cutoffs can be an effective feature selection scheme .
However , this scheme picks up on many entity words that do not contain any subjectivity .
The most common approach , used by researchers such as Das and Chen ( 2007 ), starts with a manually created lexicon specific to their particular domain whereas others ( Hurst and Nigam , 2004 ; Yi et al ., 2003 ) attempt to craft a general - purpose opinion lexicon that can be used across domains .
More recent lexicon - based approaches ( Ding et al ., 2008 ; Hu and Liu , 2004 ; Kim and Hovy , 2004 ; Riloff et al ., 2003 ) begin with a small set of ' seed ' words and bootstrap this set through synonym detection or various on - line resources to obtain a larger lexicon .
However , lexicon - based approaches have several key difficulties .
First , they take time to compile .
Whitelaw et al .
( 2005 ) report that their feature selection process took 20 person - hours , since it involves work done by human annotators .
In separate qualitative experiments done by Pang et al .
( 2002 ), agreement between human judges when given a list of sentiment - bearing words is as low as 58 % and no higher than 76 %.
In addition , some words may not be frequent enough for a classification algorithm .
2 . 2 Topic Modelling and HMM - LDA Topic models such as Latent Dirichlet Allocation ( LDA ) are generative models that allow documents to be explained by a set of unobserved ( latent ) topics .
Hidden Markov Model LDA ( HMM - LDA ) ( Griffiths et al ., 2005 ) is a topic model that simultaneously models topics and syntactic structure in a collection of documents .
The idea behind the model is that a typical word can play different roles .
It can either be part of the content and serve in a semantic ( topical ) purpose or it can be used as part of the grammatical ( syntactic ) structure .
It can also be used in both contexts .
HMM - LDA models this behavior by inducing syntactic classes for each word based on how they appear together in a sentence using a Hidden Markov Model .
Each word gets assigned to a syntactic class , but one class is reserved for the semantic words .
Words in this class behave as they would in a regular LDA topic model , participating in different topics and having certain probabilities of appearing in a document .
More formally , the model is defined in terms of three sets of variables and a generative process .
Let w = { w \, ..., wn } be a sequence of words where each word Wi is one of V words ; z = { z1 ,..., zn }, a sequence of topic assignments where each zi is one of K topics ; and c = { ci ,..., cn }, a sequence of class assignments where each ci is one of C classes .
One class , ci = 1 is designated as the ' semantic class ', and the rest , the ' syntactic ' classes .
Since we are dealing with a Hidden Markov Model , we require a variable representing the transition probabilities between the classes , given by a C x C transition matrix n that models transitions between classes ci - 1 and ci .
The generative process is described as follows : 1 .
Sample from a Dirichlet prior Dir ( a ) 2 .
For each word w i in document d : where (/>( zi ) — Dir ( ß ) and (/>( ci ) — Dir ( 6 ), both from Dirichlet distributions .
2 . 3 Text Classification Based on Maximum Entropy Modelling Maximum Entropy Modelling ( Manning and Schütze , 1999 ) is a framework whereby the features represent constraints on the overall model and the idea is to incorporate the knowledge that we have while preserving as much uncertainty as possible about the knowledge we do not have .
The features fi are binary functions where there is a vector x representing input elements ( unigram features in our case ) and c , the class label for one of the possible categories .
More specifically , a feature function is defined as follows : where word wi and category c correspond to a specific feature .
Employing the feature functions described above , a Maximum Entropy model takes the following form : where K is the number of features , ai is the weight for feature fi , and Z is a normalizing constant .
By taking the logarithm on both sides , we get the loglinear model : To classify a document , we compute P ( c \! x ) so that the c with the highest probability will be the category for the given document .
1 if x contains w i and c = cC 0 otherwise 3 Feature Selection ( FS ) Based on HMM - LDA 3 . 1 Characteristics of Salient Features To motivate our approach , we first describe criteria that are useful in selecting salient features for SA : 1 .
Features should be expressive enough to add useful information to the classification process .
As discussed in section 1 , the most expressive features in terms of polarity are the modifying words that describe an entity in a certain way .
These are usually , but not restricted to , adjectives , adverbs , subjective verbs and nouns .
All features together should form a broad and comprehensive viewpoint ofthe entire corpus .
In a corpus of many documents , some features can represent a subset of the corpus very accurately , while other features may represent another subset of the corpus .
The problem arises when representing the whole corpus with a specific feature set ( Sebastiani , 2002 ).
Features should be as domain - dependent as possible .
Examples from Hurst and Nigam many other approaches indicate that SA is a domain - dependant task , and the final features should reflect the domain of the corpus that they are representing .
Features must be frequent enough .
Rare features do not occur in many documents and make it difficult to train a machine learning algorithm .
Experiments by Pang et al .
( 2002 ) indicate that having more features does not help learning , and the best accuracy was achieved by selecting features based on document frequency .
Features should be discriminative enough .
A learning system needs to be able to pick up on their presence in certain documents for one outcome and absence in other documents for another outcome in classification .
3 . 2 FS Based on Syntactic Classes Our proposed FS scheme is to utilize HMM - LDA to obtain words that , for the most part , follow the criteria we set out in subsection 3 . 1 .
We train an HMM - LDA model to give us the syntactic classes that we further combine to form our final features .
Let word wi G V where V is the vocabulary .
Also let cj G C be a class .
We define Pcj ( wi ) as the probability of word wi in class cj , and one class , cj = 1 indicates the semantic class .
Since each class ( syntactic and semantic ) has a probability distribution over all words , we need to select words that offer a good representation of the class .
The representative words in each class have a much higher probability than the other words .
Therefore , we can select the representative words by the cumulative probabil - itty .
Specifically , we select the top percentage of the words in a class whereby the sum of their probabilities will be within some pre - defined range .
This is necessary since there are many words in each class with low probabilities in which we are not interested ( Steyvers and Griffiths , 2006 ).
The cumulative distribution function is defined as : Then , we can define the set of words in class cj as : where n is a pre - defined threshold such that 0 < n < 1 .
Next , we define the set of words in all the syntactic classes Wsyn as : and the set of words in the semantic class Wsem as : Since modifying words for sentiment typically fall into syntactic classes , we could use words in Wsyn as features for SA .
However , as observed by Pang et al .
( 2002 ), the best classification performance is achieved by a subset of features ( typically around 2500 ).
As a general step , we can apply a document frequency ( DF ) cutoff to select the most frequent features .
Let df ( wi ) denote the document frequency of word wi , indicating the number of documents in which wi occurs in the corpus .
Then the resulting features selected based on df can be defined as : cut ( Wsyn , e ) = { wilwi G Wsyn and df ( wi ) > e } where e is the minimum document frequency required for feature selection .
3 . 3 FS Based on Set Difference between Syntactic and Semantic Classes The main characteristic of using HMM - LDA classes for feature selection is that the set of words in the syntactic classes and the set of words in the semantic class are not disjoint .
In fact , there is quite a large overlap .
In this and the next subsections , we discuss ways to remedy and even exploit this situation to get a higher level of accuracy .
In the Pang et al .
movie review data , there is about 35 % overlap between words in the syntactic and semantic classes for n = 0 . 9 .
Our first systematic approach attempts to gain better accuracy by lowering the ratio of semantic words in the final feature set .
More formally , given the set of syntactic words Wsyn , we can reduce the overlap with Wsem by doing a set difference operation : This will give us all the words that are more favoured in the syntactic classes .
However , as we shall see shortly , and also as we earlier speculated , by subtracting all the words in the semantic class , we are actually getting rid ofsome useful features .
This is because ( a ) it is possible for the semantic class to contain words that are syntactic , and as a result are useful , and ( b ) there exist some semantic words that are good indicators of polarity .
Therefore , we seek to ' lessen ' the influence of the semantic class by cutting only a certain portion of it out , but not all of them .
For the above scheme , we outline Algorithm 1 that enables us to select features from Wsyn by applying a percentage cutoff for Wsem and then doing a set difference operation .
We define top ( Wsem , 6 ) to be the 6 % of the words with top probabilities in Wsem .
Note that when 6 = 1 . 0 , we get the same result as Wsyn - Wsem .
In our experiments , we try a range of 6 values for SA .
Algorithm 1 Syntactic - Semantic Set Difference Require : Wsyn and Wsem as input 3 . 4 FS Based on Max Scores of Syntactic Features The running theme through the HMM - LDA feature selection schemes is that if a word is highly ranked ( has a high probability of occurring ) in a syntactic class , we should use that word in our feature set .
Moreover , if a word is highly ranked in the semantic class , we usually do not want to use that word in our feature set because the word usually indicates a frequent noun .
Therefore , the desirable words are those that occur with high probability in the syntactic classes , but do not occur with high probability in the semantic class , or do not occur there at all .
To this end , we have formulated a scheme that adds such words to our feature set .
For each word , we obtain its highest probability in the set of syntactic classes .
Comparing this probability with the probability of the same word in the semantic class , we disregard the word if the probability in the semantic class is greater .
We define the max scores for word wi for both the syntactic and semantic classes and describe how we select features based on the max scores in Algorithm 2 .
Algorithm 2 Max Scores of Syntactic Features Require : cj G C where 1 < j < \! C \!
1 : for all wi G V do 4 Experiments This section describes the steps taken to generate some experimental results for each scheme described in the previous section .
Before we can analyze these sets of results , we take a look at some baselines .
4 . 1 Evaluation We use the corpus of2000 movie reviews ( Pang and negative documents selected from on - line forums .
In our experiments , we randomize the documents and split the data into 1800 for training / testing purposes and 200 as the validation set .
For the 1800 documents , we run a 3 - fold cross validation procedure where we train on 1200 documents and test on 600 .
We compare the resultantfeature sets aftereach FS scheme using the OpenNLP Maximum Entropy classifier .
Throughout these experiments , we are interested in the classification accuracy .
This is evaluated simply by comparing the resultant class from the classifier and the actual class annotated by Pang and Lee ( 2004 ).
The number of matches is divided by the number of documents in the test set .
Thus , given an annotated test set dtestA = {( d1 , o1 ), ( d2 , o2 ),... ( ds , os )} and the classified set , dtestB = {( d1 , Q1 ), ( d2 , q2 ),... ( ds , qs )}, we calculate the accuracy as follows : where I (•) is the indicator function .
4 . 2 Baseline Results After replicating the results from Pang et al .
( 2002 ), we varied the number of iterations per fold by using a held - out validation set ' eval '.
The higher accuracy achieved suggests that the model was not fully trained after 10 iterations .
In order to compare with our HMM - LDA based schemes , we ran experiments to explore a basic POS - based feature selection scheme .
In this approach , we first tagged the words in each document with POS tags and selected the most frequently - occurring unigrams that were not tagged as ' NN ', ' NNP ', ' NNS ' or ' NNPS ' ( the ' noun ' categories ).
This corresponds to POS (- NN *) in Table 1 .
Next , we tagged all the words and only selected the words that were tagged as ' JJ *', ' RB *', and ' VB *' categories ( the ' syntactic ' categories ).
The idea is to include as part of the feature set all the words that are not ' semantically oriented '.
This corresponds to POS ( JJ * + RB * + VB *) in Table 1 . http :// incubator . apache . org / opennlp / Table 1 : Baseline results with a different number ofiter - ations .
Each column represents a different feature selection method .
4 . 3 HMM - LDA Training Our feature selection methods involve training an HMM - LDA model on the Pang et al .
corpus of movie reviews , taking the class assignments , and combining the resultant unigrams to create features for the MaxEnt classifier .
Since HMM - LDA is an unsupervised topic model , we can train it on the entire corpus .
We trained the model using the Topic Modelling Toolbox MATLAB package on the 2000 movie reviews .
Since the HMM - LDA model requires sentences to be outlined , we used the usual end - of - sentence markers ('.
The training parameters are T = 50 topics , S = 20 classes , ALPHA = 1 . 0 , BETA = 0 . 01 , and GAMMA = 0 . 1 .
We found that 1000 iterations is sufficient as we tracked the log - likelihood of every 10 iterations .
After training , we have both the topic assignments z and the class assignments c for each word in each of the samples .
4 . 4 Selecting Features Based on Syntactic Classes In this experiment we fix n = 0 . 9 to get the top words in each class having a cumulative probability under 0 . 9 .
These are the representative words in each class which we merge into Wsyn .
Finally , we select 2500 words by the df cutoff method .
This list of words is then used as features for the Max - Ent classifier .
We run the classifier for 10 , 25 and ' eval ' number of iterations in order to compare with the baseline results .
http :// psiexp . ss . uci . edu / research / program ^ data / toolbox . htm Wsem .
At n = 0 . 9 , there are 6 , 189 words in Wsyn before we select the top 2500 using the df cutoff .
From Table 2 , we see that the accuracy has increased from 0 . 845 to 0 . 863 at the ' eval ' number iterations .
In all of our experiments , we use df cutoff to get a manageable number of features for the classifier .
This is partly based on Pang et al .
( 2002 ) and partly based on calculating the Pearson correlation for each class between the document frequency and word probability at n = 0 . 9 .
Since every class has a positive correlation in the range of [ 0 . 313938 , 0 . 888160 ] where the average is 0 . 576 , we can say that there is a correlation between the two values .
4 . 5 Selecting Features Based on Set Difference The result for Set Difference is derived by varying the percentage of top semantic words that should be excluded in the final feature set .
For example , some words in Wsyn D Wsem that have a higher probability in Wsem are : ' hollywod ', ' war ', and ' fiction ' while some words that have a higher probability in Wsyn include : ' good ', ' love ' and ' funny ' .
The 6 value is defined by the percentage of the words in Wsem that we exclude from Wsyn .
The results for 0 . 0 < 6 < 1 . 0 for increments of 6 x \!
Wsem \!, are summarized in Table 3 .
Table 3 : Results for FS Based on Syntactic - Semantic set difference method .
Each row represents the accuracy achieved at a particular 6 value .
From the results , we can see that as we remove more and more words from Wsem , the accuracy level decreases .
This suggests that Wsem D Wsyn contains some important features and if we subtract Wsem entirely , we essentially eliminate them .
At each cutoff level , we are eliminating 10 % until we have eliminated the whole set .
Clearly , a more fine - grained approach is needed , and that leads us to the Max - Score results .
4 . 6 Selecting Features Based on Max Scores For the method based on Max Scores , we may select features that are in both Wsem and Wsyn sets as long as their max scores in Wsyn are higher than those in Comparing the accuracy in Table 4 with those in the previous subsections , we can say that using the fine - grained Max - Score algorithm improves the classification accuracy .
This means that iteratively removing words that have a relatively higher probability in Wsem compared to Wsyn does not eliminate important words occurring in both sets , but lessens the influence of some high probability words in Wsem .
4 . 7 Discussion of the Results For our experiments , the best accuracy is achieved by utilizing the Max - Score algorithm ( outlined in subsection 3 . 4 ) after a further selection of 2500 with the df cutoff .
As discussed in subsection 3 . 4 , the Max - Score algorithm enables us to select words that have a higher score in Wsyn than in Wsem .
This approach has the dual advantage of keeping the words that are present in both Wsyn and Wsem but have higher scores in Wsyn and ignoring the words that are also present in both sets but have higher scores in Wsem .
Ultimately , this decreases the influence of the frequent and overlapped words that have a high probability in Wsem .
Finally , to quantify the significance level of our best approach against the baseline methods in subsection 4 .
2 , we calculated the p - values for the one - tailed t - tests comparing our best approach based on Max Scores with the DF and POS (- NN *) baselines , respectively .
The resulting p - values of 0 . 011 and 0 . 014 suggest that our best approach is significantly better than the baseline approaches .
Table 2 : Results for FS Based on Syntactic Classes at 10 , 25 and ' eval ' iterations .
Table 4 : Result for FS Based on Max Scores .
5 Conclusions and Future Directions In this paper , we have described a method for feature selection based on long - range and short - range dependencies given by the HMM - LDA topic model .
By modelling review documents based on the combinations of syntactic and semantic classes , we have devised a method of separating the topical content that describes the entities under review from the opinion context ( given by sentiment modifiers ) about that entity in each case .
By grouping all the sentiment modifiers for each entity in a document , we are selecting the features that are intuitively in line with the outlined characteristics of salient features for SA ( see subsection 3 . 1 ).
This is backed up by our experiments where we achieve competitive results for document polarity classification .
One avenue for future development of this framework could include identifying and extracting aspects from a review document .
So far , we have not identified aspects from the entities , choosing instead to classify a document as a whole .
However , this framework can be readily applied to extract relevant ( most probable ) aspects using the LDA topic model and then restrict the syntactic modifiers to the range of sentences where an aspect occurs .
This would give us an unsupervised aspect extraction scheme that we can combine with a classifier to predict polarities for each aspect .
