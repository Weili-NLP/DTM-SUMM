Semi - supervised latent variable models for sentence - level sentiment analysis We derive two variants of a semi - supervised model for fine - grained sentiment analysis .
Both models leverage abundant natural supervision in the form of review ratings , as well as a small amount of manually crafted sentence labels , to learn sentence - level sentiment classifiers .
The proposed model is a fusion of a fully supervised structured conditional model and its partially supervised counterpart .
This allows for highly efficient estimation and inference algorithms with rich feature definitions .
We describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence - level sentiment analysis compared to all baselines .
1 Sentence - level sentiment analysis In this paper , we demonstrate how combining coarse - grained and fine - grained supervision benefits sentence - level sentiment analysis - an important task in the field of opinion classification and retrieval ( Pang and Lee , 2008 ).
Typical supervised learning approaches to sentence - level sentiment analysis rely on sentence - level supervision .
While such fine - grained supervision rarely exist naturally , and thus requires labor intensive manual annotation effort ( Wiebe et al ., 2005 ), coarse - grained supervision is naturally abundant in the form of online review ratings .
This coarse - grained supervision is , of course , less informative compared to fine - grained supervision , however , by combining a small amount of sentence - level supervision with a large amount of document - level supervision , we are able to substantially improve on the sentence - level classification task .
Our work combines two strands of research : models for sentiment analysis that take document structure into account ; and models that use latent variables to learn unobserved phenomena from that which can be observed .
Exploiting document structure for sentiment analysis has attracted research attention since the early work of Pang and Lee ( 2004 ), who performed minimal cuts in a sentence graph to select subjective sentences .
McDonald et al .
( 2007 ) later showed that jointly learning fine - grained ( sentence ) and coarsegrained ( document ) sentiment improves predictions at both levels .
More recently , Yessenalina et al .
( 2010 ) described how sentence - level latent variables can be used to improve document - level prediction and Nakagawa et al .
( 2010 ) used latent variables over syntactic dependency trees to improve sentence - level prediction , using only labeled sentences for training .
In a similar vein , Sauper et al .
( 2010 ) integrated generative content structure models with discriminative models for multi - aspect sentiment summarization and ranking .
These approaches all rely on the availability of fine - grained annotations , but Tackstrom and McDonald ( 2011 ) showed that latent variables can be used to learn fine - grained sentiment using only coarse - grained supervision .
While this model was shown to beat a set of natural baselines with quite a wide margin , it has its shortcomings .
Most notably , due to the loose constraints provided by the coarse supervision , it tends to only predict the two dominant fine - grained sentiment categories well for each document sentiment category , so that almost all sentences in positive documents are deemed positive or neutral , and vice versa for negative documents .
As a way of overcoming these shortcomings , we propose to fuse a coarsely supervised model with a fully supervised model .
Below , we describe two ways of achieving such a combined model in the framework of structured conditional latent variable models .
Contrary to ( generative ) topic models ( Mei et al ., 2007 ; Titov and Figure 1 : a ) Factor graph of the fully observed graphical model , b ) Factor graph of the corresponding latent variable model .
During training , shaded nodes are observed , while non - shaded nodes are unobserved .
The input sentences sj are always observed .
Note that there are no factors connecting the document node , yd , with the input nodes , s , so that the sentence - level variables , ys , in effect form a bottleneck between the document sentiment and the input sentences .
McDonald , 2008 ; Lin and He , 2009 ), structured conditional models can handle rich and overlapping features and allow for exact inference and simple gradient based estimation .
The former models are largely orthogonal to the one we propose in this work and combining their merits might be fruitful .
As shown by Sauper et al .
( 2010 ), it is possible to fuse generative document structure models and task specific structured conditional models .
While we do model document structure in terms of sentiment transitions , we do not model topical structure .
An interesting avenue for future work would be to extend the model of Sauper et al .
( 2010 ) to take coarse - grained task - specific supervision into account , while modeling fine - grained task - specific aspects with latent variables .
Note also that the proposed approach is orthogonal to semi - supervised and unsupervised induction of context independent ( prior polarity ) lexicons ( Turney , 2002 ; Kim and Hovy , 2004 ; Esuli and Sebastiani , 2009 ; Rao and Ravichandran , 2009 ; Velikovich et al ., 2010 ).
The output of such models could readily be incorporated as features in the proposed model .
1 . 1 Preliminaries Let d be a document consisting of n sentences , s = ( si ) n = 1 , with a document - sentence - sequence pair denoted d = ( d , s ).
Let yd = ( yd , ys ) denote random variables - the document level sentiment , yd , and the sequence of sentence level sentiment , ys = ( yf )"= 1 .
We are abusing notation throughout by using the same symbols to refer to random variables and their particular assignments .
In what follows , we assume that we have access to two training sets : a small set of fully labeled instances , Dp = {( dj , y ^}™^, and a large set of coarsely labeled instances DC = {( dj , yd )} j =„+ j + 1 .
Furthermore , we assume that yd and all y \!
take values in { POS , NEG , NEU }.
We focus on structured conditional models in the exponential family , with the standard parametrization where 6 eK " is a parameter vector , </>(•) eK " is a vector valued feature function that factors according to the graph structure outlined in Figure 1 , and Ae is the log - partition function .
This class of models is known as conditional random fields ( CRFs ) ( Lafferty et al ., 2001 ), when all variables are observed , and as hidden conditional random fields ( HCRFs ) ( Quattoni et al ., 2007 ), when only a subset of the variables are observed .
1 . 2 The fully supervised fine - to - coarse model McDonald et al .
( 2007 ) introduced a fully supervised model in which predictions of coarse - grained ( document ) and fine - grained ( sentence ) sentiment are learned and inferred jointly .
They showed that learning both levels jointly improved performance at both levels , compared to learning each level individually , as well as to using a cascaded model in which the predictions at one level are used as input to the other .
Figure 1a outlines the factor graph of the corresponding conditional random field .
The parameters , Of , of this model can be estimated from the set of fully labeled data , Df , by maximizing the joint conditional likelihood function where aF is the variance of the Normal ( 0 , aF ) prior .
Note that Lf is a concave function and consequently its unique maximum can be found by gradient based optimization techniques .
1 . 3 Latent variables for coarse supervision Recently , Tackstrom and McDonald ( 2011 ) showed that fine - grained sentiment can be learned from coarse - grained supervision alone .
Specifically , they used a hcrf model with the same structure as that in Figure 1a , but with sentence labels treated as latent variables .
The factor graph corresponding to this model is outlined in Figure 1b .
The fully supervised model might benefit from factors that directly connect the document variable , yd , with the inputs s . However , as argued by Tackstrom and McDonald ( 2011 ), when only document - level supervision is available , the document variable , y , should be independent of the input , s , conditioned on the latent variables , ys .
This prohibits the model from bypassing the latent variables , which is crucial , since we seek to improve the sentence - level predictions , rather than the document - level predictions .
The parameters , 0C , of this model can be estimated from the set of coarsely labeled data , DC , by maximizing the marginalized conditional likelihood function iog ^ P & C { vj yS \ sj where the marginalization is over all possible sequences of latent sentence label assignments ys .
Due to the introduction of latent variables , the marginal likelihood function is non - concave and thus there are no guarantees of global optimality , however , we can still use a gradient based optimization technique to find a local maximum .
Figure 1a differs slightly from the model employed by McDonald et al .
( 2007 ), where they had factors connecting the document label y d with each input s i as well .
2 Combining coarse and full supervision The fully supervised and the partially supervised models both have their merits .
The former requires an expensive and laborious process of manual annotation , while the latter can be used with readily available document labels , such as review star ratings .
The latter , however , has its shortcomings in that the coarse - grained sentiment signal is less informative compared to a ine - grained signal .
Thus , in order to get the best of both worlds , we would like to combine the merits of both of these models .
2 . 1 A cascaded model A straightforward way of fusing the two models is by means of a cascaded model in which the predictions of the partially supervised model , trained by maximizing Lc ( Oc ) are used to derive additional features for the fully supervised model , trained by maximizing Lf ( Of ).
Although more complex representations are possible , we generate meta - features for each sentence based solely on operations on the estimated distributions , poG ( yd , ys \ s ).
Specifically , we encode the following probability distributions as discrete features by uniform bucketing , with bucket width 0 . 1 : the joint distribution , ( yd , ys \ s ); the marginal document distribution , pgG , ( yd \ s ); and the marginal sentence distribution , ( ys \ s ).
We also encode the argmax of these distributions , as well as the pair - wise combinations of the derived features .
The upshot of this cascaded approach is that it is very simple to implement and efficient to train .
The downside is that only the partially supervised model influences the fully supervised model ; there is no reciprocal influence between the models .
Given the non - concavity of Lc ( Oc ), such influence could be beneicial .
2 . 2 Interpolating likelihood functions A more flexible way of fusing the two models is to interpolate their likelihood functions , thereby allowing for both coarse and joint supervision of the same model .
Such a combination can be achieved by constraining the parameters so that 0 / = Of = Oc and taking the mean of the likelihood functions Lf and Lc , appropriately weighted by a hyper - parameter A .
The result is the interpolated likelihood function Li ( 0 /) = ALf ( 0 /) + ( 1 - A ) Lc ( 0 /).
A simple , yet eficient , way of optimizing this objective function is to use stochastic gradient ascent with learning rate rj .
At each step we select a fully labeled instance , ( dj , yd ) G Df , with probability A and a coarsely labeled instance , ( dj , yd ) G DC , with probability ( 1 — A ).
We then update the parameters , 0 /, according to the gradients dLF and dLC , respectively .
In principle we could use different learning rates r ?
C as well as different prior variances a " F and < rC , but in what follows we set them equal .
Since we are interpolating conditional models , we need at least partial observations of each instance .
Methods for blending discriminative and generative models ( Lasserre et al ., 2006 ; Suzuki et al ., 2007 ; Agarwal and Daume , 2009 ; Sauper et al ., 2010 ), would enable incorporation of completely unlabeled data as well .
It is straightforward to extend the proposed model along these lines , however , in practice coarsely labeled sentiment data is so abundant on the web ( e . g ., rated consumer reviews ) that incorporating completely unlabeled data seems superfluous .
Furthermore , using conditional models with shared parameters throughout allows for rich overlapping features , while maintaining simple and eficient inference and estimation .
3 Experiments For the following experiments , we used the same data set and a comparable experimental setup to that of Tackstrom and McDonald ( 2011 ).
We compare the two proposed hybrid models ( Cascaded and Interpolated ) to the fully supervised model of McDonald et al .
( 2007 ) ( FineToCoarse ) as well as to the soft variant of the coarsely supervised model of Tackstrom and McDonald ( 2011 ) ( Coarse ).
The learning rate was ixed to r = 0 . 001 , while we tuned the prior variances , < r , and the number of epochs for each model .
When sampling according to A during optimization of L /( 0 /), we cycle through Df and DC deterministically , but shuffle these sets between epochs .
Due to time constraints , we ixed the interpolation factor to A = 0 . 1 , but tuning this could potentially improve the results of the interpolated model .
For the same reason we allowed a maximum of 30 epochs , for all models , while Tackstrom and McDonald ( 2011 ) report a maximum of 75 epochs .
The annotated test data can be downloaded from http :// www . sics . se / people / oscar / datasets .
To assess the impact of fully labeled versus coarsely labeled data , we took stratiied samples without replacement , of sizes 60 , 120 , and 240 reviews , from the fully labeled folds and of sizes 15 , 000 and 143 , 580 reviews from the coarsely labeled data .
On average each review consists of ten sentences .
We performed 5 - fold stratified cross - validation over the labeled data , while using stratiied samples for the coarsely labeled data .
Statistical significance was assessed by a hierachical bootstrap of 95 % confidence intervals , using the technique described by Davison and Hinkley ( 1997 ).
3 . 1 Results and analysis Table 1 lists sentence - level accuracy along with 95 % conidence interval for all tested models .
We irst note that the interpolated model dominates all other models in terms of accuracy .
While the cascaded model requires both large amounts of fully labeled and coarsely labeled data , the interpolated model is able to take advantage of both types of data on its own and jointly .
Still , by comparing the fully supervised and the coarsely supervised models , the superior impact of fully labeled over coarsely labeled data is evident .
As can be seen in Figure 2 , when all data is used , the cascaded model outperforms the interpolated model for some recall values , and vice versa , while both models dominate the supervised approach for the full range of recall values .
As discussed earlier , and conirmed by Table 2 , the coarse - grained model only performs well on the predominant sentence - level categories for each document category .
The supervised model handles negative and neutral sentences well , but performs poorly on positive sentences even in positive documents .
The interpolated model , while still better at capturing the predominant category , does a better job overall .
These results are with a maximum of 30 training iterations .
Preliminary experiments with a maximum of 75 iterations indicate that all models gain from more iterations ; this seems to be especially true for the supervised model and for the cascaded model with less amount of course - grained data .
Table 1 : Sentence level results for varying numbers of fully labeled ( DF ) and coarsely labeled ( DC ) reviews .
Bold : significantly better than the FineToCoarse model according to a hierarchical bootstrapped confidence interval , p < 0 . 05 .
Figure 2 : Interpolated pos / neg sentence - level precision - recall curves with \ DC \ = 143 , 580 and \ DF \ = 240 . pos docs .
neg docs .
neu docs .
Table 2 : pos / neg / neu sentence - level Fi - scores per document category (\ DC \ = 143 , 580 and \ DF \ = 240 ).
4 Conclusions Learning ine - grained classiicationtasks in afully supervised manner does not scale well due to the lack of naturally occurring supervision .
We instead proposed to combine coarse - grained supervision , whichis naturally abundant but less informative , with ine - grained supervision , which is scarce but more informative .
To this end , we introduced two simple , yet effective , methods of combining fully labeled and coarsely labeled data for sentence - level sentiment analysis .
First , a cascaded approach where a coarsely supervised model is used to generate features for a fully supervised model .
Second , an interpolated model that directly optimizes a combination of joint and marginal likelihood functions .
Both proposed models are structured conditional models that allow for rich overlapping features , while maintaining highly efficient exact inference and robust estimation properties .
Empirically , the interpolated model is superior to the other investigated models , but with sufficient amounts of coarsely labeled and fully labeled data , the cascaded approach is competitive .
Acknowledgments The irst author acknowledges the support of the Swedish National Graduate School of Language Technology ( GSLT ).
The authors would also like to thank Fernando Pereira and Bob Carpenter for early discussions on using HCRFs in sentiment analysis .
